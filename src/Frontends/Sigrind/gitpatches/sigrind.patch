From 46a3e370022c4c34eae9dc4c118cde546ca0d398 Mon Sep 17 00:00:00 2001
From: Mike Lui <mike.d.lui@gmail.com>
Date: Mon, 13 Nov 2017 13:38:24 -0500
Subject: [PATCH] patch

---
 Makefile.am                |    1 +
 configure.ac               |    2 +
 sigrind/.ycm_extra_conf.py |  177 +++++
 sigrind/Makefile.am        |   83 ++
 sigrind/bb.c               |  345 ++++++++
 sigrind/bbcc.c             |  872 +++++++++++++++++++++
 sigrind/callgrind.h        |  363 +++++++++
 sigrind/callstack.c        |  425 ++++++++++
 sigrind/clo.c              |  687 ++++++++++++++++
 sigrind/context.c          |  332 ++++++++
 sigrind/debug.c            |  447 +++++++++++
 sigrind/events.c           |  261 ++++++
 sigrind/events.h           |  133 ++++
 sigrind/fn.c               |  686 ++++++++++++++++
 sigrind/global.h           |  886 +++++++++++++++++++++
 sigrind/jumps.c            |  233 ++++++
 sigrind/log_events.c       |  239 ++++++
 sigrind/log_events.h       |   63 ++
 sigrind/sg_main.c          | 1872 ++++++++++++++++++++++++++++++++++++++++++++
 sigrind/sigil2_ipc.c       |  266 +++++++
 sigrind/sigil2_ipc.h       |   28 +
 sigrind/tests/Makefile.am  |    5 +
 sigrind/threads.c          |  451 +++++++++++
 23 files changed, 8857 insertions(+)
 create mode 100644 sigrind/.ycm_extra_conf.py
 create mode 100644 sigrind/Makefile.am
 create mode 100644 sigrind/bb.c
 create mode 100644 sigrind/bbcc.c
 create mode 100644 sigrind/callgrind.h
 create mode 100644 sigrind/callstack.c
 create mode 100644 sigrind/clo.c
 create mode 100644 sigrind/context.c
 create mode 100644 sigrind/debug.c
 create mode 100644 sigrind/events.c
 create mode 100644 sigrind/events.h
 create mode 100644 sigrind/fn.c
 create mode 100644 sigrind/global.h
 create mode 100644 sigrind/jumps.c
 create mode 100644 sigrind/log_events.c
 create mode 100644 sigrind/log_events.h
 create mode 100644 sigrind/sg_main.c
 create mode 100644 sigrind/sigil2_ipc.c
 create mode 100644 sigrind/sigil2_ipc.h
 create mode 100644 sigrind/tests/Makefile.am
 create mode 100644 sigrind/threads.c

diff --git a/Makefile.am b/Makefile.am
index fdce3cf9f..e6648d35a 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -10,6 +10,7 @@ TOOLS =		memcheck \
 		lackey \
 		none \
 		helgrind \
+		sigrind \
 		drd
 
 EXP_TOOLS = 	exp-sgcheck \
diff --git a/configure.ac b/configure.ac
index 4d4521896..c2be6150e 100644
--- a/configure.ac
+++ b/configure.ac
@@ -4523,6 +4523,8 @@ AC_CONFIG_FILES([
    exp-dhat/tests/Makefile
    shared/Makefile
    solaris/Makefile
+   sigrind/Makefile
+   sigrind/tests/Makefile
 ])
 AC_CONFIG_FILES([coregrind/link_tool_exe_linux],
                 [chmod +x coregrind/link_tool_exe_linux])
diff --git a/sigrind/.ycm_extra_conf.py b/sigrind/.ycm_extra_conf.py
new file mode 100644
index 000000000..a9d50f65f
--- /dev/null
+++ b/sigrind/.ycm_extra_conf.py
@@ -0,0 +1,177 @@
+# This file is NOT licensed under the GPLv3, which is the license for the rest
+# of YouCompleteMe.
+#
+# Here's the license text for this file:
+#
+# This is free and unencumbered software released into the public domain.
+#
+# Anyone is free to copy, modify, publish, use, compile, sell, or
+# distribute this software, either in source code form or as a compiled
+# binary, for any purpose, commercial or non-commercial, and by any
+# means.
+#
+# In jurisdictions that recognize copyright laws, the author or authors
+# of this software dedicate any and all copyright interest in the
+# software to the public domain. We make this dedication for the benefit
+# of the public at large and to the detriment of our heirs and
+# successors. We intend this dedication to be an overt act of
+# relinquishment in perpetuity of all present and future rights to this
+# software under copyright law.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+# IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+# OTHER DEALINGS IN THE SOFTWARE.
+#
+# For more information, please refer to <http://unlicense.org/>
+
+import os
+import ycm_core
+
+# These are the compilation flags that will be used in case there's no
+# compilation database set (by default, one is not set).
+# CHANGE THIS LIST OF FLAGS. YES, THIS IS THE DROID YOU HAVE BEEN LOOKING FOR.
+flags = [
+'-Wall',
+'-Wextra',
+#'-Werror',
+#'-Wc++98-compat',
+#'-Wno-long-long',
+#'-Wno-variadic-macros',
+#'-fexceptions',
+#'-DNDEBUG',
+# THIS IS IMPORTANT! Without a "-std=<something>" flag, clang won't know which
+# language to use when compiling headers. So it will guess. Badly. So C++
+# headers will be compiled as C headers. You don't want that so ALWAYS specify
+# a "-std=<something>".
+# For a C project, you would set this to something like 'c99' instead of
+# 'c++11'.
+'-std=c99',
+# ...and the same thing goes for the magic -x option which specifies the
+# language that the files to be compiled are written in. This is mostly
+# relevant for c++ headers.
+# For a C project, you would set this to 'c' instead of 'c++'.
+'-x',
+'c',
+'-DVGO_linux',
+'-DVGA_amd64',
+'-isystem',
+'../BoostParts',
+'-isystem',
+# This path will only work on OS X, but extra paths that don't exist are not
+# harmful
+'/System/Library/Frameworks/Python.framework/Headers',
+'-isystem',
+'../llvm/include',
+'-isystem',
+'../llvm/tools/clang/include',
+'-I','.',
+'-I','..',
+'-I','../include',
+'-I','../coregrind',
+'-I','../VEX/pub',
+'-I','../VEX/priv',
+'-I','../inst/include/valgrind',
+'-I','../../',
+'-I','../../../../',
+'-I','../../../../../'
+]
+
+
+# Set this to the absolute path to the folder (NOT the file!) containing the
+# compile_commands.json file to use that instead of 'flags'. See here for
+# more details: http://clang.llvm.org/docs/JSONCompilationDatabase.html
+#
+# You can get CMake to generate this file for you by adding:
+#   set( CMAKE_EXPORT_COMPILE_COMMANDS 1 )
+# to your CMakeLists.txt file.
+#
+# Most projects will NOT need to set this to anything; you can just change the
+# 'flags' list of compilation flags. Notice that YCM itself uses that approach.
+compilation_database_folder = ''
+
+if os.path.exists( compilation_database_folder ):
+  database = ycm_core.CompilationDatabase( compilation_database_folder )
+else:
+  database = None
+
+SOURCE_EXTENSIONS = [ '.cpp', '.cxx', '.cc', '.c', '.m', '.mm' ]
+
+def DirectoryOfThisScript():
+  return os.path.dirname( os.path.abspath( __file__ ) )
+
+def MakeRelativePathsInFlagsAbsolute( flags, working_directory ):
+  if not working_directory:
+    return list( flags )
+  new_flags = []
+  make_next_absolute = False
+  path_flags = [ '-isystem', '-I', '-iquote', '--sysroot=' ]
+  for flag in flags:
+    new_flag = flag
+
+    if make_next_absolute:
+      make_next_absolute = False
+      if not flag.startswith( '/' ):
+        new_flag = os.path.join( working_directory, flag )
+
+    for path_flag in path_flags:
+      if flag == path_flag:
+        make_next_absolute = True
+        break
+
+      if flag.startswith( path_flag ):
+        path = flag[ len( path_flag ): ]
+        new_flag = path_flag + os.path.join( working_directory, path )
+        break
+
+    if new_flag:
+      new_flags.append( new_flag )
+  return new_flags
+
+
+def IsHeaderFile( filename ):
+  extension = os.path.splitext( filename )[ 1 ]
+  return extension in [ '.h', '.hxx', '.hpp', '.hh' ]
+
+
+def GetCompilationInfoForFile( filename ):
+  # The compilation_commands.json file generated by CMake does not have entries
+  # for header files. So we do our best by asking the db for flags for a
+  # corresponding source file, if any. If one exists, the flags for that file
+  # should be good enough.
+  if IsHeaderFile( filename ):
+    basename = os.path.splitext( filename )[ 0 ]
+    for extension in SOURCE_EXTENSIONS:
+      replacement_file = basename + extension
+      if os.path.exists( replacement_file ):
+        compilation_info = database.GetCompilationInfoForFile(
+          replacement_file )
+        if compilation_info.compiler_flags_:
+          return compilation_info
+    return None
+  return database.GetCompilationInfoForFile( filename )
+
+
+def FlagsForFile( filename, **kwargs ):
+  if database:
+    # Bear in mind that compilation_info.compiler_flags_ does NOT return a
+    # python list, but a "list-like" StringVec object
+    compilation_info = GetCompilationInfoForFile( filename )
+    if not compilation_info:
+      return None
+
+    final_flags = MakeRelativePathsInFlagsAbsolute(
+      compilation_info.compiler_flags_,
+      compilation_info.compiler_working_dir_ )
+
+  else:
+    relative_to = DirectoryOfThisScript()
+    final_flags = MakeRelativePathsInFlagsAbsolute( flags, relative_to )
+
+  return {
+    'flags': final_flags,
+    'do_cache': True
+  }
diff --git a/sigrind/Makefile.am b/sigrind/Makefile.am
new file mode 100644
index 000000000..08953ce5f
--- /dev/null
+++ b/sigrind/Makefile.am
@@ -0,0 +1,83 @@
+include $(top_srcdir)/Makefile.tool.am
+
+EXTRA_DIST =  
+
+#----------------------------------------------------------------------------
+# Headers, etc
+#----------------------------------------------------------------------------
+
+pkginclude_HEADERS = callgrind.h
+
+bin_SCRIPTS = 
+
+noinst_HEADERS = \
+	events.h \
+	global.h \
+	log_events.h
+
+
+#----------------------------------------------------------------------------
+# sigrind-<platform>
+#----------------------------------------------------------------------------
+
+noinst_PROGRAMS  = sigrind-@VGCONF_ARCH_PRI@-@VGCONF_OS@
+if VGCONF_HAVE_PLATFORM_SEC
+noinst_PROGRAMS += sigrind-@VGCONF_ARCH_SEC@-@VGCONF_OS@
+endif
+
+SIGRIND_SOURCES_COMMON = \
+	bb.c \
+	bbcc.c \
+	callstack.c \
+	clo.c \
+	context.c \
+	debug.c \
+	events.c \
+	fn.c \
+	jumps.c \
+	threads.c \
+	log_events.c \
+	sigil2_ipc.c \
+	sg_main.c 
+
+SIGRIND_CFLAGS_COMMON =  -I$(top_srcdir)/../../.. -I$(top_srcdir)/include -I$(top_srcdir)/VEX/pub
+
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_SOURCES      = \
+	$(SIGRIND_SOURCES_COMMON)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_PRI_CAPS@) $(SIGRIND_CFLAGS_COMMON)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_PRI@ \
+	$(LINK) \
+	$(sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS) \
+	$(sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS)
+
+if VGCONF_HAVE_PLATFORM_SEC
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_SOURCES      = \
+	$(SIGRIND_SOURCES_COMMON)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_SEC_CAPS@) $(SIGRIND_CFLAGS_COMMON)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_SEC@ \
+	$(LINK) \
+	$(sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS) \
+	$(sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS)
+endif
diff --git a/sigrind/bb.c b/sigrind/bb.c
new file mode 100644
index 000000000..ceea5b969
--- /dev/null
+++ b/sigrind/bb.c
@@ -0,0 +1,345 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                         bb.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/*------------------------------------------------------------*/
+/*--- Basic block (BB) operations                          ---*/
+/*------------------------------------------------------------*/
+
+/* BB hash, resizable */
+bb_hash bbs;
+
+void CLG_(init_bb_hash)()
+{
+   Int i;
+
+   bbs.size    = 8437;
+   bbs.entries = 0;
+   bbs.table = (BB**) CLG_MALLOC("cl.bb.ibh.1",
+                                 bbs.size * sizeof(BB*));
+
+   for (i = 0; i < bbs.size; i++) bbs.table[i] = NULL;
+}
+
+bb_hash* CLG_(get_bb_hash)()
+{
+  return &bbs;
+}
+
+/* The hash stores BBs according to
+ * - ELF object (is 0 for code in anonymous mapping)
+ * - BB base as object file offset
+ */
+static __inline__
+UInt bb_hash_idx(obj_node* obj, PtrdiffT offset, UInt size)
+{
+  return (((Addr)obj) + offset) % size;
+}
+
+/* double size of bb table  */
+static
+void resize_bb_table(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    BB **new_table, *curr, *next;
+    UInt new_idx;
+
+    new_size  = 2* bbs.size +3;
+    new_table = (BB**) CLG_MALLOC("cl.bb.rbt.1",
+                                  new_size * sizeof(BB*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < bbs.size; i++) {
+	if (bbs.table[i] == NULL) continue;
+ 
+	curr = bbs.table[i];
+	while (NULL != curr) {
+	    next = curr->next;
+
+	    new_idx = bb_hash_idx(curr->obj, curr->offset, new_size);
+
+	    curr->next = new_table[new_idx];
+	    new_table[new_idx] = curr;
+	    if (curr->next) {
+		conflicts1++;
+		if (curr->next->next)
+		    conflicts2++;
+	    }
+
+	    curr = next;
+	}
+    }
+
+    VG_(free)(bbs.table);
+
+
+    CLG_DEBUG(0, "Resize BB Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     bbs.size, new_size,
+	     bbs.entries, conflicts1, conflicts2);
+
+    bbs.size  = new_size;
+    bbs.table = new_table;
+    CLG_(stat).bb_hash_resizes++;
+}
+
+
+/**
+ * Allocate new BB structure (including space for event type list)
+ * Not initialized:
+ * - instr_len, cost_count, instr[]
+ */
+static BB* new_bb(obj_node* obj, PtrdiffT offset,
+		  UInt instr_count, UInt cjmp_count, Bool cjmp_inverted)
+{
+   BB* bb;
+   UInt idx, size;
+
+   /* check fill degree of bb hash table and resize if needed (>80%) */
+   bbs.entries++;
+   if (10 * bbs.entries / bbs.size > 8)
+       resize_bb_table();
+
+   size = sizeof(BB) + instr_count * sizeof(InstrInfo)
+                     + (cjmp_count+1) * sizeof(CJmpInfo);
+   bb = (BB*) CLG_MALLOC("cl.bb.nb.1", size);
+   VG_(memset)(bb, 0, size);
+
+   bb->obj        = obj;
+   bb->offset     = offset;
+   
+   bb->instr_count = instr_count;
+   bb->cjmp_count  = cjmp_count;
+   bb->cjmp_inverted = cjmp_inverted;
+   bb->jmp         = (CJmpInfo*) &(bb->instr[instr_count]);
+   bb->instr_len   = 0;
+   bb->cost_count  = 0;
+   bb->sect_kind   = VG_(DebugInfo_sect_kind)(NULL, offset + obj->offset);
+   bb->fn          = 0;
+   bb->line        = 0;
+   bb->is_entry    = 0;
+   bb->bbcc_list   = 0;
+   bb->last_bbcc   = 0;
+
+   /* insert into BB hash table */
+   idx = bb_hash_idx(obj, offset, bbs.size);
+   bb->next = bbs.table[idx];
+   bbs.table[idx] = bb;
+
+   CLG_(stat).distinct_bbs++;
+
+#if CLG_ENABLE_DEBUG
+   CLG_DEBUGIF(3) {
+     VG_(printf)("  new_bb (instr %u, jmps %u, inv %s) [now %d]: ",
+		 instr_count, cjmp_count,
+		 cjmp_inverted ? "yes":"no",
+		 CLG_(stat).distinct_bbs);
+      CLG_(print_bb)(0, bb);
+      VG_(printf)("\n");
+   }
+#endif
+
+   CLG_(get_fn_node)(bb);
+
+   return bb;
+}
+
+
+/* get the BB structure for a BB start address */
+static __inline__
+BB* lookup_bb(obj_node* obj, PtrdiffT offset)
+{
+    BB* bb;
+    Int idx;
+
+    idx = bb_hash_idx(obj, offset, bbs.size);
+    bb = bbs.table[idx];
+
+    while(bb) {
+      if ((bb->obj == obj) && (bb->offset == offset)) break;
+      bb = bb->next;
+    }
+
+    CLG_DEBUG(5, "  lookup_bb (Obj %s, off %#lx): %p\n",
+              obj->name, (UWord)offset, bb);
+    return bb;
+}
+
+static __inline__
+obj_node* obj_of_address(Addr addr)
+{
+  obj_node* obj;
+  DebugInfo* di;
+  PtrdiffT offset;
+
+  di = VG_(find_DebugInfo)(addr);
+  obj = CLG_(get_obj_node)( di );
+
+  /* Update symbol offset in object if remapped */
+  /* FIXME (or at least check this) 2008 Feb 19: 'offset' is
+     only correct for text symbols, not for data symbols */
+  offset = di ? VG_(DebugInfo_get_text_bias)(di):0;
+  if (obj->offset != offset) {
+      Addr start = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+
+      CLG_DEBUG(0, "Mapping changed for '%s': %#lx -> %#lx\n",
+		obj->name, obj->start, start);
+
+      /* Size should be the same, and offset diff == start diff */
+      CLG_ASSERT( obj->size == (di ? VG_(DebugInfo_get_text_size)(di) : 0) );
+      CLG_ASSERT( obj->start - start == obj->offset - offset );
+      obj->offset = offset;
+      obj->start = start;
+  }
+
+  return obj;
+}
+
+/* Get the BB structure for a BB start address.
+ * If the BB has to be created, the IRBB is needed to
+ * compute the event type list for costs, and seen_before is
+ * set to False. Otherwise, seen_before is set to True.
+ *
+ * BBs are never discarded. There are 2 cases where this function
+ * is called from CLG_(instrument)() and a BB already exists:
+ * - The instrumented version was removed from Valgrinds TT cache
+ * - The ELF object of the BB was unmapped and mapped again.
+ *   This involves a possibly different address, but is handled by
+ *   looking up a BB keyed by (obj_node, file offset).
+ *
+ * bbIn==0 is possible for artificial BB without real code.
+ * Such a BB is created when returning to an unknown function.
+ */
+BB* CLG_(get_bb)(Addr addr, IRSB* bbIn, /*OUT*/ Bool *seen_before)
+{
+  BB*   bb;
+  obj_node* obj;
+  UInt n_instrs, n_jmps;
+  Bool cjmp_inverted = False;
+
+  CLG_DEBUG(5, "+ get_bb(BB %#lx)\n", addr);
+
+  obj = obj_of_address(addr);
+  bb = lookup_bb(obj, addr - obj->offset);
+
+  n_instrs = 0;
+  n_jmps = 0;
+  CLG_(collectBlockInfo)(bbIn, &n_instrs, &n_jmps, &cjmp_inverted);
+
+  *seen_before = bb ? True : False;
+  if (*seen_before) {
+    if (bb->instr_count != n_instrs) {
+      VG_(message)(Vg_DebugMsg, 
+		   "ERROR: BB Retranslation Mismatch at BB %#lx\n", addr);
+      VG_(message)(Vg_DebugMsg,
+		   "  new: Obj %s, Off %#lx, BBOff %#lx, Instrs %u\n",
+		   obj->name, (UWord)obj->offset,
+		   addr - obj->offset, n_instrs);
+      VG_(message)(Vg_DebugMsg,
+		   "  old: Obj %s, Off %#lx, BBOff %#lx, Instrs %u\n",
+		   bb->obj->name, (UWord)bb->obj->offset,
+		   (UWord)bb->offset, bb->instr_count);
+      CLG_ASSERT(bb->instr_count == n_instrs );
+    }
+    CLG_ASSERT(bb->cjmp_count == n_jmps );
+    CLG_(stat).bb_retranslations++;
+
+    CLG_DEBUG(5, "- get_bb(BB %#lx): seen before.\n", addr);
+    return bb;
+  }
+
+  bb = new_bb(obj, addr - obj->offset, n_instrs, n_jmps, cjmp_inverted);
+
+  CLG_DEBUG(5, "- get_bb(BB %#lx)\n", addr);
+
+  return bb;
+}
+
+/* Delete the BB info for the bb with unredirected entry-point
+   address 'addr'. */
+void CLG_(delete_bb)(Addr addr)
+{
+    BB  *bb, *bp;
+    Int idx, size;
+
+    obj_node* obj = obj_of_address(addr);
+    PtrdiffT offset = addr - obj->offset;
+
+    idx = bb_hash_idx(obj, offset, bbs.size);
+    bb = bbs.table[idx];
+
+    /* bb points at the current bb under consideration, and bp is the
+       one before. */
+    bp = NULL;
+    while(bb) {
+      if ((bb->obj == obj) && (bb->offset == offset)) break;
+      bp = bb;
+      bb = bb->next;
+    }
+
+    if (bb == NULL) {
+	CLG_DEBUG(3, "  delete_bb (Obj %s, off %#lx): NOT FOUND\n",
+		  obj->name, (UWord)offset);
+
+	/* we didn't find it.
+	 * this happens when callgrinds instrumentation mode
+	 * was off at BB translation time, ie. no BB was created.
+	 */
+	return;
+    }
+
+    /* unlink it from hash table */
+
+    if (bp == NULL) {
+       /* we found the first one in the list. */
+       tl_assert(bb == bbs.table[idx]);
+       bbs.table[idx] = bb->next;
+    } else {
+       tl_assert(bb != bbs.table[idx]);
+       bp->next = bb->next;
+    }
+
+    CLG_DEBUG(3, "  delete_bb (Obj %s, off %#lx): %p, BBCC head: %p\n",
+	      obj->name, (UWord)offset, bb, bb->bbcc_list);
+
+    if (bb->bbcc_list == 0) {
+	/* can be safely deleted */
+
+	/* Fill the block up with junk and then free it, so we will
+	   hopefully get a segfault if it is used again by mistake. */
+	size = sizeof(BB)
+	    + bb->instr_count * sizeof(InstrInfo)
+	    + (bb->cjmp_count+1) * sizeof(CJmpInfo);
+	VG_(memset)( bb, 0xAA, size );
+	CLG_FREE(bb);
+	return;
+    }
+    CLG_DEBUG(3, "  delete_bb: BB in use, can not free!\n");
+}
diff --git a/sigrind/bbcc.c b/sigrind/bbcc.c
new file mode 100644
index 000000000..01e30531d
--- /dev/null
+++ b/sigrind/bbcc.c
@@ -0,0 +1,872 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                       bbcc.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+#include "pub_tool_threadstate.h"
+
+/*------------------------------------------------------------*/
+/*--- BBCC operations                                      ---*/
+/*------------------------------------------------------------*/
+
+#define N_BBCC_INITIAL_ENTRIES  10437
+
+/* BBCC table (key is BB/Context), per thread, resizable */
+bbcc_hash current_bbccs;
+
+void CLG_(init_bbcc_hash)(bbcc_hash* bbccs)
+{
+   Int i;
+
+   CLG_ASSERT(bbccs != 0);
+
+   bbccs->size    = N_BBCC_INITIAL_ENTRIES;
+   bbccs->entries = 0;
+   bbccs->table = (BBCC**) CLG_MALLOC("cl.bbcc.ibh.1",
+                                      bbccs->size * sizeof(BBCC*));
+
+   for (i = 0; i < bbccs->size; i++) bbccs->table[i] = NULL;
+}
+
+void CLG_(copy_current_bbcc_hash)(bbcc_hash* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size    = current_bbccs.size;
+  dst->entries = current_bbccs.entries;
+  dst->table   = current_bbccs.table;
+}
+
+bbcc_hash* CLG_(get_current_bbcc_hash)()
+{
+  return &current_bbccs;
+}
+
+void CLG_(set_current_bbcc_hash)(bbcc_hash* h)
+{
+  CLG_ASSERT(h != 0);
+
+  current_bbccs.size    = h->size;
+  current_bbccs.entries = h->entries;
+  current_bbccs.table   = h->table;
+}
+
+/*
+ * Zero all costs of a BBCC
+ */
+void CLG_(zero_bbcc)(BBCC* bbcc)
+{
+  Int i;
+
+  CLG_ASSERT(bbcc->cxt != 0);
+  CLG_DEBUG(1, "  zero_bbcc: BB %#lx, Cxt %u "
+	   "(fn '%s', rec %u)\n", 
+	   bb_addr(bbcc->bb),
+	   bbcc->cxt->base_number + bbcc->rec_index,
+	   bbcc->cxt->fn[0]->name,
+	   bbcc->rec_index);
+
+  if ((bbcc->ecounter_sum ==0) &&
+      (bbcc->ret_counter ==0)) return;
+
+  for(i=0;i <= bbcc->bb->cjmp_count;i++) {
+    bbcc->jmp[i].ecounter = 0;
+  }
+  bbcc->ecounter_sum = 0;
+  bbcc->ret_counter = 0;
+}
+
+
+
+void CLG_(forall_bbccs)(void (*func)(BBCC*))
+{
+  BBCC *bbcc, *bbcc2;
+  int i, j;
+	
+  for (i = 0; i < current_bbccs.size; i++) {
+    if ((bbcc=current_bbccs.table[i]) == NULL) continue;
+    while (bbcc) {
+      /* every bbcc should have a rec_array */
+      CLG_ASSERT(bbcc->rec_array != 0);
+
+      for(j=0;j<bbcc->cxt->fn[0]->separate_recursions;j++) {
+	if ((bbcc2 = bbcc->rec_array[j]) == 0) continue;
+
+	(*func)(bbcc2);
+      }
+      bbcc = bbcc->next;
+    }
+  }
+}
+
+
+/* All BBCCs for recursion level 0 are inserted into a
+ * thread specific hash table with key
+ * - address of BB structure (unique, as never freed)
+ * - current context (includes caller chain)
+ * BBCCs for other recursion levels are in bbcc->rec_array.
+ *
+ * The hash is used in setup_bb(), i.e. to find the cost
+ * counters to be changed in the execution of a BB.
+ */
+
+static __inline__
+UInt bbcc_hash_idx(BB* bb, Context* cxt, UInt size)
+{
+   CLG_ASSERT(bb != 0);
+   CLG_ASSERT(cxt != 0);
+
+   return ((Addr)bb + (Addr)cxt) % size;
+}
+ 
+
+/* Lookup for a BBCC in hash.
+ */ 
+static
+BBCC* lookup_bbcc(BB* bb, Context* cxt)
+{
+   BBCC* bbcc = bb->last_bbcc;
+   UInt  idx;
+
+   /* check LRU */
+   if (bbcc->cxt == cxt) {
+       if (!CLG_(clo).separate_threads) {
+	   /* if we don't dump threads separate, tid doesn't have to match */
+	   return bbcc;
+       }
+       if (bbcc->tid == CLG_(current_tid)) return bbcc;
+   }
+
+   CLG_(stat).bbcc_lru_misses++;
+
+   idx = bbcc_hash_idx(bb, cxt, current_bbccs.size);
+   bbcc = current_bbccs.table[idx];
+   while (bbcc &&
+	  (bb      != bbcc->bb ||
+	   cxt     != bbcc->cxt)) {
+       bbcc = bbcc->next;
+   }
+   
+   CLG_DEBUG(2,"  lookup_bbcc(BB %#lx, Cxt %u, fn '%s'): %p (tid %u)\n",
+	    bb_addr(bb), cxt->base_number, cxt->fn[0]->name, 
+	    bbcc, bbcc ? bbcc->tid : 0);
+
+   CLG_DEBUGIF(2)
+     if (bbcc) CLG_(print_bbcc)(-2,bbcc);
+
+   return bbcc;
+}
+
+
+/* double size of hash table 1 (addr->BBCC) */
+static void resize_bbcc_hash(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    BBCC** new_table;
+    UInt new_idx;
+    BBCC *curr_BBCC, *next_BBCC;
+
+    new_size = 2*current_bbccs.size+3;
+    new_table = (BBCC**) CLG_MALLOC("cl.bbcc.rbh.1",
+                                    new_size * sizeof(BBCC*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < current_bbccs.size; i++) {
+	if (current_bbccs.table[i] == NULL) continue;
+ 
+	curr_BBCC = current_bbccs.table[i];
+	while (NULL != curr_BBCC) {
+	    next_BBCC = curr_BBCC->next;
+
+	    new_idx = bbcc_hash_idx(curr_BBCC->bb,
+				    curr_BBCC->cxt,
+				    new_size);
+
+	    curr_BBCC->next = new_table[new_idx];
+	    new_table[new_idx] = curr_BBCC;
+	    if (curr_BBCC->next) {
+		conflicts1++;
+		if (curr_BBCC->next->next)
+		    conflicts2++;
+	    }
+
+	    curr_BBCC = next_BBCC;
+	}
+    }
+
+    VG_(free)(current_bbccs.table);
+
+
+    CLG_DEBUG(0,"Resize BBCC Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     current_bbccs.size, new_size,
+	     current_bbccs.entries, conflicts1, conflicts2);
+
+    current_bbccs.size = new_size;
+    current_bbccs.table = new_table;
+    CLG_(stat).bbcc_hash_resizes++;
+}
+
+
+static __inline
+BBCC** new_recursion(int size)
+{
+    BBCC** bbccs;
+    int i;
+
+    bbccs = (BBCC**) CLG_MALLOC("cl.bbcc.nr.1", sizeof(BBCC*) * size);
+    for(i=0;i<size;i++)
+	bbccs[i] = 0;
+
+    CLG_DEBUG(3,"  new_recursion(size %d): %p\n", size, bbccs);
+
+    return bbccs;
+}
+  
+
+/*
+ * Allocate a new BBCC
+ *
+ * Uninitialized:
+ * cxt, rec_index, rec_array, next_bbcc, next1, next2
+ */
+static __inline__ 
+BBCC* new_bbcc(BB* bb)
+{
+   BBCC* bbcc;
+   Int i;
+
+   /* We need cjmp_count+1 JmpData structs:
+    * the last is for the unconditional jump/call/ret at end of BB
+    */
+   bbcc = (BBCC*)CLG_MALLOC("cl.bbcc.nb.1",
+			    sizeof(BBCC) +
+			    (bb->cjmp_count+1) * sizeof(JmpData));
+   bbcc->bb  = bb;
+   bbcc->tid = CLG_(current_tid);
+
+   bbcc->ret_counter = 0;
+   bbcc->skipped = 0;
+   for(i=0; i<=bb->cjmp_count; i++) {
+       bbcc->jmp[i].ecounter = 0;
+       bbcc->jmp[i].jcc_list = 0;
+   }
+   bbcc->ecounter_sum = 0;
+
+   /* Init pointer caches (LRU) */
+   bbcc->lru_next_bbcc = 0;
+   bbcc->lru_from_jcc  = 0;
+   bbcc->lru_to_jcc  = 0;
+   
+   CLG_(stat).distinct_bbccs++;
+
+   CLG_DEBUG(3, "  new_bbcc(BB %#lx): %p (now %d)\n",
+	    bb_addr(bb), bbcc, CLG_(stat).distinct_bbccs);
+
+   return bbcc;
+}
+
+
+/**
+ * Inserts a new BBCC into hashes.
+ * BBCC specific items must be set as this is used for the hash
+ * keys:
+ *  fn     : current function
+ *  tid    : current thread ID
+ *  from   : position where current function is called from
+ *
+ * Recursion level doesn't need to be set as this is not included
+ * in the hash key: Only BBCCs with rec level 0 are in hashes.
+ */
+static
+void insert_bbcc_into_hash(BBCC* bbcc)
+{
+    UInt idx;
+    
+    CLG_ASSERT(bbcc->cxt != 0);
+
+    CLG_DEBUG(3,"+ insert_bbcc_into_hash(BB %#lx, fn '%s')\n",
+	     bb_addr(bbcc->bb), bbcc->cxt->fn[0]->name);
+
+    /* check fill degree of hash and resize if needed (>90%) */
+    current_bbccs.entries++;
+    if (100 * current_bbccs.entries / current_bbccs.size > 90)
+	resize_bbcc_hash();
+
+    idx = bbcc_hash_idx(bbcc->bb, bbcc->cxt, current_bbccs.size);
+    bbcc->next = current_bbccs.table[idx];
+    current_bbccs.table[idx] = bbcc;
+
+    CLG_DEBUG(3,"- insert_bbcc_into_hash: %u entries\n",
+	     current_bbccs.entries);
+}
+
+/* String is returned in a dynamically allocated buffer. Caller is
+   responsible for free'ing it. */
+static HChar* mangled_cxt(const Context* cxt, Int rec_index)
+{
+    Int i, p;
+
+    if (!cxt) return VG_(strdup)("cl.bbcc.mcxt", "(no context)");
+
+    /* Overestimate the number of bytes we need to hold the string. */
+    SizeT need = 20;   // rec_index + nul-terminator
+    for (i = 0; i < cxt->size; ++i)
+       need += VG_(strlen)(cxt->fn[i]->name) + 1;   // 1 for leading '
+
+    HChar *mangled = CLG_MALLOC("cl.bbcc.mcxt", need);
+    p = VG_(sprintf)(mangled, "%s", cxt->fn[0]->name);
+    if (rec_index >0)
+	p += VG_(sprintf)(mangled+p, "'%d", rec_index +1);
+    for(i=1;i<cxt->size;i++)
+	p += VG_(sprintf)(mangled+p, "'%s", cxt->fn[i]->name);
+
+    return mangled;
+}
+
+
+/* Create a new BBCC as a copy of an existing one,
+ * but with costs set to 0 and jcc chains empty.
+ *
+ * This is needed when a BB is executed in another context than
+ * the one at instrumentation time of the BB.
+ *
+ * Use cases:
+ *  rec_index == 0: clone from a BBCC with differing tid/cxt
+ *                  and insert into hashes
+ *  rec_index >0  : clone from a BBCC with same tid/cxt and rec_index 0
+ *                  don't insert into hashes
+ */
+static BBCC* clone_bbcc(BBCC* orig, Context* cxt, Int rec_index)
+{
+    BBCC* bbcc;
+
+    CLG_DEBUG(3,"+ clone_bbcc(BB %#lx, rec %d, fn %s)\n",
+	     bb_addr(orig->bb), rec_index, cxt->fn[0]->name);
+
+    bbcc = new_bbcc(orig->bb);
+
+    if (rec_index == 0) {
+
+      /* hash insertion is only allowed if tid or cxt is different */
+      CLG_ASSERT((orig->tid != CLG_(current_tid)) ||
+		(orig->cxt != cxt));
+
+      bbcc->rec_index = 0;
+      bbcc->cxt = cxt;
+      bbcc->rec_array = new_recursion(cxt->fn[0]->separate_recursions);
+      bbcc->rec_array[0] = bbcc;
+
+      insert_bbcc_into_hash(bbcc);
+    }
+    else {
+      if (CLG_(clo).separate_threads)
+	CLG_ASSERT(orig->tid == CLG_(current_tid));
+
+      CLG_ASSERT(orig->cxt == cxt);
+      CLG_ASSERT(orig->rec_array);
+      CLG_ASSERT(cxt->fn[0]->separate_recursions > rec_index);
+      CLG_ASSERT(orig->rec_array[rec_index] ==0);
+
+      /* new BBCC will only have differing recursion level */
+      bbcc->rec_index = rec_index;
+      bbcc->cxt = cxt;
+      bbcc->rec_array = orig->rec_array;
+      bbcc->rec_array[rec_index] = bbcc;
+    }
+
+    /* update list of BBCCs for same BB */
+    bbcc->next_bbcc = orig->bb->bbcc_list;
+    orig->bb->bbcc_list = bbcc;
+
+
+    CLG_DEBUGIF(3)
+      CLG_(print_bbcc)(-2, bbcc);
+
+    HChar *mangled_orig = mangled_cxt(orig->cxt, orig->rec_index);
+    HChar *mangled_bbcc = mangled_cxt(bbcc->cxt, bbcc->rec_index);
+    CLG_DEBUG(2,"- clone_BBCC(%p, %d) for BB %#lx\n"
+		"   orig %s\n"
+		"   new  %s\n",
+	     orig, rec_index, bb_addr(orig->bb),
+             mangled_orig,
+             mangled_bbcc);
+    CLG_FREE(mangled_orig);
+    CLG_FREE(mangled_bbcc);
+
+    CLG_(stat).bbcc_clones++;
+ 
+    return bbcc;
+};
+
+
+
+/* Get a pointer to the cost centre structure for given basic block
+ * address. If created, the BBCC is inserted into the BBCC hash.
+ * Also sets BB_seen_before by reference.
+ *
+ */ 
+BBCC* CLG_(get_bbcc)(BB* bb)
+{
+   BBCC* bbcc;
+
+   CLG_DEBUG(3, "+ get_bbcc(BB %#lx)\n", bb_addr(bb));
+
+   bbcc = bb->bbcc_list;
+
+   if (!bbcc) {
+     bbcc = new_bbcc(bb);
+
+     /* initialize BBCC */
+     bbcc->cxt       = 0;
+     bbcc->rec_array = 0;
+     bbcc->rec_index = 0;
+
+     bbcc->next_bbcc = bb->bbcc_list;
+     bb->bbcc_list = bbcc;
+     bb->last_bbcc = bbcc;
+
+     CLG_DEBUGIF(3)
+       CLG_(print_bbcc)(-2, bbcc);
+   }
+
+   CLG_DEBUG(3, "- get_bbcc(BB %#lx): BBCC %p\n",
+		bb_addr(bb), bbcc);
+
+   return bbcc;
+}
+
+
+/* Callgrind manages its own call stack for each thread.
+ * When leaving a function, a underflow can happen when
+ * Callgrind's tracing was switched on in the middle of
+ * a run, i.e. when Callgrind was not able to trace the
+ * call instruction.
+ * This function tries to reconstruct the original call.
+ * As we know the return address (the address following
+ * the CALL instruction), we can detect the function
+ * we return back to, but the original call site is unknown.
+ * We suppose a call site at return address - 1.
+ * (TODO: other heuristic: lookup info of instrumented BBs).
+ */
+static void handleUnderflow(BB* bb)
+{
+  /* RET at top of call stack */
+  BBCC* source_bbcc;
+  BB* source_bb;
+  Bool seen_before;
+  fn_node* caller;
+  int fn_number;
+  unsigned *pactive;
+
+  CLG_DEBUG(1,"  Callstack underflow !\n");
+
+  /* we emulate an old call from the function we return to
+   * by using (<return address> -1) */
+  source_bb = CLG_(get_bb)(bb_addr(bb)-1, 0, &seen_before);
+  source_bbcc = CLG_(get_bbcc)(source_bb);
+
+  /* seen_before can be true if RET from a signal handler */
+  if (!seen_before) {
+    source_bbcc->ecounter_sum = CLG_(current_state).collect ? 1 : 0;
+  }
+  else if (CLG_(current_state).collect)
+    source_bbcc->ecounter_sum++;
+  
+  /* Force a new top context, will be set active by push_cxt() */
+  CLG_(current_fn_stack).top--;
+  CLG_(current_state).cxt = 0;
+  caller = CLG_(get_fn_node)(bb);
+  CLG_(push_cxt)( caller );
+
+  if (!seen_before) {
+    /* set rec array for source BBCC: this is at rec level 1 */
+    source_bbcc->rec_array = new_recursion(caller->separate_recursions);
+    source_bbcc->rec_array[0] = source_bbcc;
+
+    CLG_ASSERT(source_bbcc->cxt == 0);
+    source_bbcc->cxt = CLG_(current_state).cxt;
+    insert_bbcc_into_hash(source_bbcc);
+  }
+  CLG_ASSERT(CLG_(current_state).bbcc);
+
+  /* correct active counts */
+  fn_number = CLG_(current_state).bbcc->cxt->fn[0]->number;
+  pactive = CLG_(get_fn_entry)(fn_number);
+  (*pactive)--;
+
+  /* This assertion is not correct for reentrant
+   * signal handlers */
+  /* CLG_ASSERT(*pactive == 0); */
+
+  CLG_(current_state).nonskipped = 0; /* we didn't skip this function */
+  /* back to current context */
+  CLG_(push_cxt)( CLG_(current_state).bbcc->cxt->fn[0] );
+  CLG_(push_call_stack)(source_bbcc, 0, CLG_(current_state).bbcc,
+		       (Addr)-1, False);
+}
+
+
+/*
+ * Helper function called at start of each instrumented BB to setup
+ * pointer to costs for current thread/context/recursion level
+ */
+
+VG_REGPARM(1)
+void CLG_(setup_bbcc)(BB* bb)
+{
+  BBCC *bbcc, *last_bbcc;
+  Bool  call_emulation = False, delayed_push = False, skip = False;
+  Addr sp;
+  BB* last_bb;
+  ThreadId tid;
+  ClgJumpKind jmpkind;
+  Bool isConditionalJump;
+  Int passed = 0, csp;
+  Bool ret_without_call = False;
+  Int popcount_on_return = 1;
+
+  CLG_DEBUG(3,"+ setup_bbcc(BB %#lx)\n", bb_addr(bb));
+
+  /* This is needed because thread switches can not reliable be tracked
+   * with callback CLG_(run_thread) only: we have otherwise no way to get
+   * the thread ID after a signal handler returns.
+   * This could be removed again if that bug is fixed in Valgrind.
+   * This is in the hot path but hopefully not to costly.
+   */
+  tid = VG_(get_running_tid)();
+#if 1
+  /* CLG_(switch_thread) is a no-op when tid is equal to CLG_(current_tid).
+   * As this is on the hot path, we only call CLG_(switch_thread)(tid)
+   * if tid differs from the CLG_(current_tid).
+   */
+  if (UNLIKELY(tid != CLG_(current_tid)))
+  {
+     CLG_(switch_thread)(tid);
+  }
+#else
+  CLG_ASSERT(VG_(get_running_tid)() == CLG_(current_tid));
+#endif
+
+  sp = VG_(get_SP)(tid);
+  last_bbcc = CLG_(current_state).bbcc;
+  last_bb = last_bbcc ? last_bbcc->bb : 0;
+
+  if (last_bb) {
+      passed = CLG_(current_state).jmps_passed;
+      CLG_ASSERT(passed <= last_bb->cjmp_count);
+      jmpkind = last_bb->jmp[passed].jmpkind;
+      isConditionalJump = (passed < last_bb->cjmp_count);
+
+      CLG_DEBUGIF(4) {
+      }
+  }
+  else {
+      jmpkind = jk_None;
+      isConditionalJump = False;
+  }
+
+  /* Manipulate JmpKind if needed, only using BB specific info */
+
+  csp = CLG_(current_call_stack).sp;
+
+  /* A return not matching the top call in our callstack is a jump */
+  if ( (jmpkind == jk_Return) && (csp >0)) {
+      Int csp_up = csp-1;      
+      call_entry* top_ce = &(CLG_(current_call_stack).entry[csp_up]);
+
+      /* We have a real return if
+       * - the stack pointer (SP) left the current stack frame, or
+       * - SP has the same value as when reaching the current function
+       *   and the address of this BB is the return address of last call
+       *   (we even allow to leave multiple frames if the SP stays the
+       *    same and we find a matching return address)
+       * The latter condition is needed because on PPC, SP can stay
+       * the same over CALL=b(c)l / RET=b(c)lr boundaries
+       */
+      if (sp < top_ce->sp) popcount_on_return = 0;
+      else if (top_ce->sp == sp) {
+	  while(1) {
+	      if (top_ce->ret_addr == bb_addr(bb)) break;
+	      if (csp_up>0) {
+		  csp_up--;
+		  top_ce = &(CLG_(current_call_stack).entry[csp_up]);
+		  if (top_ce->sp == sp) {
+		      popcount_on_return++;
+		      continue; 
+		  }
+	      }
+	      popcount_on_return = 0;
+	      break;
+	  }
+      }
+      if (popcount_on_return == 0) {
+	  jmpkind = jk_Jump;
+	  ret_without_call = True;
+      }
+  }
+
+  /* Should this jump be converted to call or pop/call ? */
+  if (( jmpkind != jk_Return) &&
+      ( jmpkind != jk_Call) && last_bb) {
+
+    /* We simulate a JMP/Cont to be a CALL if
+     * - jump is in another ELF object or section kind
+     * - jump is to first instruction of a function (tail recursion)
+     */
+    if (ret_without_call ||
+	/* This is for detection of optimized tail recursion.
+	 * On PPC, this is only detected as call when going to another
+	 * function. The problem is that on PPC it can go wrong
+	 * more easily (no stack frame setup needed)
+	 */
+#if defined(VGA_ppc32)
+	(bb->is_entry && (last_bb->fn != bb->fn)) ||
+#else
+	bb->is_entry ||
+#endif
+	(last_bb->sect_kind != bb->sect_kind) ||
+	(last_bb->obj->number != bb->obj->number)) {
+
+	CLG_DEBUG(1,"     JMP: %s[%s] to %s[%s]%s!\n",
+		  last_bb->fn->name, last_bb->obj->name,
+		  bb->fn->name, bb->obj->name,
+		  ret_without_call?" (RET w/o CALL)":"");
+
+	if (CLG_(get_fn_node)(last_bb)->pop_on_jump && (csp>0)) {
+
+	    call_entry* top_ce = &(CLG_(current_call_stack).entry[csp-1]);
+	    
+	    if (top_ce->jcc) {
+
+		CLG_DEBUG(1,"     Pop on Jump!\n");
+
+		/* change source for delayed push */
+		CLG_(current_state).bbcc = top_ce->jcc->from;
+		sp = top_ce->sp;
+		passed = top_ce->jcc->jmp;
+		CLG_(pop_call_stack)();
+	    }
+	    else {
+		CLG_ASSERT(CLG_(current_state).nonskipped != 0);
+	    }
+	}
+
+	jmpkind = jk_Call;
+	call_emulation = True;
+    }
+  }
+
+  if (jmpkind == jk_Call)
+    skip = CLG_(get_fn_node)(bb)->skip;
+
+  CLG_DEBUGIF(1) {
+    if (isConditionalJump)
+      VG_(printf)("Cond-");
+    switch(jmpkind) {
+    case jk_None:   VG_(printf)("Fall-through"); break;
+    case jk_Jump:   VG_(printf)("Jump"); break;
+    case jk_Call:   VG_(printf)("Call"); break;
+    case jk_Return: VG_(printf)("Return"); break;
+    default:        tl_assert(0);
+    }
+    VG_(printf)(" %08lx -> %08lx, SP %08lx\n",
+		last_bb ? bb_jmpaddr(last_bb) : 0,
+		bb_addr(bb), sp);
+  }
+
+  /* Handle CALL/RET and update context to get correct BBCC */
+  
+  if (jmpkind == jk_Return) {
+    
+    if ((csp == 0) || 
+	((CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom) &&
+	 ( *(CLG_(current_fn_stack).top-1)==0)) ) {
+
+      /* On an empty call stack or at a signal separation marker,
+       * a RETURN generates an call stack underflow.
+       */	
+      handleUnderflow(bb);
+      CLG_(pop_call_stack)();
+    }
+    else {
+	CLG_ASSERT(popcount_on_return >0);
+	CLG_(unwind_call_stack)(sp, popcount_on_return);
+    }
+  }
+  else {
+    Int unwind_count = CLG_(unwind_call_stack)(sp, 0);
+    if (unwind_count > 0) {
+      /* if unwinding was done, this actually is a return */
+      jmpkind = jk_Return;
+    }
+    
+    if (jmpkind == jk_Call) {
+      delayed_push = True;
+
+      csp = CLG_(current_call_stack).sp;
+      if (call_emulation && csp>0)
+	sp = CLG_(current_call_stack).entry[csp-1].sp;	
+
+    }
+  }
+  
+  /* Change new context if needed, taking delayed_push into account */
+  if ((delayed_push && !skip) || (CLG_(current_state).cxt == 0)) {
+    CLG_(push_cxt)(CLG_(get_fn_node)(bb));
+  }
+  CLG_ASSERT(CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom);
+  
+  /* If there is a fresh instrumented BBCC, assign current context */
+  bbcc = CLG_(get_bbcc)(bb);
+  if (bbcc->cxt == 0) {
+    CLG_ASSERT(bbcc->rec_array == 0);
+      
+    bbcc->cxt = CLG_(current_state).cxt;
+    bbcc->rec_array = 
+      new_recursion((*CLG_(current_fn_stack).top)->separate_recursions);
+    bbcc->rec_array[0] = bbcc;
+      
+    insert_bbcc_into_hash(bbcc);
+  }
+  else {
+    /* get BBCC with current context */
+    
+    /* first check LRU of last bbcc executed */
+    
+    if (last_bbcc) {
+      bbcc = last_bbcc->lru_next_bbcc;
+      if (bbcc &&
+	  ((bbcc->bb != bb) ||
+	   (bbcc->cxt != CLG_(current_state).cxt)))
+	bbcc = 0;
+    }
+    else
+      bbcc = 0;
+
+    if (!bbcc)
+      bbcc = lookup_bbcc(bb, CLG_(current_state).cxt);
+    if (!bbcc)
+      bbcc = clone_bbcc(bb->bbcc_list, CLG_(current_state).cxt, 0);
+    
+    bb->last_bbcc = bbcc;
+  }
+
+  /* save for fast lookup */
+  if (last_bbcc)
+    last_bbcc->lru_next_bbcc = bbcc;
+
+  if ((*CLG_(current_fn_stack).top)->separate_recursions >1) {
+    UInt level, idx;
+    fn_node* top = *(CLG_(current_fn_stack).top);
+
+    level = *CLG_(get_fn_entry)(top->number);
+
+    if (delayed_push && !skip) {
+      if (CLG_(clo).skip_direct_recursion) {
+        /* a call was detected, which means that the source BB != 0 */
+	CLG_ASSERT(CLG_(current_state).bbcc != 0);
+	/* only increment rec. level if called from different function */ 
+	if (CLG_(current_state).bbcc->cxt->fn[0] != bbcc->cxt->fn[0])
+	  level++;
+      }
+      else level++;
+    }
+    if (level> top->separate_recursions)
+      level = top->separate_recursions;
+
+    if (level == 0) {
+      /* can only happen if instrumentation just was switched on */
+      level = 1;
+      *CLG_(get_fn_entry)(top->number) = 1;
+    }
+
+    idx = level -1;
+    if (bbcc->rec_array[idx])
+      bbcc = bbcc->rec_array[idx];
+    else
+      bbcc = clone_bbcc(bbcc, CLG_(current_state).cxt, idx);
+
+    CLG_ASSERT(bbcc->rec_array[bbcc->rec_index] == bbcc);
+  }
+
+  if (delayed_push) {
+    if (!skip && CLG_(current_state).nonskipped) {
+      /* a call from skipped to nonskipped */
+      CLG_(current_state).bbcc = CLG_(current_state).nonskipped;
+      /* FIXME: take the real passed count from shadow stack */
+      passed = CLG_(current_state).bbcc->bb->cjmp_count;
+    }
+    CLG_(push_call_stack)(CLG_(current_state).bbcc, passed,
+			 bbcc, sp, skip);
+  }
+
+  if (CLG_(clo).collect_jumps && (jmpkind == jk_Jump)) {
+    
+    /* Handle conditional jumps followed, i.e. trace arcs
+     * This uses JCC structures, too */
+    
+    jCC* jcc = CLG_(get_jcc)(last_bbcc, passed, bbcc);
+    CLG_ASSERT(jcc != 0);
+    // Change from default, and check if already changed
+    if (jcc->jmpkind == jk_Call)
+      jcc->jmpkind = isConditionalJump ? jk_CondJump : jk_Jump;
+    else {
+	// FIXME: Why can this fail?
+	// CLG_ASSERT(jcc->jmpkind == jmpkind);
+    }
+    
+    jcc->call_counter++;
+    if (isConditionalJump)
+      CLG_(stat).jcnd_counter++;
+    else
+      CLG_(stat).jump_counter++;
+  }
+  
+  CLG_(current_state).bbcc = bbcc;
+  /* Even though this will be set in instrumented code directly before
+   * side exits, it needs to be set to 0 here in case an exception
+   * happens in first instructions of the BB */
+  CLG_(current_state).jmps_passed = 0;
+  
+  CLG_DEBUGIF(1) {
+    VG_(printf)("     ");
+    CLG_(print_bbcc_fn)(bbcc);
+    VG_(printf)("\n");
+  }
+  
+  CLG_DEBUG(3,"- setup_bbcc (BB %#lx): Cost %p (Len %u), Instrs %u (Len %u)\n",
+	   bb_addr(bb), bbcc->cost, bb->cost_count, 
+	   bb->instr_count, bb->instr_len);
+  CLG_DEBUGIF(3)
+    CLG_(print_cxt)(-8, CLG_(current_state).cxt, bbcc->rec_index);
+  CLG_DEBUG(3,"\n");
+  
+  CLG_(stat).bb_executions++;
+}
diff --git a/sigrind/callgrind.h b/sigrind/callgrind.h
new file mode 100644
index 000000000..6d980ad0c
--- /dev/null
+++ b/sigrind/callgrind.h
@@ -0,0 +1,363 @@
+
+/*
+   ----------------------------------------------------------------
+
+   Notice that the following BSD-style license applies to this one
+   file (callgrind.h) only.  The rest of Valgrind is licensed under the
+   terms of the GNU General Public License, version 2, unless
+   otherwise indicated.  See the COPYING file in the source
+   distribution for details.
+
+   ----------------------------------------------------------------
+
+   This file is part of callgrind, a valgrind tool for cache simulation
+   and call tree tracing.
+
+   Copyright (C) 2003-2015 Josef Weidendorfer.  All rights reserved.
+
+   Redistribution and use in source and binary forms, with or without
+   modification, are permitted provided that the following conditions
+   are met:
+
+   1. Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+
+   2. The origin of this software must not be misrepresented; you must
+      not claim that you wrote the original software.  If you use this
+      software in a product, an acknowledgment in the product
+      documentation would be appreciated but is not required.
+
+   3. Altered source versions must be plainly marked as such, and must
+      not be misrepresented as being the original software.
+
+   4. The name of the author may not be used to endorse or promote
+      products derived from this software without specific prior written
+      permission.
+
+   THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
+   OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+   ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
+   DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+   DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
+   GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+   INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+   WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+   ----------------------------------------------------------------
+
+   Notice that the above BSD-style license applies to this one file
+   (callgrind.h) only.  The entire rest of Valgrind is licensed under
+   the terms of the GNU General Public License, version 2.  See the
+   COPYING file in the source distribution for details.
+
+   ----------------------------------------------------------------
+*/
+
+#ifndef SIGRIND_H
+#define SIGRIND_H
+
+#include "valgrind.h"
+
+/* !! ABIWARNING !! ABIWARNING !! ABIWARNING !! ABIWARNING !!
+   This enum comprises an ABI exported by Valgrind to programs
+   which use client requests.  DO NOT CHANGE THE ORDER OF THESE
+   ENTRIES, NOR DELETE ANY -- add new ones at the end.
+
+   The identification ('C','T') for Callgrind has historical
+   reasons: it was called "Calltree" before. Besides, ('C','G') would
+   clash with cachegrind.
+ */
+
+typedef
+   enum {
+      VG_USERREQ__DUMP_STATS = VG_USERREQ_TOOL_BASE('C','T'),
+      VG_USERREQ__ZERO_STATS,
+      VG_USERREQ__TOGGLE_COLLECT,
+      VG_USERREQ__DUMP_STATS_AT,
+      VG_USERREQ__START_INSTRUMENTATION,
+      VG_USERREQ__STOP_INSTRUMENTATION,
+
+      VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE,
+
+      VG_USERREQ__SIGIL_GOMP_LOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER,
+      VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE
+   } Vg_CallgrindClientRequest;
+
+/* Dump current state of cost centers, and zero them afterwards */
+#define CALLGRIND_DUMP_STATS                                    \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__DUMP_STATS,       \
+                                  0, 0, 0, 0, 0)
+
+/* Dump current state of cost centers, and zero them afterwards.
+   The argument is appended to a string stating the reason which triggered
+   the dump. This string is written as a description field into the
+   profile data dump. */
+#define CALLGRIND_DUMP_STATS_AT(pos_str)                        \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__DUMP_STATS_AT,    \
+                                  pos_str, 0, 0, 0, 0)
+
+/* Zero cost centers */
+#define CALLGRIND_ZERO_STATS                                    \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__ZERO_STATS,       \
+                                  0, 0, 0, 0, 0)
+
+/* Toggles collection state.
+   The collection state specifies whether the happening of events
+   should be noted or if they are to be ignored. Events are noted
+   by increment of counters in a cost center */
+#define CALLGRIND_TOGGLE_COLLECT                                \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__TOGGLE_COLLECT,   \
+                                  0, 0, 0, 0, 0)
+
+/* Start full callgrind instrumentation if not already switched on.
+   When cache simulation is done, it will flush the simulated cache;
+   this will lead to an artifical cache warmup phase afterwards with
+   cache misses which would not have happened in reality. */
+#define CALLGRIND_START_INSTRUMENTATION                              \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__START_INSTRUMENTATION, \
+                                  0, 0, 0, 0, 0)
+
+/* Stop full callgrind instrumentation if not already switched off.
+   This flushes Valgrinds translation cache, and does no additional
+   instrumentation afterwards, which effectivly will run at the same
+   speed as the "none" tool (ie. at minimal slowdown).
+   Use this to bypass Callgrind aggregation for uninteresting code parts.
+   To start Callgrind in this mode to ignore the setup phase, use
+   the option "--instr-atstart=no". */
+#define CALLGRIND_STOP_INSTRUMENTATION                               \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__STOP_INSTRUMENTATION,  \
+                                  0, 0, 0, 0, 0)
+
+/*------------------------------------------------------------*/
+/*---  Sigil handling for synchronization sys calls        ---*/
+/*------------------------------------------------------------*/
+#define SIGIL_PTHREAD_CREATE_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CREATE_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_JOIN_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_JOIN_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_LOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_LOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_UNLOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_UNLOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDWAIT_ENTER(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER,     \
+                                  cond, mtx, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDWAIT_LEAVE(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE,     \
+                                  cond, mtx, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDSIG_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDSIG_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDBROAD_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDBROAD_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_SPINLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_SPINLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_SPINUNLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_SPINUNLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_LOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_LOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define SIGIL_GOMP_LOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_UNLOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define SIGIL_GOMP_UNLOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_ATOMICSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_ATOMICSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_ATOMICEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_ATOMICEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITNAMESTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITNAMESTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITNAMEEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITNAMEEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_SETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_SETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_UNSETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_UNSETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_TEAMBARRIERWAIT_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+#endif /* __CALLGRIND_H */
diff --git a/sigrind/callstack.c b/sigrind/callstack.c
new file mode 100644
index 000000000..6f5184b3c
--- /dev/null
+++ b/sigrind/callstack.c
@@ -0,0 +1,425 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                               ct_callstack.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+#include "log_events.h"
+#include "Core/PrimitiveEnums.h"
+
+/*------------------------------------------------------------*/
+/*--- Call stack, operations                               ---*/
+/*------------------------------------------------------------*/
+
+/* Stack of current thread. Gets initialized when switching to 1st thread.
+ *
+ * The artificial call stack is an array of call_entry's, representing
+ * stack frames of the executing program. 
+ * Array call_stack and call_stack_esp have same size and grow on demand.
+ * Array call_stack_esp holds SPs of corresponding stack frames.
+ *
+ */
+
+#define N_CALL_STACK_INITIAL_ENTRIES 500
+
+call_stack CLG_(current_call_stack);
+
+void CLG_(init_call_stack)(call_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  s->size = N_CALL_STACK_INITIAL_ENTRIES;   
+  s->entry = (call_entry*) CLG_MALLOC("cl.callstack.ics.1",
+                                      s->size * sizeof(call_entry));
+  s->sp = 0;
+  s->entry[0].cxt = 0; /* for assertion in push_cxt() */
+}
+
+call_entry* CLG_(get_call_entry)(Int sp)
+{
+  CLG_ASSERT(sp <= CLG_(current_call_stack).sp);
+  return &(CLG_(current_call_stack).entry[sp]);
+}
+
+void CLG_(copy_current_call_stack)(call_stack* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size  = CLG_(current_call_stack).size;
+  dst->entry = CLG_(current_call_stack).entry;
+  dst->sp    = CLG_(current_call_stack).sp;
+}
+
+void CLG_(set_current_call_stack)(call_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  CLG_(current_call_stack).size  = s->size;
+  CLG_(current_call_stack).entry = s->entry;
+  CLG_(current_call_stack).sp    = s->sp;
+}
+
+
+static __inline__
+void ensure_stack_size(Int i)
+{
+  call_stack *cs = &CLG_(current_call_stack);
+
+  if (i < cs->size) return;
+
+  cs->size *= 2;
+  while (i > cs->size) cs->size *= 2;
+
+  cs->entry = (call_entry*) VG_(realloc)("cl.callstack.ess.1",
+                                         cs->entry,
+					 cs->size * sizeof(call_entry));
+
+  CLG_(stat).call_stack_resizes++;
+ 
+  CLG_DEBUGIF(2)
+    VG_(printf)("        call stack enlarged to %u entries\n",
+		CLG_(current_call_stack).size);
+}
+
+
+/* Called when function entered nonrecursive */
+static void function_entered(fn_node* fn)
+{
+  CLG_ASSERT(fn != 0);
+
+  if ( (SGL_(clo).collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Entering %s: turning on event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = True;
+
+    // let Sigil2 know which thread this function starts in
+    SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+  }
+  else if ( (SGL_(clo).start_collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).start_collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Entering %s: turning on event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = True;
+
+    // let Sigil2 know which thread this function starts in
+    SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+  }
+
+  /* send to sigil */
+  SGL_(log_fn_entry)(fn);
+
+#if CLG_ENABLE_DEBUG
+  if (fn->verbosity >=0) {
+    Int old = CLG_(clo).verbose;
+    CLG_(clo).verbose = fn->verbosity;
+    fn->verbosity = old;
+    VG_(message)(Vg_DebugMsg, 
+    "Entering %s: Verbosity set to %d\n",
+    fn->name, CLG_(clo).verbose);
+  }
+#endif
+}
+
+/* Called when function left (no recursive level active) */
+static void function_left(fn_node* fn)
+{
+  CLG_ASSERT(fn != 0);
+
+  /*send to sigil*/
+  SGL_(log_fn_leave)(fn);
+
+  if ( (SGL_(clo).collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Leaving %s: turning off event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = False;
+  }
+  else if ( (SGL_(clo).stop_collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).stop_collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Leaving %s: turning off event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = False;
+  }
+
+#if CLG_ENABLE_DEBUG
+  if (fn->verbosity >=0) {
+    Int old = CLG_(clo).verbose;
+    CLG_(clo).verbose = fn->verbosity;
+    fn->verbosity = old;
+    VG_(message)(Vg_DebugMsg, 
+    "Leaving %s: Verbosity set back to %d\n",
+    fn->name, CLG_(clo).verbose);
+  }
+#endif
+}
+
+
+/* Push call on call stack.
+ *
+ * Increment the usage count for the function called.
+ * A jump from <from> to <to>, with <sp>.
+ * If <skip> is true, this is a call to a function to be skipped;
+ * for this, we set jcc = 0.
+ */
+void CLG_(push_call_stack)(BBCC* from, UInt jmp, BBCC* to, Addr sp, Bool skip)
+{
+    jCC* jcc;
+    UInt* pdepth;
+    call_entry* current_entry;
+    Addr ret_addr;
+
+    /* Ensure a call stack of size <current_sp>+1.
+     * The +1 is needed as push_cxt will store the
+     * context at [current_sp]
+     */
+    ensure_stack_size(CLG_(current_call_stack).sp +1);
+    current_entry = &(CLG_(current_call_stack).entry[CLG_(current_call_stack).sp]);
+
+    if (skip) {
+	jcc = 0;
+    }
+    else {
+	fn_node* to_fn = to->cxt->fn[0];
+
+	if (CLG_(current_state).nonskipped) {
+	    /* this is a jmp from skipped to nonskipped */
+	    CLG_ASSERT(CLG_(current_state).nonskipped == from);
+	}
+
+	/* As push_cxt() has to be called before push_call_stack if not
+	 * skipping, the old context should already be saved on the stack */
+	CLG_ASSERT(current_entry->cxt != 0);
+
+	jcc = CLG_(get_jcc)(from, jmp, to);
+	CLG_ASSERT(jcc != 0);
+
+	pdepth = CLG_(get_fn_entry)(to_fn->number);
+	if (CLG_(clo).skip_direct_recursion) {
+	    /* only increment depth if another function is called */
+	  if (jcc->from->cxt->fn[0] != to_fn) (*pdepth)++;
+	}
+	else (*pdepth)++;
+
+	if (*pdepth>1)
+	  CLG_(stat).rec_call_counter++;
+	
+	jcc->call_counter++;
+	CLG_(stat).call_counter++;
+
+	if (*pdepth == 1) function_entered(to_fn);
+    }
+
+    /* return address is only is useful with a real call;
+     * used to detect RET w/o CALL */
+    if (from->bb->jmp[jmp].jmpkind == jk_Call) {
+      UInt instr = from->bb->jmp[jmp].instr;
+      ret_addr = bb_addr(from->bb) +
+	from->bb->instr[instr].instr_offset +
+	from->bb->instr[instr].instr_size;
+    }
+    else
+      ret_addr = 0;
+
+    /* put jcc on call stack */
+    current_entry->jcc = jcc;
+    current_entry->sp = sp;
+    current_entry->ret_addr = ret_addr;
+    current_entry->nonskipped = CLG_(current_state).nonskipped;
+
+    CLG_(current_call_stack).sp++;
+
+    /* To allow for above assertion we set context of next frame to 0 */
+    CLG_ASSERT(CLG_(current_call_stack).sp < CLG_(current_call_stack).size);
+    current_entry++;
+    current_entry->cxt = 0;
+
+    if (!skip)
+	CLG_(current_state).nonskipped = 0;
+    else if (!CLG_(current_state).nonskipped) {
+	/* a call from nonskipped to skipped */
+	CLG_(current_state).nonskipped = from;
+	if (!CLG_(current_state).nonskipped->skipped) {
+	  CLG_(stat).distinct_skips++;
+	}
+    }
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(0) {
+	if (CLG_(clo).verbose<2) {
+	  if (jcc && jcc->to && jcc->to->bb) {
+	    const HChar spaces[][41] = {
+                                  "   .   .   .   .   .   .   .   .   .   .",
+				  "  .   .   .   .   .   .   .   .   .   . ",
+				  " .   .   .   .   .   .   .   .   .   .  ",
+				  ".   .   .   .   .   .   .   .   .   .   " };
+
+	    int s = CLG_(current_call_stack).sp;
+	    UInt* pars = (UInt*) sp;
+
+	    BB* bb = jcc->to->bb;
+	    if (s>40) s=40;
+	    VG_(printf)("%s> %s(0x%x, 0x%x, ...) [%s / %#lx]\n", spaces[s%4]+40-s, bb->fn->name,
+                        pars ? pars[1]:0,
+			pars ? pars[2]:0,
+			bb->obj->name + bb->obj->last_slash_pos,
+			(UWord)bb->offset);
+	  }
+	}
+	else if (CLG_(clo).verbose<4) {
+	    VG_(printf)("+ %2d ", CLG_(current_call_stack).sp);
+	    VG_(printf)(", SP %#lx, RA %#lx\n", sp, ret_addr);
+	}
+	else {
+	    VG_(printf)("  Pushed ");
+	    CLG_(print_stackentry)(3, CLG_(current_call_stack).sp-1);
+	}
+    }
+#endif
+
+}
+
+
+/* Pop call stack and update inclusive sums.
+ * Returns modified fcc.
+ *
+ * If the JCC becomes inactive, call entries are freed if possible
+ */
+void CLG_(pop_call_stack)()
+{
+    jCC* jcc;
+    Int depth = 0;
+    call_entry* lower_entry;
+
+    if (CLG_(current_state).sig >0) {
+	/* Check if we leave a signal handler; this can happen when
+	 * calling longjmp() in the handler */
+	CLG_(run_post_signal_on_call_stack_bottom)();
+    }
+
+    lower_entry =
+	&(CLG_(current_call_stack).entry[CLG_(current_call_stack).sp-1]);
+
+    CLG_DEBUG(4,"+ pop_call_stack: frame %d, jcc %p\n", 
+		CLG_(current_call_stack).sp, lower_entry->jcc);
+
+    /* jCC item not any more on real stack: pop */
+    jcc = lower_entry->jcc;
+    CLG_(current_state).nonskipped = lower_entry->nonskipped;
+
+    if (jcc) {
+	fn_node* to_fn  = jcc->to->cxt->fn[0];
+	UInt* pdepth =  CLG_(get_fn_entry)(to_fn->number);
+	if (CLG_(clo).skip_direct_recursion) {
+	    /* only decrement depth if another function was called */
+	  if (jcc->from->cxt->fn[0] != to_fn) (*pdepth)--;
+	}
+	else (*pdepth)--;
+	depth = *pdepth;
+
+	/* restore context */
+	CLG_(current_state).cxt  = lower_entry->cxt;
+	CLG_(current_fn_stack).top =
+	  CLG_(current_fn_stack).bottom + lower_entry->fn_sp;
+	CLG_ASSERT(CLG_(current_state).cxt != 0);
+
+	if (depth == 0) function_left(to_fn);
+    }
+
+    /* To allow for an assertion in push_call_stack() */
+    lower_entry->cxt = 0;
+
+    CLG_(current_call_stack).sp--;
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(1) {
+	if (CLG_(clo).verbose<4) {
+	    if (jcc) {
+		/* popped JCC target first */
+		VG_(printf)("- %2d %#lx => ",
+			    CLG_(current_call_stack).sp,
+			    bb_addr(jcc->to->bb));
+		CLG_(print_addr)(bb_jmpaddr(jcc->from->bb));
+		VG_(printf)(", SP %#lx\n",
+			    CLG_(current_call_stack).entry[CLG_(current_call_stack).sp].sp);
+	    }
+	    else
+		VG_(printf)("- %2d [Skipped JCC], SP %#lx\n",
+			    CLG_(current_call_stack).sp,
+			    CLG_(current_call_stack).entry[CLG_(current_call_stack).sp].sp);
+	}
+	else {
+	    VG_(printf)("  Popped ");
+	    CLG_(print_stackentry)(7, CLG_(current_call_stack).sp);
+	    if (jcc) {
+		VG_(printf)("       returned to ");
+		CLG_(print_addr_ln)(bb_jmpaddr(jcc->from->bb));
+	    }
+	}
+    }
+#endif
+
+}
+
+
+/* Unwind enough CallStack items to sync with current stack pointer.
+ * Returns the number of stack frames unwinded.
+ */
+Int CLG_(unwind_call_stack)(Addr sp, Int minpops)
+{
+    Int csp;
+    Int unwind_count = 0;
+    CLG_DEBUG(4,"+ unwind_call_stack(sp %#lx, minpops %d): frame %d\n",
+	      sp, minpops, CLG_(current_call_stack).sp);
+
+    /* We pop old stack frames.
+     * For a call, be p the stack address with return address.
+     *  - call_stack_esp[] has SP after the CALL: p-4
+     *  - current sp is after a RET: >= p
+     */
+    
+    while( (csp=CLG_(current_call_stack).sp) >0) {
+	call_entry* top_ce = &(CLG_(current_call_stack).entry[csp-1]);
+
+	if ((top_ce->sp < sp) ||
+	    ((top_ce->sp == sp) && minpops>0)) {
+
+	    minpops--;
+	    unwind_count++;
+	    CLG_(pop_call_stack)();
+	    csp=CLG_(current_call_stack).sp;
+	    continue;
+	}
+	break;
+    }
+
+    CLG_DEBUG(4,"- unwind_call_stack\n");
+    return unwind_count;
+}
diff --git a/sigrind/clo.c b/sigrind/clo.c
new file mode 100644
index 000000000..d1d3e526e
--- /dev/null
+++ b/sigrind/clo.c
@@ -0,0 +1,687 @@
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains lot of code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "config.h" // for VG_PREFIX
+
+#include "global.h"
+
+
+
+/*------------------------------------------------------------*/
+/*--- Function specific configuration options              ---*/
+/*------------------------------------------------------------*/
+
+/* Special value for separate_callers: automatic = adaptive */
+#define CONFIG_AUTO    -1
+
+#define CONFIG_DEFAULT -1
+#define CONFIG_FALSE    0
+#define CONFIG_TRUE     1
+
+/* Logging configuration for a function */
+struct _fn_config {
+    Int dump_before;
+    Int dump_after;
+    Int zero_before;
+    Int toggle_collect;
+
+    Int skip;    /* Handle CALL to this function as JMP (= Skip)? */
+    Int group;   /* don't change caller dependency inside group !=0 */
+    Int pop_on_jump; 
+
+    Int separate_callers;    /* separate logging dependent on caller  */
+    Int separate_recursions; /* separate logging of rec. levels       */
+
+#if CLG_ENABLE_DEBUG
+    Int verbosity; /* Change debug verbosity level while in function */
+#endif
+};
+
+/* Configurations for function name prefix patterns.
+ * Currently, only very limit patterns are possible:
+ * Exact prefix patterns and "*::" are allowed.
+ * E.g.
+ *  - "abc" matches all functions starting with "abc".
+ *  - "abc*::def" matches all functions starting with "abc" and
+ *    starting with "def" after the first "::" separator.
+ *  - "*::print(" matches C++ methods "print" in all classes
+ *    without namespace. I.e. "*" doesn't match a "::".
+ *
+ * We build a trie from patterns, and for a given function, we
+ * go down the tree and apply all non-default configurations.
+ */
+
+
+#define NODE_DEGREE 30
+
+/* node of compressed trie search structure */
+typedef struct _config_node config_node;
+struct _config_node {
+  Int length;
+    
+  fn_config* config;
+  config_node* sub_node[NODE_DEGREE];
+  config_node* next;
+  config_node* wild_star;
+  config_node* wild_char;
+
+  HChar name[1];
+};
+
+/* root of trie */
+static config_node* fn_configs = 0;
+
+static __inline__ 
+fn_config* new_fnc(void)
+{
+   fn_config* fnc = (fn_config*) CLG_MALLOC("cl.clo.nf.1",
+                                            sizeof(fn_config));
+
+   fnc->dump_before  = CONFIG_DEFAULT;
+   fnc->dump_after   = CONFIG_DEFAULT;
+   fnc->zero_before  = CONFIG_DEFAULT;
+   fnc->toggle_collect = CONFIG_DEFAULT;
+   fnc->skip         = CONFIG_DEFAULT;
+   fnc->pop_on_jump  = CONFIG_DEFAULT;
+   fnc->group        = CONFIG_DEFAULT;
+   fnc->separate_callers    = CONFIG_DEFAULT;
+   fnc->separate_recursions = CONFIG_DEFAULT;
+
+#if CLG_ENABLE_DEBUG
+   fnc->verbosity    = CONFIG_DEFAULT;
+#endif
+
+   return fnc;
+}
+
+
+static config_node* new_config(const HChar* name, int length)
+{
+    int i;
+    config_node* node = (config_node*) CLG_MALLOC("cl.clo.nc.1",
+                                                  sizeof(config_node) + length);
+
+    for(i=0;i<length;i++) {
+      if (name[i] == 0) break;
+      node->name[i] = name[i];
+    }
+    node->name[i] = 0;
+
+    node->length = length;
+    node->config = 0;
+    for(i=0;i<NODE_DEGREE;i++)
+	node->sub_node[i] = 0;
+    node->next = 0;
+    node->wild_char = 0;
+    node->wild_star = 0;
+
+    CLG_DEBUG(3, "   new_config('%s', len %d)\n", node->name, length);
+
+    return node;
+}
+
+static __inline__
+Bool is_wild(HChar n)
+{
+  return (n == '*') || (n == '?');
+}
+
+/* Recursively build up function matching tree (prefix tree).
+ * Returns function config object for pattern <name>
+ * and starting at tree node <*pnode>.
+ *
+ * Tree nodes (config_node) are created as needed,
+ * tree root is stored into <*pnode>, and the created
+ * leaf (fn_config) for the given pattern is returned.
+ */
+static fn_config* get_fnc2(config_node* node, const HChar* name)
+{
+  config_node *new_sub, *n, *nprev;
+  int offset, len;
+
+  CLG_DEBUG(3, "  get_fnc2(%p, '%s')\n", node, name);
+
+  if (name[0] == 0) {
+    if (!node->config) node->config = new_fnc();
+    return node->config;
+  }
+
+  if (is_wild(*name)) {
+    if (*name == '*') {
+      while(name[1] == '*') name++;
+      new_sub = node->wild_star;
+    }
+    else
+      new_sub = node->wild_char;
+
+    if (!new_sub) {
+      new_sub = new_config(name, 1);
+      if (*name == '*')
+	node->wild_star = new_sub;
+      else
+	node->wild_char = new_sub;
+    }
+
+    return get_fnc2( new_sub, name+1);
+  }
+
+  n = node->sub_node[ name[0]%NODE_DEGREE ];
+  nprev = 0;
+  len = 0;
+  while(n) {
+    for(len=0; name[len] == n->name[len]; len++);
+    if (len>0) break;
+    nprev = n;
+    n = n->next;
+  }
+
+  if (!n) {
+    len = 1;
+    while(name[len] && (!is_wild(name[len]))) len++;
+    new_sub = new_config(name, len);
+    new_sub->next = node->sub_node[ name[0]%NODE_DEGREE ];
+    node->sub_node[ name[0]%NODE_DEGREE ] = new_sub;	
+
+    if (name[len] == 0) {
+      new_sub->config = new_fnc();
+      return new_sub->config;
+    }
+    
+    /* recurse on wildcard */
+    return get_fnc2( new_sub, name+len);
+  }
+
+  if (len < n->length) {
+
+    /* split up the subnode <n> */
+    config_node *new_node;
+    int i;
+
+    new_node = new_config(n->name, len);
+    if (nprev)
+      nprev->next = new_node;
+    else
+      node->sub_node[ n->name[0]%NODE_DEGREE ] = new_node;
+    new_node->next = n->next;
+
+    new_node->sub_node[ n->name[len]%NODE_DEGREE ] = n;
+
+    for(i=0, offset=len; offset < n->length; i++, offset++)
+      n->name[i] = n->name[offset];
+    n->name[i] = 0;
+    n->length = i;
+
+    name += len;
+    offset = 0;
+    while(name[offset] && (!is_wild(name[offset]))) offset++;
+    new_sub  = new_config(name, offset);
+    /* this sub_node of new_node could already be set: chain! */
+    new_sub->next = new_node->sub_node[ name[0]%NODE_DEGREE ];
+    new_node->sub_node[ name[0]%NODE_DEGREE ] = new_sub;
+
+    if (name[offset]==0) {
+      new_sub->config = new_fnc();
+      return new_sub->config;
+    }
+
+    /* recurse on wildcard */
+    return get_fnc2( new_sub, name+offset);
+  }
+
+  name += n->length;
+
+  if (name[0] == 0) {
+    /* name and node name are the same */
+    if (!n->config) n->config = new_fnc();
+    return n->config;
+  }
+
+  offset = 1;
+  while(name[offset] && (!is_wild(name[offset]))) offset++;
+
+  new_sub = new_config(name, offset);
+  new_sub->next = n->sub_node[ name[0]%NODE_DEGREE ];
+  n->sub_node[ name[0]%NODE_DEGREE ] = new_sub;
+
+  return get_fnc2(new_sub, name+offset);
+}
+
+static void print_config_node(int depth, int hash, config_node* node)
+{
+  config_node* n;
+  int i;
+
+  if (node != fn_configs) {
+    const HChar sp[] = "                                        ";
+
+    if (depth>40) depth=40;
+    VG_(printf)("%s", sp+40-depth);
+    if (hash >=0) VG_(printf)(" [hash %2d]", hash);
+    else if (hash == -2) VG_(printf)(" [wildc ?]");
+    else if (hash == -3) VG_(printf)(" [wildc *]");
+    VG_(printf)(" '%s' (len %d)\n", node->name, node->length);
+  }
+  for(i=0;i<NODE_DEGREE;i++) {
+    n = node->sub_node[i];
+    while(n) {
+      print_config_node(depth+1, i, n);
+      n = n->next;
+    }
+  }
+  if (node->wild_char) print_config_node(depth+1, -2, node->wild_char);
+  if (node->wild_star) print_config_node(depth+1, -3, node->wild_star);
+}
+
+/* get a function config for a name pattern (from command line) */
+static fn_config* get_fnc(const HChar* name)
+{
+  fn_config* fnc;
+
+  CLG_DEBUG(3, " +get_fnc(%s)\n", name);
+  if (fn_configs == 0)
+    fn_configs = new_config(name, 0);
+  fnc =  get_fnc2(fn_configs, name);
+
+  CLG_DEBUGIF(3) {
+    CLG_DEBUG(3, " -get_fnc(%s):\n", name);
+    print_config_node(3, -1, fn_configs);
+  }
+  return fnc;
+}
+
+  
+
+static void update_fn_config1(fn_node* fn, fn_config* fnc)
+{
+    if (fnc->dump_before != CONFIG_DEFAULT)
+	fn->dump_before = (fnc->dump_before == CONFIG_TRUE);
+
+    if (fnc->dump_after != CONFIG_DEFAULT)
+	fn->dump_after = (fnc->dump_after == CONFIG_TRUE);
+
+    if (fnc->zero_before != CONFIG_DEFAULT)
+	fn->zero_before = (fnc->zero_before == CONFIG_TRUE);
+
+    if (fnc->toggle_collect != CONFIG_DEFAULT)
+	fn->toggle_collect = (fnc->toggle_collect == CONFIG_TRUE);
+
+    if (fnc->skip != CONFIG_DEFAULT)
+	fn->skip = (fnc->skip == CONFIG_TRUE);
+
+    if (fnc->pop_on_jump != CONFIG_DEFAULT)
+	fn->pop_on_jump = (fnc->pop_on_jump == CONFIG_TRUE);
+
+    if (fnc->group != CONFIG_DEFAULT)
+	fn->group = fnc->group;
+
+    if (fnc->separate_callers != CONFIG_DEFAULT)
+	fn->separate_callers = fnc->separate_callers;
+
+    if (fnc->separate_recursions != CONFIG_DEFAULT)
+	fn->separate_recursions = fnc->separate_recursions;
+
+#if CLG_ENABLE_DEBUG
+    if (fnc->verbosity != CONFIG_DEFAULT)
+	fn->verbosity = fnc->verbosity;
+#endif
+}
+
+/* Recursively go down the function matching tree,
+ * looking for a match to <name>. For every matching leaf,
+ * <fn> is updated with the pattern config.
+ */
+static void update_fn_config2(fn_node* fn, const HChar* name,
+                              config_node* node)
+{
+    config_node* n;
+
+    CLG_DEBUG(3, "  update_fn_config2('%s', node '%s'): \n",
+	     name, node->name);
+    if ((*name == 0) && node->config) {
+      CLG_DEBUG(3, "   found!\n");
+      update_fn_config1(fn, node->config);
+      return;
+    }
+
+    n = node->sub_node[ name[0]%NODE_DEGREE ];
+    while(n) {
+      if (VG_(strncmp)(name, n->name, n->length)==0) break;
+      n = n->next;
+    }
+    if (n) {
+	CLG_DEBUG(3, "   '%s' matching at hash %d\n",
+		  n->name, name[0]%NODE_DEGREE);
+	update_fn_config2(fn, name+n->length, n);
+    }
+    
+    if (node->wild_char) {
+	CLG_DEBUG(3, "   skip '%c' for wildcard '?'\n", *name);
+	update_fn_config2(fn, name+1, node->wild_char);
+    }
+
+    if (node->wild_star) {
+      CLG_DEBUG(3, "   wildcard '*'\n");
+      while(*name) {
+	update_fn_config2(fn, name, node->wild_star);
+	name++;
+      }
+      update_fn_config2(fn, name, node->wild_star);
+    }
+}
+
+/* Update function config according to configs of name prefixes */
+void CLG_(update_fn_config)(fn_node* fn)
+{
+    CLG_DEBUG(3, "  update_fn_config('%s')\n", fn->name);
+    if (fn_configs)
+      update_fn_config2(fn, fn->name, fn_configs);
+}
+
+
+/*--------------------------------------------------------------------*/
+/*--- Command line processing                                      ---*/
+/*--------------------------------------------------------------------*/
+
+Bool CLG_(process_cmd_line_option)(const HChar* arg)
+{
+   const HChar* tmp_str;
+
+   /* XXX tmpdir should not be set by the end-user, only for Sigil2 use */
+   if      VG_STR_CLO(arg,  "--ipc-dir",    SGL_(clo).ipc_dir) {}
+   else if VG_STR_CLO(arg,  "--at-func",    SGL_(clo).collect_func) {}
+   else if VG_STR_CLO(arg,  "--start-func", SGL_(clo).start_collect_func) {}
+   else if VG_STR_CLO(arg,  "--stop-func",  SGL_(clo).stop_collect_func) {}
+   else if VG_BOOL_CLO(arg, "--gen-mem",    SGL_(clo).gen_mem) {}
+   else if VG_BOOL_CLO(arg, "--gen-comp",   SGL_(clo).gen_comp) {}
+   else if VG_BOOL_CLO(arg, "--gen-sync",   SGL_(clo).gen_sync) {}
+   else if VG_BOOL_CLO(arg, "--gen-instr",  SGL_(clo).gen_instr) {}
+   else if VG_BOOL_CLO(arg, "--gen-fn",     SGL_(clo).gen_fn) {}
+   else if VG_BOOL_CLO(arg, "--gen-cf",     SGL_(clo).gen_cf) {}
+   else if VG_BOOL_CLO(arg, "--gen-bb",     SGL_(clo).gen_bb) {}
+
+   /* XXX
+    * ML: leftover from Callgrind. Most of these should be left at defaults
+    * for Sigrind, except perhaps --separate callers depending on the application
+    */
+   else if VG_BOOL_CLO(arg, "--skip-plt", CLG_(clo).skip_plt) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-jumps", CLG_(clo).collect_jumps) {}
+   /* compatibility alias, deprecated option */
+   else if VG_BOOL_CLO(arg, "--trace-jump",    CLG_(clo).collect_jumps) {}
+
+   else if VG_BOOL_CLO(arg, "--combine-dumps", CLG_(clo).combine_dumps) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-atstart", CLG_(clo).collect_atstart) {}
+
+   else if VG_BOOL_CLO(arg, "--instr-atstart", CLG_(clo).instrument_atstart) {}
+
+   else if VG_BOOL_CLO(arg, "--separate-threads", CLG_(clo).separate_threads) {}
+
+   else if VG_BOOL_CLO(arg, "--compress-strings", CLG_(clo).compress_strings) {}
+   else if VG_BOOL_CLO(arg, "--compress-mangled", CLG_(clo).compress_mangled) {}
+   else if VG_BOOL_CLO(arg, "--compress-pos",     CLG_(clo).compress_pos) {}
+
+   else if VG_STR_CLO(arg, "--dump-before", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->dump_before = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--zero-before", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->zero_before = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--dump-after", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->dump_after = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--toggle-collect", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->toggle_collect = CONFIG_TRUE;
+       /* defaults to initial collection off */
+       CLG_(clo).collect_atstart = False;
+   }
+
+   else if VG_INT_CLO(arg, "--separate-recs", CLG_(clo).separate_recursions) {}
+
+   /* change handling of a jump between functions to ret+call */
+   else if VG_XACT_CLO(arg, "--pop-on-jump", CLG_(clo).pop_on_jump, True) {}
+   else if VG_STR_CLO( arg, "--pop-on-jump", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->pop_on_jump = CONFIG_TRUE;
+   }
+
+#if CLG_ENABLE_DEBUG
+   else if VG_INT_CLO(arg, "--ct-verbose", CLG_(clo).verbose) {}
+   else if VG_INT_CLO(arg, "--ct-vstart",  CLG_(clo).verbose_start) {}
+
+   else if VG_STREQN(12, arg, "--ct-verbose") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+12, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->verbosity = n;
+   }
+#endif
+
+   else if VG_XACT_CLO(arg, "--separate-callers=auto", 
+                            CLG_(clo).separate_callers, CONFIG_AUTO) {}
+   else if VG_INT_CLO( arg, "--separate-callers", 
+                            CLG_(clo).separate_callers) {}
+
+   else if VG_STREQN(10, arg, "--fn-group") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+10, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->group = n;
+   }
+
+   else if VG_STREQN(18, arg, "--separate-callers") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+18, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->separate_callers = n;
+   }
+
+   else if VG_STREQN(15, arg, "--separate-recs") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+15, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->separate_recursions = n;
+   }
+
+   else if VG_STR_CLO(arg, "--callgrind-out-file", CLG_(clo).out_format) {}
+
+   else if VG_BOOL_CLO(arg, "--mangle-names", CLG_(clo).mangle_names) {}
+
+   else if VG_BOOL_CLO(arg, "--skip-direct-rec",
+                            CLG_(clo).skip_direct_recursion) {}
+
+   else if VG_BOOL_CLO(arg, "--dump-bbs",   CLG_(clo).dump_bbs) {}
+   else if VG_BOOL_CLO(arg, "--dump-line",  CLG_(clo).dump_line) {}
+   else if VG_BOOL_CLO(arg, "--dump-instr", CLG_(clo).dump_instr) {}
+   else if VG_BOOL_CLO(arg, "--dump-bb",    CLG_(clo).dump_bb) {}
+
+   else if VG_INT_CLO( arg, "--dump-every-bb", CLG_(clo).dump_every_bb) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-alloc",   CLG_(clo).collect_alloc) {}
+   else if VG_BOOL_CLO(arg, "--collect-systime", CLG_(clo).collect_systime) {}
+   else if VG_BOOL_CLO(arg, "--collect-bus",     CLG_(clo).collect_bus) {}
+   /* for option compatibility with cachegrind */
+   else if VG_BOOL_CLO(arg, "--branch-sim",      CLG_(clo).simulate_branch) {}
+   else {
+       return False;
+   }
+
+   return True;
+}
+
+void CLG_(print_usage)(void)
+{
+	/*
+   VG_(printf)(
+"\n   dump creation options:\n"
+"    --callgrind-out-file=<f>  Output file name [callgrind.out.%%p]\n"
+"    --dump-line=no|yes        Dump source lines of costs? [yes]\n"
+"    --dump-instr=no|yes       Dump instruction address of costs? [no]\n"
+"    --compress-strings=no|yes Compress strings in profile dump? [yes]\n"
+"    --compress-pos=no|yes     Compress positions in profile dump? [yes]\n"
+"    --combine-dumps=no|yes    Concat all dumps into same file [no]\n"
+#if CLG_EXPERIMENTAL
+"    --compress-events=no|yes  Compress events in profile dump? [no]\n"
+"    --dump-bb=no|yes          Dump basic block address of costs? [no]\n"
+"    --dump-bbs=no|yes         Dump basic block info? [no]\n"
+"    --dump-skipped=no|yes     Dump info on skipped functions in calls? [no]\n"
+"    --mangle-names=no|yes     Mangle separation into names? [yes]\n"
+#endif
+
+"\n   activity options (for interactivity use callgrind_control):\n"
+"    --dump-every-bb=<count>   Dump every <count> basic blocks [0=never]\n"
+"    --dump-before=<func>      Dump when entering function\n"
+"    --zero-before=<func>      Zero all costs when entering function\n"
+"    --dump-after=<func>       Dump when leaving function\n"
+#if CLG_EXPERIMENTAL
+"    --dump-objs=no|yes        Dump static object information [no]\n"
+#endif
+
+"\n   data collection options:\n"
+"    --instr-atstart=no|yes    Do instrumentation at callgrind start [yes]\n"
+"    --collect-atstart=no|yes  Collect at process/thread start [yes]\n"
+"    --toggle-collect=<func>   Toggle collection on enter/leave function\n"
+"    --collect-jumps=no|yes    Collect jumps? [no]\n"
+"    --collect-bus=no|yes      Collect global bus events? [no]\n"
+#if CLG_EXPERIMENTAL
+"    --collect-alloc=no|yes    Collect memory allocation info? [no]\n"
+#endif
+"    --collect-systime=no|yes  Collect system call time info? [no]\n"
+
+"\n   cost entity separation options:\n"
+"    --separate-threads=no|yes Separate data per thread [no]\n"
+"    --separate-callers=<n>    Separate functions by call chain length [0]\n"
+"    --separate-callers<n>=<f> Separate <n> callers for function <f>\n"
+"    --separate-recs=<n>       Separate function recursions up to level [2]\n"
+"    --separate-recs<n>=<f>    Separate <n> recursions for function <f>\n"
+"    --skip-plt=no|yes         Ignore calls to/from PLT sections? [yes]\n"
+"    --skip-direct-rec=no|yes  Ignore direct recursions? [yes]\n"
+"    --fn-skip=<function>      Ignore calls to/from function?\n"
+#if CLG_EXPERIMENTAL
+"    --fn-group<no>=<func>     Put function into separation group <no>\n"
+#endif
+"\n   simulation options:\n"
+"    --branch-sim=no|yes       Do branch prediction simulation [no]\n"
+"    --cache-sim=no|yes        Do cache simulation [no]\n"
+    );
+
+//   VG_(printf)("\n"
+//	       "  For full callgrind documentation, see\n"
+//	       "  "VG_PREFIX"/share/doc/callgrind/html/callgrind.html\n\n");
+	*/
+}
+
+void CLG_(print_debug_usage)(void)
+{
+    VG_(printf)(
+
+#if CLG_ENABLE_DEBUG
+"    --ct-verbose=<level>       Verbosity of standard debug output [0]\n"
+"    --ct-vstart=<BB number>    Only be verbose after basic block [0]\n"
+"    --ct-verbose<level>=<func> Verbosity while in <func>\n"
+#else
+"    (none)\n"
+#endif
+
+    );
+}
+
+void SGL_(set_clo_defaults)(void)
+{
+  SGL_(clo).ipc_dir            = NULL;
+  SGL_(clo).collect_func       = NULL;
+  SGL_(clo).start_collect_func = NULL;
+  SGL_(clo).stop_collect_func  = NULL;
+  SGL_(clo).gen_mem            = False;
+  SGL_(clo).gen_comp           = False;
+  SGL_(clo).gen_cf             = False;
+  SGL_(clo).gen_sync           = False;
+  SGL_(clo).gen_instr          = False;
+  SGL_(clo).gen_bb             = False;
+  SGL_(clo).gen_fn             = False;
+  SGL_(clo).gen_thr            = False;
+}
+
+void CLG_(set_clo_defaults)(void)
+{
+  /* Default values for command line arguments */
+
+  /* dump options */
+  CLG_(clo).out_format       = 0;
+  CLG_(clo).combine_dumps    = False;
+  CLG_(clo).compress_strings = True;
+  CLG_(clo).compress_mangled = False;
+  CLG_(clo).compress_events  = False;
+  CLG_(clo).compress_pos     = True;
+  CLG_(clo).mangle_names     = True;
+  CLG_(clo).dump_line        = True;
+  CLG_(clo).dump_instr       = False;
+  CLG_(clo).dump_bb          = False;
+  CLG_(clo).dump_bbs         = False;
+
+  CLG_(clo).dump_every_bb    = 0;
+
+  /* Collection */
+  CLG_(clo).separate_threads = False;
+  CLG_(clo).collect_atstart  = True;
+  CLG_(clo).collect_jumps    = False;
+  CLG_(clo).collect_alloc    = False;
+  CLG_(clo).collect_systime  = False;
+  CLG_(clo).collect_bus      = False;
+
+  CLG_(clo).skip_plt         = True;
+  CLG_(clo).separate_callers = 0;
+  CLG_(clo).separate_recursions = 2;
+  CLG_(clo).skip_direct_recursion = False;
+
+  /* Instrumentation */
+  CLG_(clo).instrument_atstart = True;
+  CLG_(clo).simulate_branch = False;
+
+  /* Call graph */
+  CLG_(clo).pop_on_jump = False;
+
+#if CLG_ENABLE_DEBUG
+  CLG_(clo).verbose = 0;
+  CLG_(clo).verbose_start = 0;
+#endif
+}
diff --git a/sigrind/context.c b/sigrind/context.c
new file mode 100644
index 000000000..33f738627
--- /dev/null
+++ b/sigrind/context.c
@@ -0,0 +1,332 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                 ct_context.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+
+/*------------------------------------------------------------*/
+/*--- Context operations                                   ---*/
+/*------------------------------------------------------------*/
+
+#define N_FNSTACK_INITIAL_ENTRIES 500
+#define N_CXT_INITIAL_ENTRIES 2537
+
+fn_stack CLG_(current_fn_stack);
+
+void CLG_(init_fn_stack)(fn_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  s->size   = N_FNSTACK_INITIAL_ENTRIES;   
+  s->bottom = (fn_node**) CLG_MALLOC("cl.context.ifs.1",
+                                     s->size * sizeof(fn_node*));
+  s->top    = s->bottom;
+  s->bottom[0] = 0;
+}
+
+void CLG_(copy_current_fn_stack)(fn_stack* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size   = CLG_(current_fn_stack).size;
+  dst->bottom = CLG_(current_fn_stack).bottom;
+  dst->top    = CLG_(current_fn_stack).top;
+}
+
+void CLG_(set_current_fn_stack)(fn_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  CLG_(current_fn_stack).size   = s->size;
+  CLG_(current_fn_stack).bottom = s->bottom;
+  CLG_(current_fn_stack).top    = s->top;
+}
+
+static cxt_hash cxts;
+
+void CLG_(init_cxt_table)()
+{
+   Int i;
+   
+   cxts.size    = N_CXT_INITIAL_ENTRIES;
+   cxts.entries = 0;
+   cxts.table   = (Context**) CLG_MALLOC("cl.context.ict.1",
+                                         cxts.size * sizeof(Context*));
+
+   for (i = 0; i < cxts.size; i++)
+     cxts.table[i] = 0;
+}
+
+/* double size of cxt table  */
+static void resize_cxt_table(void)
+{
+    UInt i, new_size, conflicts1 = 0, conflicts2 = 0;
+    Context **new_table, *curr, *next;
+    UInt new_idx;
+
+    new_size  = 2* cxts.size +3;
+    new_table = (Context**) CLG_MALLOC("cl.context.rct.1",
+                                       new_size * sizeof(Context*));
+
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+
+    for (i = 0; i < cxts.size; i++) {
+        if (cxts.table[i] == NULL) continue;
+
+        curr = cxts.table[i];
+        while (NULL != curr) {
+            next = curr->next;
+
+            new_idx = (UInt) (curr->hash % new_size);
+
+            curr->next = new_table[new_idx];
+            new_table[new_idx] = curr;
+            if (curr->next) {
+                conflicts1++;
+                if (curr->next->next)
+                    conflicts2++;
+            }
+
+            curr = next;
+        }
+    }
+
+    VG_(free)(cxts.table);
+
+
+    CLG_DEBUG(0, "Resize Context Hash: %u => %u (entries %u, conflicts %u/%u)\n",
+             cxts.size, new_size,
+             cxts.entries, conflicts1, conflicts2);
+
+    cxts.size  = new_size;
+    cxts.table = new_table;
+    CLG_(stat).cxt_hash_resizes++;
+}
+
+__inline__
+static UWord cxt_hash_val(fn_node** fn, UInt size)
+{
+    UWord hash = 0;
+    UInt count = size;
+    while(*fn != 0) {
+        hash = (hash<<7) + (hash>>25) + (UWord)(*fn);
+        fn--;
+        count--;
+        if (count==0) break;
+    }
+    return hash;
+}
+
+__inline__
+static Bool is_cxt(UWord hash, fn_node** fn, Context* cxt)
+{
+    int count;
+    fn_node** cxt_fn;
+
+    if (hash != cxt->hash) return False;
+
+    count = cxt->size;
+    cxt_fn = &(cxt->fn[0]);
+    while((*fn != 0) && (count>0)) {
+        if (*cxt_fn != *fn) return False;
+        fn--;
+        cxt_fn++;
+        count--;
+    }
+    return True;
+}
+
+/**
+ * Allocate new Context structure
+ */
+static Context* new_cxt(fn_node** fn)
+{
+    Context* cxt;
+    UInt idx, offset;
+    UWord hash;
+    int size, recs;
+    fn_node* top_fn;
+
+    CLG_ASSERT(fn);
+    top_fn = *fn;
+    if (top_fn == 0) return 0;
+
+    size = top_fn->separate_callers +1;
+    recs = top_fn->separate_recursions;
+    if (recs<1) recs=1;
+
+    /* check fill degree of context hash table and resize if needed (>80%) */
+    cxts.entries++;
+    if (10 * cxts.entries / cxts.size > 8)
+        resize_cxt_table();
+
+    cxt = (Context*) CLG_MALLOC("cl.context.nc.1",
+                                sizeof(Context)+sizeof(fn_node*)*size);
+
+    // hash value calculation similar to cxt_hash_val(), but additionally
+    // copying function pointers in one run
+    hash = 0;
+    offset = 0;
+    while(*fn != 0) {
+        hash = (hash<<7) + (hash>>25) + (UWord)(*fn);
+	cxt->fn[offset] = *fn;
+        offset++;
+        fn--;
+        if (offset >= size) break;
+    }
+    if (offset < size) size = offset;
+
+    cxt->size        = size;
+    cxt->base_number = CLG_(stat).context_counter;
+    cxt->hash        = hash;
+
+    CLG_(stat).context_counter += recs;
+    CLG_(stat).distinct_contexts++;
+
+    /* insert into Context hash table */
+    idx = (UInt) (hash % cxts.size);
+    cxt->next = cxts.table[idx];
+    cxts.table[idx] = cxt;
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(3) {
+      VG_(printf)("  new_cxt ox%p: ", cxt);
+      CLG_(print_cxt)(12, cxt, 0);
+    }
+#endif
+
+    return cxt;
+}
+
+/* get the Context structure for current context */
+Context* CLG_(get_cxt)(fn_node** fn)
+{
+    Context* cxt;
+    UInt size, idx;
+    UWord hash;
+
+    CLG_ASSERT(fn != 0);
+    if (*fn == 0) return 0;
+    size = (*fn)->separate_callers+1;
+    if (size<=0) { size = -size+1; }
+
+    CLG_DEBUG(5, "+ get_cxt(fn '%s'): size %u\n",
+                (*fn)->name, size);
+
+    hash = cxt_hash_val(fn, size);
+
+    if ( ((cxt = (*fn)->last_cxt) != 0) && is_cxt(hash, fn, cxt)) {
+        CLG_DEBUG(5, "- get_cxt: %p\n", cxt);
+        return cxt;
+    }
+
+    CLG_(stat).cxt_lru_misses++;
+
+    idx = (UInt) (hash % cxts.size);
+    cxt = cxts.table[idx];
+
+    while(cxt) {
+        if (is_cxt(hash,fn,cxt)) break;
+        cxt = cxt->next;
+    }
+
+    if (!cxt)
+        cxt = new_cxt(fn);
+
+    (*fn)->last_cxt = cxt;
+
+    CLG_DEBUG(5, "- get_cxt: %p\n", cxt);
+
+    return cxt;
+}
+
+
+/**
+ * Change execution context by calling a new function from current context
+ * Pushing 0x0 specifies a marker for a signal handler entry
+ */
+void CLG_(push_cxt)(fn_node* fn)
+{
+  call_stack* cs = &CLG_(current_call_stack);
+  Int fn_entries;
+
+  CLG_DEBUG(5, "+ push_cxt(fn '%s'): old ctx %d\n", 
+	    fn ? fn->name : "0x0",
+	    CLG_(current_state).cxt ?
+	    (Int)CLG_(current_state).cxt->base_number : -1);
+
+  /* save old context on stack (even if not changed at all!) */
+  CLG_ASSERT(cs->sp < cs->size);
+  CLG_ASSERT(cs->entry[cs->sp].cxt == 0);
+  cs->entry[cs->sp].cxt = CLG_(current_state).cxt;
+  cs->entry[cs->sp].fn_sp = CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom;
+
+  if (fn && (*(CLG_(current_fn_stack).top) == fn)) return;
+  if (fn && (fn->group>0) &&
+      ((*(CLG_(current_fn_stack).top))->group == fn->group)) return;
+
+  /* resizing needed ? */
+  fn_entries = CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom;
+  if (fn_entries == CLG_(current_fn_stack).size-1) {
+    UInt new_size = CLG_(current_fn_stack).size *2;
+    fn_node** new_array = (fn_node**) CLG_MALLOC("cl.context.pc.1",
+						 new_size * sizeof(fn_node*));
+    int i;
+    for(i=0;i<CLG_(current_fn_stack).size;i++)
+      new_array[i] = CLG_(current_fn_stack).bottom[i];
+    VG_(free)(CLG_(current_fn_stack).bottom);
+    CLG_(current_fn_stack).top = new_array + fn_entries;
+    CLG_(current_fn_stack).bottom = new_array;
+
+    CLG_DEBUG(0, "Resize Context Stack: %u => %u (pushing '%s')\n", 
+	     CLG_(current_fn_stack).size, new_size,
+	     fn ? fn->name : "0x0");
+
+    CLG_(current_fn_stack).size = new_size;
+  }
+
+  if (fn && (*(CLG_(current_fn_stack).top) == 0)) {
+    UInt *pactive;
+
+    /* this is first function: increment its active count */
+    pactive = CLG_(get_fn_entry)(fn->number);
+    (*pactive)++;
+  }
+
+  CLG_(current_fn_stack).top++;
+  *(CLG_(current_fn_stack).top) = fn;
+  CLG_(current_state).cxt = CLG_(get_cxt)(CLG_(current_fn_stack).top);
+
+  CLG_DEBUG(5, "- push_cxt(fn '%s'): new cxt %d, fn_sp %ld\n",
+	    fn ? fn->name : "0x0",
+	    CLG_(current_state).cxt ?
+	    (Int)CLG_(current_state).cxt->base_number : -1,
+	    CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom + 0L);
+}
+			       
diff --git a/sigrind/debug.c b/sigrind/debug.c
new file mode 100644
index 000000000..011470692
--- /dev/null
+++ b/sigrind/debug.c
@@ -0,0 +1,447 @@
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains lot of code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+#include "events.h"
+
+/* If debugging mode of, dummy functions are provided (see below)
+ */
+#if CLG_ENABLE_DEBUG
+
+/*------------------------------------------------------------*/
+/*--- Debug output helpers                                 ---*/
+/*------------------------------------------------------------*/
+
+static void print_indent(int s)
+{
+    /* max of 40 spaces */
+    const HChar sp[] = "                                        ";
+    if (s>40) s=40;
+    VG_(printf)("%s", sp+40-s);
+}
+
+void CLG_(print_bb)(int s, BB* bb)
+{
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    VG_(printf)("BB %#lx (Obj '%s')", bb_addr(bb), bb->obj->name);
+}
+
+static
+void print_mangled_cxt(Context* cxt, int rec_index)
+{
+    int i;
+
+    if (!cxt)
+      VG_(printf)("(none)");
+    else {
+      VG_(printf)("%s", cxt->fn[0]->name);
+      if (rec_index >0)
+	VG_(printf)("'%d", rec_index +1);
+      for(i=1;i<cxt->size;i++)
+	VG_(printf)("'%s", cxt->fn[i]->name);
+    }
+}
+
+
+
+void CLG_(print_cxt)(Int s, Context* cxt, int rec_index)
+{
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (cxt) {
+    UInt *pactive = CLG_(get_fn_entry)(cxt->fn[0]->number);
+    CLG_ASSERT(rec_index < cxt->fn[0]->separate_recursions);
+    
+    VG_(printf)("Cxt %u" ,cxt->base_number + rec_index);
+    if (*pactive>0)
+      VG_(printf)(" [active=%u]", *pactive);
+    VG_(printf)(": ");	
+    print_mangled_cxt(cxt, rec_index);
+    VG_(printf)("\n");
+  }
+  else
+    VG_(printf)("(no context)\n");
+}
+
+void CLG_(print_execstate)(int s, exec_state* es)
+{
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!es) {
+    VG_(printf)("ExecState 0x0\n");
+    return;
+  }
+
+  VG_(printf)("ExecState [Sig %d, collect %s, nonskipped %p]: jmps_passed %d\n",
+	      es->sig, es->collect?"yes":"no",
+	      es->nonskipped, es->jmps_passed);
+}
+
+
+void CLG_(print_bbcc)(int s, BBCC* bbcc)
+{
+  BB* bb;
+
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!bbcc) {
+    VG_(printf)("BBCC 0x0\n");
+    return;
+  }
+ 
+  bb = bbcc->bb;
+  CLG_ASSERT(bb!=0);
+
+  VG_(printf)("%s +%#lx=%#lx, ",
+	      bb->obj->name + bb->obj->last_slash_pos,
+	      (UWord)bb->offset, bb_addr(bb));
+  CLG_(print_cxt)(s+8, bbcc->cxt, bbcc->rec_index);
+}
+
+void CLG_(print_eventset)(int s, EventSet* es)
+{
+    int i, j;
+    UInt mask;
+    EventGroup* eg;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!es) {
+	VG_(printf)("(EventSet not set)\n");
+	return;
+    }
+
+    VG_(printf)("EventSet %u (%d groups, size %d):",
+		es->mask, es->count, es->size);
+
+    if (es->count == 0) {
+	VG_(printf)("-\n");
+	return;
+    }
+
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+	if ((es->mask & mask)==0) continue;
+	eg = CLG_(get_event_group)(i);
+	if (!eg) continue;
+	VG_(printf)(" (%d: %s", i, eg->name[0]);
+	for(j=1; j<eg->size; j++)
+	    VG_(printf)(" %s", eg->name[j]);
+	VG_(printf)(")");
+    }
+    VG_(printf)("\n");
+}
+
+
+void CLG_(print_cost)(int s, EventSet* es, ULong* c)
+{
+    Int i, j, pos, off;
+    UInt mask;
+    EventGroup* eg;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!es) {
+      VG_(printf)("Cost (Nothing, EventSet not set)\n");
+      return;
+    }
+    if (!c) {
+      VG_(printf)("Cost (Null, EventSet %u)\n", es->mask);
+      return;
+    }
+
+    if (es->size == 0) {
+      VG_(printf)("Cost (Nothing, EventSet with len 0)\n");
+      return;
+    } 
+
+    pos = s;
+    pos += VG_(printf)("Cost [%p]: ", c);
+    off = 0;
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+	if ((es->mask & mask)==0) continue;
+	eg = CLG_(get_event_group)(i);
+	if (!eg) continue;
+	for(j=0; j<eg->size; j++) {
+
+	    if (off>0) {
+		if (pos > 70) {
+		    VG_(printf)(",\n");
+		    print_indent(s+5);
+		    pos = s+5;
+		}
+		else
+		    pos += VG_(printf)(", ");
+	    }
+
+	    pos += VG_(printf)("%s %llu", eg->name[j], c[off++]);
+	}
+    }
+    VG_(printf)("\n");
+}
+
+
+void CLG_(print_jcc)(int s, jCC* jcc)
+{
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!jcc) {
+	VG_(printf)("JCC to skipped function\n");
+	return;
+    }
+    VG_(printf)("JCC %p from ", jcc);
+    CLG_(print_bbcc)(s+9, jcc->from);
+    print_indent(s+4);    
+    VG_(printf)("to   ");
+    CLG_(print_bbcc)(s+9, jcc->to);
+    print_indent(s+4);
+    VG_(printf)("Calls %llu\n", jcc->call_counter);
+    print_indent(s+4);
+}
+
+/* dump out the current call stack */
+void CLG_(print_stackentry)(int s, int sp)
+{
+    call_entry* ce;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    ce = CLG_(get_call_entry)(sp);
+    VG_(printf)("[%-2d] SP %#lx, RA %#lx", sp, ce->sp, ce->ret_addr);
+    if (ce->nonskipped)
+	VG_(printf)(" NonSkipped BB %#lx / %s",
+		    bb_addr(ce->nonskipped->bb),
+		    ce->nonskipped->cxt->fn[0]->name);
+    VG_(printf)("\n");
+    print_indent(s+5);
+    CLG_(print_jcc)(5,ce->jcc);
+}
+
+/* debug output */
+#if 0
+static void print_call_stack()
+{
+    int c;
+
+    VG_(printf)("Call Stack:\n");
+    for(c=0;c<CLG_(current_call_stack).sp;c++)
+      CLG_(print_stackentry)(-2, c);
+}
+#endif
+
+void CLG_(print_bbcc_fn)(BBCC* bbcc)
+{
+    obj_node* obj;
+
+    if (!bbcc) {
+	VG_(printf)("%08x", 0u);
+	return;
+    }
+
+    VG_(printf)("%08lx/%c  %u:", bb_addr(bbcc->bb), 
+		(bbcc->bb->sect_kind == Vg_SectText) ? 'T' :
+		(bbcc->bb->sect_kind == Vg_SectData) ? 'D' :
+		(bbcc->bb->sect_kind == Vg_SectBSS) ? 'B' :
+		(bbcc->bb->sect_kind == Vg_SectGOT) ? 'G' :
+		(bbcc->bb->sect_kind == Vg_SectPLT) ? 'P' : 'U',
+		bbcc->cxt->base_number+bbcc->rec_index);
+    print_mangled_cxt(bbcc->cxt, bbcc->rec_index);
+
+    obj = bbcc->cxt->fn[0]->file->obj;
+    if (obj->name[0])
+	VG_(printf)(" %s", obj->name+obj->last_slash_pos);
+
+    if (VG_(strcmp)(bbcc->cxt->fn[0]->file->name, "???") !=0) {
+	VG_(printf)(" %s", bbcc->cxt->fn[0]->file->name);
+	if ((bbcc->cxt->fn[0] == bbcc->bb->fn) && (bbcc->bb->line>0))
+	    VG_(printf)(":%u", bbcc->bb->line);
+    }
+}	
+
+void CLG_(print_bbcc_cost)(int s, BBCC* bbcc)
+{
+  BB* bb;
+  Int i, cjmpNo;
+  ULong ecounter;
+
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!bbcc) {
+    VG_(printf)("BBCC 0x0\n");
+    return;
+  }
+ 
+  bb = bbcc->bb;
+  CLG_ASSERT(bb!=0);
+    
+  CLG_(print_bbcc)(s, bbcc);
+
+  ecounter = bbcc->ecounter_sum;
+
+  print_indent(s+2);
+  VG_(printf)("ECounter: sum %llu ", ecounter);
+  for(i=0; i<bb->cjmp_count; i++) {
+      VG_(printf)("[%u]=%llu ",
+		  bb->jmp[i].instr, bbcc->jmp[i].ecounter);
+  }
+  VG_(printf)("\n");
+
+  cjmpNo = 0; 
+  for(i=0; i<bb->instr_count; i++) {
+      InstrInfo* ii = &(bb->instr[i]);
+      print_indent(s+2);
+      VG_(printf)("[%2d] IOff %2u ecnt %3llu ",
+		  i, ii->instr_offset, ecounter);
+      CLG_(print_cost)(s+5, ii->eventset, bbcc->cost + ii->cost_offset);
+
+      /* update execution counter */
+      if (cjmpNo < bb->cjmp_count)
+	  if (bb->jmp[cjmpNo].instr == i) {
+	      ecounter -= bbcc->jmp[cjmpNo].ecounter;
+	      cjmpNo++;
+	  }
+  }
+}
+
+
+/* dump out an address with source info if available */
+void CLG_(print_addr)(Addr addr)
+{
+    const HChar *fn_buf, *fl_buf, *dir_buf;
+    const HChar* obj_name;
+    DebugInfo* di;
+    UInt ln, i=0, opos=0;
+	
+    if (addr == 0) {
+	VG_(printf)("%08lx", addr);
+	return;
+    }
+
+    CLG_(get_debug_info)(addr, &dir_buf, &fl_buf, &fn_buf, &ln, &di);
+
+    if (VG_(strcmp)(fn_buf,"???")==0)
+	VG_(printf)("%#lx", addr);
+    else
+	VG_(printf)("%#lx %s", addr, fn_buf);
+
+    if (di) {
+      obj_name = VG_(DebugInfo_get_filename)(di);
+      if (obj_name) {
+	while(obj_name[i]) {
+	  if (obj_name[i]=='/') opos = i+1;
+	  i++;
+	}
+	if (obj_name[0])
+	  VG_(printf)(" %s", obj_name+opos);
+      }
+    }
+
+    if (ln>0) {
+       if (dir_buf[0])
+          VG_(printf)(" (%s/%s:%u)", dir_buf, fl_buf, ln);
+       else
+          VG_(printf)(" (%s:%u)", fl_buf, ln);
+    }
+}
+
+void CLG_(print_addr_ln)(Addr addr)
+{
+  CLG_(print_addr)(addr);
+  VG_(printf)("\n");
+}
+
+static ULong bb_written = 0;
+
+void CLG_(print_bbno)(void)
+{
+  if (bb_written != CLG_(stat).bb_executions) {
+    bb_written = CLG_(stat).bb_executions;
+    VG_(printf)("BB# %llu\n",CLG_(stat).bb_executions);
+  }
+}
+
+void CLG_(print_context)(void)
+{
+  BBCC* bbcc;
+
+  CLG_DEBUG(0,"In tid %u [%d] ",
+	   CLG_(current_tid),  CLG_(current_call_stack).sp);
+  bbcc =  CLG_(current_state).bbcc;
+  print_mangled_cxt(CLG_(current_state).cxt,
+		    bbcc ? bbcc->rec_index : 0);
+  VG_(printf)("\n");
+}
+
+void* CLG_(malloc)(const HChar* cc, UWord s, const HChar* f)
+{
+    CLG_DEBUG(3, "Malloc(%lu) in %s.\n", s, f);
+    return VG_(malloc)(cc,s);
+}
+
+#else /* CLG_ENABLE_DEBUG */
+
+void CLG_(print_bbno)(void) {}
+void CLG_(print_context)(void) {}
+void CLG_(print_jcc)(int s, jCC* jcc) {}
+void CLG_(print_bbcc)(int s, BBCC* bbcc) {}
+void CLG_(print_bbcc_fn)(BBCC* bbcc) {}
+void CLG_(print_cost)(int s, EventSet* es, ULong* cost) {}
+void CLG_(print_bb)(int s, BB* bb) {}
+void CLG_(print_cxt)(int s, Context* cxt, int rec_index) {}
+void CLG_(print_short_jcc)(jCC* jcc) {}
+void CLG_(print_stackentry)(int s, int sp) {}
+void CLG_(print_addr)(Addr addr) {}
+void CLG_(print_addr_ln)(Addr addr) {}
+
+#endif
diff --git a/sigrind/events.c b/sigrind/events.c
new file mode 100644
index 000000000..47cf6ee18
--- /dev/null
+++ b/sigrind/events.c
@@ -0,0 +1,261 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                     events.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/* This should be 2**MAX_EVENTGROUP_COUNT */
+#define MAX_EVENTSET_COUNT 1024
+
+static EventGroup* eventGroup[MAX_EVENTGROUP_COUNT];
+static EventSet* eventSetTable[MAX_EVENTSET_COUNT];
+static Bool eventSets_initialized = 0;
+
+static
+void initialize_event_sets(void)
+{
+    Int i;
+
+    if (eventSets_initialized) return;
+
+    for(i=0; i< MAX_EVENTGROUP_COUNT; i++)
+        eventGroup[i] = 0;
+
+    for(i=0; i< MAX_EVENTSET_COUNT; i++)
+        eventSetTable[i] = 0; 
+
+    eventSets_initialized = 1;
+ }
+
+static
+EventGroup* new_event_group(int id, int n)
+{
+    EventGroup* eg;
+
+    initialize_event_sets();
+
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(eventGroup[id]==0);
+
+    eg = (EventGroup*) CLG_MALLOC("cl.events.group.1",
+                                  sizeof(EventGroup) + n * sizeof(HChar*));
+    eg->size = n;
+    eventGroup[id] = eg;
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group) (int id, const HChar* n1)
+{
+    EventGroup* eg = new_event_group(id, 1);
+    eg->name[0] = n1;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group2)(int id, const HChar* n1,
+                                        const HChar* n2)
+{
+    EventGroup* eg = new_event_group(id, 2);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group3)(int id, const HChar* n1,
+                                        const HChar* n2, const HChar* n3)
+{
+    EventGroup* eg = new_event_group(id, 3);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+    eg->name[2] = n3;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group4)(int id, const HChar* n1,
+                                        const HChar* n2, const HChar* n3,
+                                        const HChar* n4)
+{
+    EventGroup* eg = new_event_group(id, 4);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+    eg->name[2] = n3;
+    eg->name[3] = n4;
+
+    return eg;
+}
+
+EventGroup* CLG_(get_event_group)(int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+
+    return eventGroup[id];
+}
+
+
+static
+EventSet* eventset_from_mask(UInt mask)
+{
+    EventSet* es;
+    Int i, count, offset;
+
+    if (mask >= MAX_EVENTSET_COUNT) return 0;
+
+    initialize_event_sets();
+    if (eventSetTable[mask]) return eventSetTable[mask];
+
+    es = (EventSet*) CLG_MALLOC("cl.events.eventset.1", sizeof(EventSet));
+    es->mask = mask;
+
+    offset = 0;
+    count = 0;
+    for(i=0;i<MAX_EVENTGROUP_COUNT;i++) {
+        es->offset[i] = offset;
+        if ( ((mask & (1u<<i))==0) || (eventGroup[i]==0))
+            continue;
+
+        offset += eventGroup[i]->size;
+        count++;
+    }
+    es->size = offset;
+    es->count = count;
+
+    eventSetTable[mask] = es;
+    return es;
+}
+
+EventSet* CLG_(get_event_set)(Int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    return eventset_from_mask(1u << id);
+}
+
+EventSet* CLG_(get_event_set2)(Int id1, Int id2)
+{
+    CLG_ASSERT(id1>=0 && id1<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(id2>=0 && id2<MAX_EVENTGROUP_COUNT);
+    return eventset_from_mask((1u << id1) | (1u << id2));
+}
+
+EventSet* CLG_(add_event_group)(EventSet* es, Int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    if (!es) es = eventset_from_mask(0);
+    return eventset_from_mask(es->mask | (1u << id));
+}
+
+EventSet* CLG_(add_event_group2)(EventSet* es, Int id1, Int id2)
+{
+    CLG_ASSERT(id1>=0 && id1<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(id2>=0 && id2<MAX_EVENTGROUP_COUNT);
+    if (!es) es = eventset_from_mask(0);
+    return eventset_from_mask(es->mask | (1u << id1) | (1u << id2));
+}
+
+EventSet* CLG_(add_event_set)(EventSet* es1, EventSet* es2)
+{
+    if (!es1) es1 = eventset_from_mask(0);
+    if (!es2) es2 = eventset_from_mask(0);
+    return eventset_from_mask(es1->mask | es2->mask);
+}
+
+
+
+
+/* Allocate space for an event mapping */
+EventMapping* CLG_(get_eventmapping)(EventSet* es)
+{
+    EventMapping* em;
+
+    CLG_ASSERT(es != 0);
+
+    em = (EventMapping*) CLG_MALLOC("cl.events.geMapping.1",
+                                    sizeof(EventMapping) +
+                                    sizeof(struct EventMappingEntry) *
+                                    es->size);
+    em->capacity = es->size;
+    em->size = 0;
+    em->es = es;
+
+    return em;
+}
+
+void CLG_(append_event)(EventMapping* em, const HChar* n)
+{
+    Int i, j, offset = 0;
+    UInt mask;
+    EventGroup* eg;
+
+    CLG_ASSERT(em != 0);
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+        if ((em->es->mask & mask)==0) continue;
+        if (eventGroup[i] ==0) continue;
+
+        eg = eventGroup[i];
+        for(j=0; j<eg->size; j++, offset++) {
+            if (VG_(strcmp)(n, eg->name[j])!=0)
+                    continue;
+
+            CLG_ASSERT(em->capacity > em->size);
+            em->entry[em->size].group = i;
+            em->entry[em->size].index = j;
+            em->entry[em->size].offset = offset;
+            em->size++;
+            return;
+        }
+    }
+}
+
+
+/* Returns pointer to dynamically string. The string will be overwritten
+   with each invocation. */
+HChar *CLG_(eventmapping_as_string)(const EventMapping* em)
+{
+    Int i;
+    EventGroup* eg;
+
+    CLG_ASSERT(em != 0);
+
+    XArray *xa = VG_(newXA)(VG_(malloc), "cl.events.emas", VG_(free),
+                            sizeof(HChar));
+
+    for(i=0; i< em->size; i++) {
+        if (i > 0) {
+           VG_(xaprintf)(xa, "%c", ' ');
+        }
+        eg = eventGroup[em->entry[i].group];
+        CLG_ASSERT(eg != 0);
+        VG_(xaprintf)(xa, "%s", eg->name[em->entry[i].index]);
+    }
+    VG_(xaprintf)(xa, "%c", '\0');   // zero terminate the string
+
+    HChar *buf = VG_(strdup)("cl.events.emas", VG_(indexXA)(xa, 0));
+    VG_(deleteXA)(xa);
+
+    return buf;
+}
diff --git a/sigrind/events.h b/sigrind/events.h
new file mode 100644
index 000000000..b38b3c313
--- /dev/null
+++ b/sigrind/events.h
@@ -0,0 +1,133 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                     events.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* Abstractions for 64-bit cost lists (events.h) */
+
+#ifndef CLG_EVENTS
+#define CLG_EVENTS
+
+#include "pub_tool_basics.h"
+
+#define CLG_(str) VGAPPEND(vgCallgrind_,str)
+
+/* Event groups consist of one or more named event types.
+ * Event sets are constructed from such event groups.
+ *
+ * Event groups have to be registered globally with a unique ID
+ * before they can be used in an event set.
+ * A group can appear at most once in a event set.
+ */
+
+#define MAX_EVENTGROUP_COUNT 10
+
+typedef struct _EventGroup EventGroup;
+struct _EventGroup {
+    Int size;
+    const HChar* name[0];
+};
+
+/* return 0 if event group can not be registered */
+EventGroup* CLG_(register_event_group) (int id, const HChar*);
+EventGroup* CLG_(register_event_group2)(int id, const HChar*, const HChar*);
+EventGroup* CLG_(register_event_group3)(int id, const HChar*, const HChar*,
+                                        const HChar*);
+EventGroup* CLG_(register_event_group4)(int id, const HChar*, const HChar*,
+                                        const HChar*, const HChar*);
+EventGroup* CLG_(get_event_group)(int id);
+
+/* Event sets are defined by event groups they consist of. */
+
+typedef struct _EventSet EventSet;
+struct _EventSet {
+    /* if subset with ID x is in the set, then bit x is set */
+    UInt mask;
+    Int count;
+    Int size;
+    Int offset[MAX_EVENTGROUP_COUNT];
+ };
+
+/* Same event set is returned when requesting same event groups */
+EventSet* CLG_(get_event_set)(Int id);
+EventSet* CLG_(get_event_set2)(Int id1, Int id2);
+EventSet* CLG_(add_event_group)(EventSet*, Int id);
+EventSet* CLG_(add_event_group2)(EventSet*, Int id1, Int id2);
+EventSet* CLG_(add_event_set)(EventSet*, EventSet*);
+
+
+/* Operations on costs. A cost pointer of 0 means zero cost.
+ * Functions ending in _lz allocate cost arrays only when needed
+ */
+ULong* CLG_(get_eventset_cost)(EventSet*);
+/* Set costs of event set to 0 */
+void CLG_(init_cost)(EventSet*,ULong*);
+/* This always allocates counter and sets them to 0 */
+void CLG_(init_cost_lz)(EventSet*,ULong**);
+/* Set costs of an event set to zero */
+void CLG_(zero_cost)(EventSet*,ULong*);
+Bool CLG_(is_zero_cost)(EventSet*,ULong*);
+void CLG_(copy_cost)(EventSet*,ULong* dst, ULong* src);
+void CLG_(copy_cost_lz)(EventSet*,ULong** pdst, ULong* src);
+void CLG_(add_cost)(EventSet*,ULong* dst, ULong* src);
+void CLG_(add_cost_lz)(EventSet*,ULong** pdst, ULong* src);
+/* Adds src to dst and zeros src. Returns false if nothing changed */
+Bool CLG_(add_and_zero_cost)(EventSet*,ULong* dst, ULong* src);
+Bool CLG_(add_and_zero_cost2)(EventSet*,ULong* dst,EventSet*,ULong* src);
+/* Adds difference of new and old to to dst, and set old to new.
+ * Returns false if nothing changed */
+Bool CLG_(add_diff_cost)(EventSet*,ULong* dst, ULong* old, ULong* new_cost);
+Bool CLG_(add_diff_cost_lz)(EventSet*,ULong** pdst, ULong* old, ULong* new_cost);
+
+/* EventMapping: An ordered subset of events from an event set.
+ * This is used to print out part of an EventSet, or in another order.
+ */
+struct EventMappingEntry {
+    Int group;
+    Int index;
+    Int offset;
+};
+typedef struct _EventMapping EventMapping;
+struct _EventMapping {
+  EventSet* es;
+  Int size;
+  Int capacity;
+  struct EventMappingEntry entry[0];
+};
+
+/* Allocate space for an event mapping */
+EventMapping* CLG_(get_eventmapping)(EventSet*);
+void CLG_(append_event)(EventMapping*, const HChar*);
+/* Returns event mapping as a character string. That string is dynamically
+   allocated and it is the caller's responsibility to free it.
+   The function never returns NULL. */
+HChar *CLG_(eventmapping_as_string)(const EventMapping*);
+/* Returns mapping cost as a character string. That string is dynamically
+   allocated and it is the caller's responsibility to free it.
+   The function never returns NULL. */
+HChar *CLG_(mappingcost_as_string)(const EventMapping*, const ULong*);
+
+#endif /* CLG_EVENTS */
diff --git a/sigrind/fn.c b/sigrind/fn.c
new file mode 100644
index 000000000..243494146
--- /dev/null
+++ b/sigrind/fn.c
@@ -0,0 +1,686 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                      ct_fn.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+#define N_INITIAL_FN_ARRAY_SIZE 10071
+
+static fn_array current_fn_active;
+
+static Addr runtime_resolve_addr = 0;
+static int  runtime_resolve_length = 0;
+
+// a code pattern is a list of tuples (start offset, length)
+struct chunk_t { int start, len; };
+struct pattern
+{
+    const HChar* name;
+    int len;
+    struct chunk_t chunk[];
+};
+
+/* Scan for a pattern in the code of an ELF object.
+ * If found, return true and set runtime_resolve_{addr,length}
+ */
+__attribute__((unused))    // Possibly;  depends on the platform.
+static Bool check_code(obj_node* obj,
+                       UChar code[], struct pattern* pat)
+{
+    Bool found;
+    Addr addr, end;
+    int chunk, start, len;
+
+    /* first chunk of pattern should always start at offset 0 and
+     * have at least 3 bytes */
+    CLG_ASSERT((pat->chunk[0].start == 0) && (pat->chunk[0].len >2));
+    
+    CLG_DEBUG(1, "check_code: %s, pattern %s, check %d bytes of [%x %x %x...]\n",
+              obj->name, pat->name, pat->chunk[0].len, code[0], code[1], code[2]);
+
+    end = obj->start + obj->size - pat->len;
+    addr = obj->start;
+    while(addr < end) {
+	found = (VG_(memcmp)( (void*)addr, code, pat->chunk[0].len) == 0);
+
+        if (found) {
+	    chunk = 1;
+	    while(1) {		
+		start = pat->chunk[chunk].start;
+		len   = pat->chunk[chunk].len;
+		if (len == 0) break;
+
+		CLG_ASSERT(len >2);
+                CLG_DEBUG(1, " found chunk %d at %#lx, checking %d bytes "
+                             "of [%x %x %x...]\n",
+                          chunk-1, addr - obj->start, len,
+			  code[start], code[start+1], code[start+2]);
+
+                if (VG_(memcmp)( (void*)(addr+start), code+start, len) != 0) {
+                    found = False;
+                    break;
+                }
+		chunk++;
+	    }
+
+            if (found) {
+		CLG_DEBUG(1, "found at offset %#lx.\n", addr - obj->start);
+		if (VG_(clo_verbosity) > 1)
+		    VG_(message)(Vg_DebugMsg, "Found runtime_resolve (%s): "
+                                              "%s +%#lx=%#lx, length %d\n",
+				 pat->name, obj->name + obj->last_slash_pos,
+				 addr - obj->start, addr, pat->len);
+		    
+		runtime_resolve_addr   = addr;
+		runtime_resolve_length = pat->len;
+		return True;
+	    }
+        }
+        addr++;
+    }
+    CLG_DEBUG(1, " found nothing.\n");
+    return False;
+}
+
+
+/* _ld_runtime_resolve, located in ld.so, needs special handling:
+ * The jump at end into the resolved function should not be
+ * represented as a call (as usually done in callgrind with jumps),
+ * but as a return + call. Otherwise, the repeated existance of
+ * _ld_runtime_resolve in call chains will lead to huge cycles,
+ * making the profile almost worthless.
+ *
+ * If ld.so is stripped, the symbol will not appear. But as this
+ * function is handcrafted assembler, we search for it.
+ *
+ * We stop if the ELF object name does not seem to be the runtime linker
+ */
+static Bool search_runtime_resolve(obj_node* obj)
+{
+#if defined(VGP_x86_linux)
+    static UChar code[] = {
+	/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+	/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+	/*16*/ 0x5a, 0x59, 0x87, 0x04, 0x24, 0xc2, 0x08, 0x00 };
+    /* Check ranges [0-11] and [16-23] ([12-15] is an absolute address) */
+    static struct pattern pat = {
+	"x86-def", 24, {{ 0,12 }, { 16,8 }, { 24,0}} };
+
+    /* Pattern for glibc-2.8 on OpenSuse11.0 */
+    static UChar code_28[] = {
+	/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+	/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+	/*16*/ 0x5a, 0x8b, 0x0c, 0x24, 0x89, 0x04, 0x24, 0x8b,
+	/*24*/ 0x44, 0x24, 0x04, 0xc2, 0x0c, 0x00 };
+    static struct pattern pat_28 = {
+	"x86-glibc2.8", 30, {{ 0,12 }, { 16,14 }, { 30,0}} };
+
+    if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+    if (check_code(obj, code, &pat)) return True;
+    if (check_code(obj, code_28, &pat_28)) return True;
+    return False;
+#endif
+
+#if defined(VGP_ppc32_linux)
+    static UChar code[] = {
+	/* 0*/ 0x94, 0x21, 0xff, 0xc0, 0x90, 0x01, 0x00, 0x0c,
+	/* 8*/ 0x90, 0x61, 0x00, 0x10, 0x90, 0x81, 0x00, 0x14,
+	/*16*/ 0x7d, 0x83, 0x63, 0x78, 0x90, 0xa1, 0x00, 0x18,
+	/*24*/ 0x7d, 0x64, 0x5b, 0x78, 0x90, 0xc1, 0x00, 0x1c,
+	/*32*/ 0x7c, 0x08, 0x02, 0xa6, 0x90, 0xe1, 0x00, 0x20,
+	/*40*/ 0x90, 0x01, 0x00, 0x30, 0x91, 0x01, 0x00, 0x24,
+	/*48*/ 0x7c, 0x00, 0x00, 0x26, 0x91, 0x21, 0x00, 0x28,
+	/*56*/ 0x91, 0x41, 0x00, 0x2c, 0x90, 0x01, 0x00, 0x08,
+	/*64*/ 0x48, 0x00, 0x02, 0x91, 0x7c, 0x69, 0x03, 0xa6, /* at 64: bl aff0 <fixup> */
+	/*72*/ 0x80, 0x01, 0x00, 0x30, 0x81, 0x41, 0x00, 0x2c,
+	/*80*/ 0x81, 0x21, 0x00, 0x28, 0x7c, 0x08, 0x03, 0xa6,
+	/*88*/ 0x81, 0x01, 0x00, 0x24, 0x80, 0x01, 0x00, 0x08,
+	/*96*/ 0x80, 0xe1, 0x00, 0x20, 0x80, 0xc1, 0x00, 0x1c,
+	/*104*/0x7c, 0x0f, 0xf1, 0x20, 0x80, 0xa1, 0x00, 0x18,
+	/*112*/0x80, 0x81, 0x00, 0x14, 0x80, 0x61, 0x00, 0x10,
+	/*120*/0x80, 0x01, 0x00, 0x0c, 0x38, 0x21, 0x00, 0x40,
+	/*128*/0x4e, 0x80, 0x04, 0x20 };
+    static struct pattern pat = {
+	"ppc32-def", 132, {{ 0,65 }, { 68,64 }, { 132,0 }} };
+
+    if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+    return check_code(obj, code, &pat);
+#endif
+
+#if defined(VGP_amd64_linux)
+    static UChar code[] = {
+	/* 0*/ 0x48, 0x83, 0xec, 0x38, 0x48, 0x89, 0x04, 0x24,
+	/* 8*/ 0x48, 0x89, 0x4c, 0x24, 0x08, 0x48, 0x89, 0x54, 0x24, 0x10,
+	/*18*/ 0x48, 0x89, 0x74, 0x24, 0x18, 0x48, 0x89, 0x7c, 0x24, 0x20,
+	/*28*/ 0x4c, 0x89, 0x44, 0x24, 0x28, 0x4c, 0x89, 0x4c, 0x24, 0x30,
+	/*38*/ 0x48, 0x8b, 0x74, 0x24, 0x40, 0x49, 0x89, 0xf3,
+	/*46*/ 0x4c, 0x01, 0xde, 0x4c, 0x01, 0xde, 0x48, 0xc1, 0xe6, 0x03,
+	/*56*/ 0x48, 0x8b, 0x7c, 0x24, 0x38, 0xe8, 0xee, 0x01, 0x00, 0x00,
+	/*66*/ 0x49, 0x89, 0xc3, 0x4c, 0x8b, 0x4c, 0x24, 0x30,
+	/*74*/ 0x4c, 0x8b, 0x44, 0x24, 0x28, 0x48, 0x8b, 0x7c, 0x24, 0x20,
+	/*84*/ 0x48, 0x8b, 0x74, 0x24, 0x18, 0x48, 0x8b, 0x54, 0x24, 0x10,
+	/*94*/ 0x48, 0x8b, 0x4c, 0x24, 0x08, 0x48, 0x8b, 0x04, 0x24,
+	/*103*/0x48, 0x83, 0xc4, 0x48, 0x41, 0xff, 0xe3 };
+    static struct pattern pat = {
+	"amd64-def", 110, {{ 0,62 }, { 66,44 }, { 110,0 }} };
+
+    if ((VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) &&
+	(VG_(strncmp)(obj->name, "/lib64/ld", 9) != 0)) return False;
+    return check_code(obj, code, &pat);
+#endif
+
+    /* For other platforms, no patterns known */
+    return False;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Object/File/Function hash entry operations           ---*/
+/*------------------------------------------------------------*/
+
+/* Object hash table, fixed */
+static obj_node* obj_table[N_OBJ_ENTRIES];
+
+void CLG_(init_obj_table)()
+{
+    Int i;
+    for (i = 0; i < N_OBJ_ENTRIES; i++)
+	obj_table[i] = 0;
+}
+
+#define HASH_CONSTANT   256
+
+static UInt str_hash(const HChar *s, UInt table_size)
+{
+    int hash_value = 0;
+    for ( ; *s; s++)
+        hash_value = (HASH_CONSTANT * hash_value + *s) % table_size;
+    return hash_value;
+}
+
+
+static const HChar* anonymous_obj = "???";
+
+static __inline__ 
+obj_node* new_obj_node(DebugInfo* di, obj_node* next)
+{
+   Int i;
+   obj_node* obj;
+
+   obj = (obj_node*) CLG_MALLOC("cl.fn.non.1", sizeof(obj_node));
+   obj->name  = di ? VG_(strdup)( "cl.fn.non.2",
+                                  VG_(DebugInfo_get_filename)(di) )
+                   : anonymous_obj;
+   for (i = 0; i < N_FILE_ENTRIES; i++) {
+      obj->files[i] = NULL;
+   }
+   CLG_(stat).distinct_objs ++;
+   obj->number  = CLG_(stat).distinct_objs;
+   /* JRS 2008 Feb 19: maybe rename .start/.size/.offset to
+      .text_avma/.text_size/.test_bias to make it clearer what these
+      fields really mean */
+   obj->start   = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+   obj->size    = di ? VG_(DebugInfo_get_text_size)(di) : 0;
+   obj->offset  = di ? VG_(DebugInfo_get_text_bias)(di) : 0;
+   obj->next    = next;
+
+   // not only used for debug output (see static.c)
+   obj->last_slash_pos = 0;
+   i = 0;
+   while(obj->name[i]) {
+	if (obj->name[i]=='/') obj->last_slash_pos = i+1;
+	i++;
+   }
+
+   if (runtime_resolve_addr == 0) search_runtime_resolve(obj);
+
+   return obj;
+}
+
+obj_node* CLG_(get_obj_node)(DebugInfo* di)
+{
+    obj_node*    curr_obj_node;
+    UInt         objname_hash;
+    const HChar* obj_name;
+    
+    obj_name = di ? VG_(DebugInfo_get_filename)(di) : anonymous_obj;
+
+    /* lookup in obj hash */
+    objname_hash = str_hash(obj_name, N_OBJ_ENTRIES);
+    curr_obj_node = obj_table[objname_hash];
+    while (NULL != curr_obj_node && 
+	   VG_(strcmp)(obj_name, curr_obj_node->name) != 0) {
+	curr_obj_node = curr_obj_node->next;
+    }
+    if (NULL == curr_obj_node) {
+	obj_table[objname_hash] = curr_obj_node = 
+	    new_obj_node(di, obj_table[objname_hash]);
+    }
+
+    return curr_obj_node;
+}
+
+
+static __inline__ 
+file_node* new_file_node(const HChar *filename,
+			 obj_node* obj, file_node* next)
+{
+  Int i;
+  file_node* file = (file_node*) CLG_MALLOC("cl.fn.nfn.1",
+                                           sizeof(file_node));
+  file->name  = VG_(strdup)("cl.fn.nfn.2", filename);
+  for (i = 0; i < N_FN_ENTRIES; i++) {
+    file->fns[i] = NULL;
+  }
+  CLG_(stat).distinct_files++;
+  file->number  = CLG_(stat).distinct_files;
+  file->obj     = obj;
+  file->next      = next;
+  return file;
+}
+
+ 
+file_node* CLG_(get_file_node)(obj_node* curr_obj_node,
+                               const HChar *dir, const HChar *file)
+{
+    file_node* curr_file_node;
+    UInt       filename_hash;
+
+    /* Build up an absolute pathname, if there is a directory available */
+    HChar filename[VG_(strlen)(dir) + 1 + VG_(strlen)(file) + 1];
+    VG_(strcpy)(filename, dir);
+    if (filename[0] != '\0') {
+       VG_(strcat)(filename, "/");
+    }
+    VG_(strcat)(filename, file);
+
+    /* lookup in file hash */
+    filename_hash = str_hash(filename, N_FILE_ENTRIES);
+    curr_file_node = curr_obj_node->files[filename_hash];
+    while (NULL != curr_file_node && 
+	   VG_(strcmp)(filename, curr_file_node->name) != 0) {
+	curr_file_node = curr_file_node->next;
+    }
+    if (NULL == curr_file_node) {
+	curr_obj_node->files[filename_hash] = curr_file_node = 
+	    new_file_node(filename, curr_obj_node, 
+			  curr_obj_node->files[filename_hash]);
+    }
+
+    return curr_file_node;
+}
+
+/* forward decl. */
+static void resize_fn_array(void);
+
+static __inline__ 
+fn_node* new_fn_node(const HChar *fnname,
+		     file_node* file, fn_node* next)
+{
+    fn_node* fn = (fn_node*) CLG_MALLOC("cl.fn.nfnnd.1",
+                                         sizeof(fn_node));
+    fn->name = VG_(strdup)("cl.fn.nfnnd.2", fnname);
+
+    CLG_(stat).distinct_fns++;
+    fn->number   = CLG_(stat).distinct_fns;
+    fn->last_cxt = 0;
+    fn->pure_cxt = 0;
+    fn->file     = file;
+    fn->next     = next;
+
+    fn->dump_before  = False;
+    fn->dump_after   = False;
+    fn->zero_before  = False;
+    fn->toggle_collect = False;
+    fn->skip         = False;
+    fn->pop_on_jump  = CLG_(clo).pop_on_jump;
+    fn->is_malloc    = False;
+    fn->is_realloc   = False;
+    fn->is_free      = False;
+
+    fn->group        = 0;
+    fn->separate_callers    = CLG_(clo).separate_callers;
+    fn->separate_recursions = CLG_(clo).separate_recursions;
+
+#if CLG_ENABLE_DEBUG
+    fn->verbosity    = -1;
+#endif
+
+    if (CLG_(stat).distinct_fns >= current_fn_active.size)
+	resize_fn_array();
+
+    return fn;
+}
+
+
+/* Get a function node in hash2 with known file node.
+ * hash nodes are created if needed
+ */
+static
+fn_node* get_fn_node_infile(file_node* curr_file_node,
+			    const HChar *fnname)
+{
+    fn_node* curr_fn_node;
+    UInt     fnname_hash;
+
+    CLG_ASSERT(curr_file_node != 0);
+
+    /* lookup in function hash */
+    fnname_hash = str_hash(fnname, N_FN_ENTRIES);
+    curr_fn_node = curr_file_node->fns[fnname_hash];
+    while (NULL != curr_fn_node && 
+	   VG_(strcmp)(fnname, curr_fn_node->name) != 0) {
+	curr_fn_node = curr_fn_node->next;
+    }
+    if (NULL == curr_fn_node) {
+	curr_file_node->fns[fnname_hash] = curr_fn_node = 
+            new_fn_node(fnname, curr_file_node,
+			curr_file_node->fns[fnname_hash]);
+    }
+
+    return curr_fn_node;
+}
+
+
+/* Get a function node in a Segment.
+ * Hash nodes are created if needed.
+ */
+static __inline__
+fn_node* get_fn_node_inseg(DebugInfo* di,
+			   const HChar *dirname,
+			   const HChar *filename,
+			   const HChar *fnname)
+{
+  obj_node  *obj  = CLG_(get_obj_node)(di);
+  file_node *file = CLG_(get_file_node)(obj, dirname, filename);
+  fn_node   *fn   = get_fn_node_infile(file, fnname);
+
+  return fn;
+}
+
+
+Bool CLG_(get_debug_info)(Addr instr_addr,
+                          const HChar **dir,
+                          const HChar **file,
+                          const HChar **fn_name, UInt* line_num,
+                          DebugInfo** pDebugInfo)
+{
+  Bool found_file_line, found_fn, result = True;
+  UInt line;
+  
+  CLG_DEBUG(6, "  + get_debug_info(%#lx)\n", instr_addr);
+
+  if (pDebugInfo) {
+      *pDebugInfo = VG_(find_DebugInfo)(instr_addr);
+
+      // for generated code in anonymous space, pSegInfo is 0
+   }
+
+   found_file_line = VG_(get_filename_linenum)(instr_addr,
+					       file,
+					       dir,
+					       &line);
+   found_fn = VG_(get_fnname)(instr_addr, fn_name);
+
+   if (!found_file_line && !found_fn) {
+     CLG_(stat).no_debug_BBs++;
+     *file = "???";
+     *fn_name = "???";
+     if (line_num) *line_num=0;
+     result = False;
+
+   } else if ( found_file_line &&  found_fn) {
+     CLG_(stat).full_debug_BBs++;
+     if (line_num) *line_num=line;
+
+   } else if ( found_file_line && !found_fn) {
+     CLG_(stat).file_line_debug_BBs++;
+     *fn_name = "???";
+     if (line_num) *line_num=line;
+
+   } else  /*(!found_file_line &&  found_fn)*/ {
+     CLG_(stat).fn_name_debug_BBs++;
+     *file = "???";
+     if (line_num) *line_num=0;
+   }
+
+   CLG_DEBUG(6, "  - get_debug_info(%#lx): seg '%s', fn %s\n",
+	    instr_addr,
+	    !pDebugInfo   ? "-" :
+	    (*pDebugInfo) ? VG_(DebugInfo_get_filename)(*pDebugInfo) :
+	    "(None)",
+	    *fn_name);
+
+  return result;
+}
+
+/* for _libc_freeres_wrapper => _exit renaming */
+static BB* exit_bb = 0;
+
+
+/*
+ * Attach function struct to a BB from debug info.
+ */
+fn_node* CLG_(get_fn_node)(BB* bb)
+{
+    const HChar *fnname, *filename, *dirname;
+    DebugInfo* di;
+    UInt       line_num;
+    fn_node*   fn;
+
+    /* fn from debug info is idempotent for a BB */
+    if (bb->fn) return bb->fn;
+
+    CLG_DEBUG(3,"+ get_fn_node(BB %#lx)\n", bb_addr(bb));
+
+    /* get function/file name, line number and object of
+     * the BB according to debug information
+     */
+    CLG_(get_debug_info)(bb_addr(bb),
+                         &dirname, &filename, &fnname, &line_num, &di);
+
+    if (0 == VG_(strcmp)(fnname, "???")) {
+	int p;
+        static HChar buf[32];  // for sure large enough
+	/* Use address as found in library */
+	if (sizeof(Addr) == 4)
+          p = VG_(sprintf)(buf, "%#08lx", (UWord)bb->offset);
+	else 	    
+	    // 64bit address
+          p = VG_(sprintf)(buf, "%#016lx", (UWord)bb->offset);
+
+	VG_(sprintf)(buf + p, "%s", 
+		     (bb->sect_kind == Vg_SectData) ? " [Data]" :
+		     (bb->sect_kind == Vg_SectBSS)  ? " [BSS]"  :
+		     (bb->sect_kind == Vg_SectGOT)  ? " [GOT]"  :
+		     (bb->sect_kind == Vg_SectPLT)  ? " [PLT]"  : "");
+        fnname = buf;
+    }
+    else {
+      if (VG_(get_fnname_if_entry)(bb_addr(bb), &fnname))
+	bb->is_entry = 1;
+    }
+
+    /* HACK for correct _exit: 
+     * _exit is redirected to VG_(__libc_freeres_wrapper) by valgrind,
+     * so we rename it back again :-)
+     */
+    if (0 == VG_(strcmp)(fnname, "vgPlain___libc_freeres_wrapper")
+	&& exit_bb) {
+      CLG_(get_debug_info)(bb_addr(exit_bb),
+                           &dirname, &filename, &fnname, &line_num, &di);
+	
+	CLG_DEBUG(1, "__libc_freeres_wrapper renamed to _exit\n");
+    }
+    if (0 == VG_(strcmp)(fnname, "_exit") && !exit_bb)
+	exit_bb = bb;
+    
+    if (runtime_resolve_addr && 
+	(bb_addr(bb) >= runtime_resolve_addr) &&
+	(bb_addr(bb) < runtime_resolve_addr + runtime_resolve_length)) {
+	/* BB in runtime_resolve found by code check; use this name */
+      fnname = "_dl_runtime_resolve";
+    }
+
+    /* get fn_node struct for this function */
+    fn = get_fn_node_inseg( di, dirname, filename, fnname);
+
+    /* if this is the 1st time the function is seen,
+     * some attributes are set */
+    if (fn->pure_cxt == 0) {
+
+      /* Every function gets a "pure" context, i.e. a context with stack
+       * depth 1 only with this function. This is for compression of mangled
+       * names
+       */
+      fn_node* pure[2];
+      pure[0] = 0;
+      pure[1] = fn;
+      fn->pure_cxt = CLG_(get_cxt)(pure+1);
+
+      if (bb->sect_kind == Vg_SectPLT)	
+	fn->skip = CLG_(clo).skip_plt;
+
+      if (VG_(strcmp)(fn->name, "_dl_runtime_resolve")==0) {
+	  fn->pop_on_jump = True;
+
+	  if (VG_(clo_verbosity) > 1)
+	      VG_(message)(Vg_DebugMsg, "Symbol match: found runtime_resolve:"
+                                        " %s +%#lx=%#lx\n",
+		      bb->obj->name + bb->obj->last_slash_pos,
+                      (UWord)bb->offset, bb_addr(bb));
+      }
+
+      fn->is_malloc  = (VG_(strcmp)(fn->name, "malloc")==0);
+      fn->is_realloc = (VG_(strcmp)(fn->name, "realloc")==0);
+      fn->is_free    = (VG_(strcmp)(fn->name, "free")==0);
+
+      /* apply config options from function name patterns
+       * given on command line */
+      CLG_(update_fn_config)(fn);
+    }
+
+
+    bb->fn   = fn;
+    bb->line = line_num;
+
+    if (dirname[0]) {
+       CLG_DEBUG(3,"- get_fn_node(BB %#lx): %s (in %s:%u)\n",
+                 bb_addr(bb), fnname, filename, line_num);
+    } else
+       CLG_DEBUG(3,"- get_fn_node(BB %#lx): %s (in %s/%s:%u)\n",
+                 bb_addr(bb), fnname, dirname, filename, line_num);
+
+    return fn;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Active function array operations                     ---*/
+/*------------------------------------------------------------*/
+
+/* The active function array is a thread-specific array
+ * of UInts, mapping function numbers to the active count of
+ * functions.
+ * The active count is the number of times a function appears
+ * in the current call stack, and is used when costs for recursion
+ * levels should be separated.
+ */
+
+UInt* CLG_(get_fn_entry)(Int n)
+{
+  CLG_ASSERT(n < current_fn_active.size);
+  return current_fn_active.array + n;
+}
+
+void CLG_(init_fn_array)(fn_array* a)
+{
+  Int i;
+
+  CLG_ASSERT(a != 0);
+
+  a->size = N_INITIAL_FN_ARRAY_SIZE;
+  if (a->size <= CLG_(stat).distinct_fns)
+    a->size = CLG_(stat).distinct_fns+1;
+  
+  a->array = (UInt*) CLG_MALLOC("cl.fn.gfe.1",
+                                a->size * sizeof(UInt));
+  for(i=0;i<a->size;i++)
+    a->array[i] = 0;
+}
+
+void CLG_(copy_current_fn_array)(fn_array* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size  = current_fn_active.size;
+  dst->array = current_fn_active.array;
+}
+
+fn_array* CLG_(get_current_fn_array)()
+{
+  return &current_fn_active;
+}
+
+void CLG_(set_current_fn_array)(fn_array* a)
+{
+  CLG_ASSERT(a != 0);
+
+  current_fn_active.size  = a->size;
+  current_fn_active.array = a->array;
+  if (current_fn_active.size <= CLG_(stat).distinct_fns)
+    resize_fn_array();
+}
+
+/* ensure that active_array is big enough:
+ *  <distinct_fns> is the highest index, so <fn_active_array_size>
+ *  has to be bigger than that.
+ */
+static void resize_fn_array(void)
+{
+    UInt* new_array;
+    Int i;
+
+    UInt newsize = current_fn_active.size;
+    while (newsize <= CLG_(stat).distinct_fns) newsize *=2;
+
+    CLG_DEBUG(0, "Resize fn_active_array: %u => %u\n",
+	     current_fn_active.size, newsize);
+
+    new_array = (UInt*) CLG_MALLOC("cl.fn.rfa.1", newsize * sizeof(UInt));
+    for(i=0;i<current_fn_active.size;i++)
+      new_array[i] = current_fn_active.array[i];
+    while(i<newsize)
+	new_array[i++] = 0;
+
+    VG_(free)(current_fn_active.array);
+    current_fn_active.size = newsize;
+    current_fn_active.array = new_array;
+    CLG_(stat).fn_array_resizes++;
+}
+
+
diff --git a/sigrind/global.h b/sigrind/global.h
new file mode 100644
index 000000000..5ead5a6b6
--- /dev/null
+++ b/sigrind/global.h
@@ -0,0 +1,886 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind data structures, functions.               global.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2004-2015 Josef Weidendorfer
+      josef.weidendorfer@gmx.de
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef CLG_GLOBAL
+#define CLG_GLOBAL
+
+#include "pub_tool_basics.h"
+#include "pub_tool_vki.h"
+#include "pub_tool_debuginfo.h"
+#include "pub_tool_libcbase.h"
+#include "pub_tool_libcassert.h"
+#include "pub_tool_libcfile.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_libcproc.h"
+#include "pub_tool_machine.h"
+#include "pub_tool_mallocfree.h"
+#include "pub_tool_options.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_xarray.h"
+#include "pub_tool_clientstate.h"
+#include "pub_tool_machine.h"      // VG_(fnptr_to_fnentry)
+
+#include "events.h" // defines CLG_ macro
+
+#define SGL_(str) VGAPPEND(vgSigrind_,str)
+
+
+/*------------------------------------------------------------*/
+/*--- Callgrind compile options                           --- */
+/*------------------------------------------------------------*/
+
+/* Enable debug output */
+#define CLG_ENABLE_DEBUG 1
+
+/* Enable experimental features? */
+#define CLG_EXPERIMENTAL 0
+
+/* Syscall Timing in microseconds? 
+ * (define to 0 if you get compile errors) */
+#define CLG_MICROSYSTIME 0
+
+
+
+/*------------------------------------------------------------*/
+/*--- Command line options                                 ---*/
+/*------------------------------------------------------------*/
+
+#define DEFAULT_OUTFORMAT   "callgrind.out.%p"
+
+typedef struct _SglCommandLineOptions SglCommandLineOptions;
+struct _SglCommandLineOptions {
+  const HChar* ipc_dir;
+  const HChar* collect_func;
+  const HChar* start_collect_func;
+  const HChar* stop_collect_func;
+  Bool gen_mem;
+  Bool gen_comp;
+  Bool gen_cf;
+  Bool gen_sync;
+  Bool gen_instr;
+  Bool gen_bb;
+  Bool gen_fn;
+  Bool gen_thr;
+};
+
+typedef struct _CommandLineOptions CommandLineOptions;
+struct _CommandLineOptions {
+
+  /* Dump format options */
+  const HChar* out_format;  /* Format string for callgrind output file name */
+  Bool combine_dumps;       /* Dump trace parts into same file? */
+  Bool compress_strings;
+  Bool compress_events;
+  Bool compress_pos;
+  Bool mangle_names;
+  Bool compress_mangled;
+  Bool dump_line;
+  Bool dump_instr;
+  Bool dump_bb;
+  Bool dump_bbs;         /* Dump basic block information? */
+  
+  /* Dump generation options */
+  ULong dump_every_bb;     /* Dump every xxx BBs. */
+  
+  /* Collection options */
+  Bool separate_threads; /* Separate threads in dump? */
+  Int  separate_callers; /* Separate dependent on how many callers? */
+  Int  separate_recursions; /* Max level of recursions to separate */
+  Bool skip_plt;         /* Skip functions in PLT section? */
+  Bool skip_direct_recursion; /* Increment direct recursions the level? */
+
+  Bool collect_atstart;  /* Start in collecting state ? */
+  Bool collect_jumps;    /* Collect (cond.) jumps in functions ? */
+
+  Bool collect_alloc;    /* Collect size of allocated memory */
+  Bool collect_systime;  /* Collect time for system calls */
+
+  Bool collect_bus;      /* Collect global bus events */
+
+  /* Instrument options */
+  Bool instrument_atstart;  /* Instrument at start? */
+  Bool simulate_cache;      /* Call into cache simulator ? */
+  Bool simulate_branch;     /* Call into branch prediction simulator ? */
+
+  /* Call graph generation */
+  Bool pop_on_jump;       /* Handle a jump between functions as ret+call */
+
+#if CLG_ENABLE_DEBUG
+  Int   verbose;
+  ULong verbose_start;
+#endif
+};
+
+/*------------------------------------------------------------*/
+/*--- Constants                                            ---*/
+/*------------------------------------------------------------*/
+
+/* Minimum cache line size allowed */
+#define MIN_LINE_SIZE   16
+
+
+/*------------------------------------------------------------*/
+/*--- Statistics                                           ---*/
+/*------------------------------------------------------------*/
+
+typedef struct _Statistics Statistics;
+struct _Statistics {
+  ULong call_counter;
+  ULong jcnd_counter;
+  ULong jump_counter;
+  ULong rec_call_counter;
+  ULong ret_counter;
+  ULong bb_executions;
+
+  Int  context_counter;
+  Int  bb_retranslations;  
+
+  Int  distinct_objs;
+  Int  distinct_files;
+  Int  distinct_fns;
+  Int  distinct_contexts;
+  Int  distinct_bbs;
+  Int  distinct_jccs;
+  Int  distinct_bbccs;
+  Int  distinct_instrs;
+  Int  distinct_skips;
+
+  Int  bb_hash_resizes;
+  Int  bbcc_hash_resizes;
+  Int  jcc_hash_resizes;
+  Int  cxt_hash_resizes;
+  Int  fn_array_resizes;
+  Int  call_stack_resizes;
+  Int  fn_stack_resizes;
+
+  Int  full_debug_BBs;
+  Int  file_line_debug_BBs;
+  Int  fn_name_debug_BBs;
+  Int  no_debug_BBs;
+  Int  bbcc_lru_misses;
+  Int  jcc_lru_misses;
+  Int  cxt_lru_misses;
+  Int  bbcc_clones;
+};
+
+
+/*------------------------------------------------------------*/
+/*--- Structure declarations                               ---*/
+/*------------------------------------------------------------*/
+
+typedef struct _Context     Context;
+typedef struct _CC          CC;
+typedef struct _BB          BB;
+typedef struct _BBCC        BBCC;
+typedef struct _jCC         jCC;
+typedef struct _fCC         fCC;
+typedef struct _fn_node     fn_node;
+typedef struct _file_node   file_node;
+typedef struct _obj_node    obj_node;
+typedef struct _fn_config   fn_config;
+typedef struct _call_entry  call_entry;
+typedef struct _thread_info thread_info;
+
+/* Costs of event sets. Aliases to arrays of 64-bit values */
+typedef ULong* SimCost;  /* All events the simulator can produce */
+typedef ULong* UserCost;
+typedef ULong* FullCost; /* Simulator + User */
+
+
+/* The types of control flow changes that can happen between
+ * execution of two BBs in a thread.
+ */
+typedef enum {
+  jk_None = 0,   /* no explicit change by a guest instruction */
+  jk_Jump,       /* regular jump */
+  jk_Call,
+  jk_Return,
+  jk_CondJump    /* conditional jump taken (only used as jCC type) */
+} ClgJumpKind;
+
+
+/* JmpCall cost center
+ * for subroutine call (from->bb->jmp_addr => to->bb->addr)
+ *
+ * Each BB has at most one CALL instruction. The list of JCC from
+ * this call is a pointer to the list head (stored in BBCC), and
+ * <next_from> in the JCC struct.
+ *
+ * For fast lookup, JCCs are reachable with a hash table, keyed by
+ * the (from_bbcc,to) pair. <next_hash> is used for the JCC chain
+ * of one hash table entry.
+ *
+ * Cost <sum> holds event counts for already returned executions.
+ * <last> are the event counters at last enter of the subroutine.
+ * <sum> is updated on returning from the subroutine by
+ * adding the diff of <last> and current event counters to <sum>.
+ *
+ * After updating, <last> is set to current event counters. Thus,
+ * events are not counted twice for recursive calls (TODO: True?)
+ */
+
+struct _jCC {
+  ClgJumpKind jmpkind; /* jk_Call, jk_Jump, jk_CondJump */
+  jCC* next_hash;   /* for hash entry chain */
+  jCC* next_from;   /* next JCC from a BBCC */
+  BBCC *from, *to;  /* call arc from/to this BBCC */
+  UInt jmp;         /* jump no. in source */
+
+  ULong call_counter; /* no wraparound with 64 bit */
+
+  FullCost cost; /* simulator + user counters */
+};
+
+
+/* 
+ * Info for one instruction of a basic block.
+ */
+typedef struct _InstrInfo InstrInfo;
+struct _InstrInfo {
+  UInt instr_offset;
+  Addr instr_addr;
+  UInt instr_size;
+  UInt cost_offset;
+  EventSet* eventset;
+};
+
+
+
+/*
+ * Info for a side exit in a BB
+ */
+typedef struct _CJmpInfo CJmpInfo;
+struct _CJmpInfo {
+  UInt instr;          /* instruction index for BB.instr array */
+  ClgJumpKind jmpkind; /* jump kind when leaving BB at this side exit */
+};
+
+
+/**
+ * An instrumented basic block (BB).
+ *
+ * BBs are put into a resizable hash to allow for fast detection if a
+ * BB is to be retranslated but cost info is already available.
+ * The key for a BB is a (object, offset) tupel making it independent
+ * from possibly multiple mappings of the same ELF object.
+ *
+ * At the beginning of each instrumented BB,
+ * a call to setup_bbcc(), specifying a pointer to the
+ * according BB structure, is added.
+ *
+ * As cost of a BB has to be distinguished depending on the context,
+ * multiple cost centers for one BB (struct BBCC) exist and the according
+ * BBCC is set by setup_bbcc.
+ */
+struct _BB {
+  obj_node*  obj;         /* ELF object of BB */
+  PtrdiffT   offset;      /* offset of BB in ELF object file */
+  BB*        next;       /* chaining for a hash entry */
+
+  VgSectKind sect_kind;  /* section of this BB, e.g. PLT */
+  UInt       instr_count;
+  
+  /* filled by CLG_(get_fn_node) if debug info is available */
+  fn_node*   fn;          /* debug info for this BB */
+  UInt       line;
+  Bool       is_entry;    /* True if this BB is a function entry */
+        
+  BBCC*      bbcc_list;  /* BBCCs for same BB (see next_bbcc in BBCC) */
+  BBCC*      last_bbcc;  /* Temporary: Cached for faster access (LRU) */
+
+  /* filled by CLG_(instrument) if not seen before */
+  UInt       cjmp_count;  /* number of side exits */
+  CJmpInfo*  jmp;         /* array of info for condition jumps,
+			   * allocated directly after this struct */
+  Bool       cjmp_inverted; /* is last side exit actually fall through? */
+
+  UInt       instr_len;
+  UInt       cost_count;
+  InstrInfo  instr[0];   /* info on instruction sizes and costs */
+};
+
+
+
+/**
+ * Function context
+ *
+ * Basic blocks are always executed in the scope of a context.
+ * A function context is a list of function nodes representing
+ * the call chain to the current context: I.e. fn[0] is the
+ * function we are currently in, fn[1] has called fn[0], and so on.
+ * Recursion levels are used for fn[0].
+ *
+ * To get a unique number for a full execution context, use
+ *  rec_index = min(<fn->rec_separation>,<active>) - 1;
+ *  unique_no = <number> + rec_index
+ *
+ * For each Context, recursion index and BB, there can be a BBCC.
+ */
+struct _Context {
+    UInt size;        // number of function dependencies
+    UInt base_number; // for context compression & dump array
+    Context* next;    // entry chaining for hash
+    UWord hash;       // for faster lookup...
+    fn_node* fn[0];
+};
+
+
+/*
+ * Cost info for a side exits from a BB
+ */
+typedef struct _JmpData JmpData;
+struct _JmpData {
+    ULong ecounter; /* number of times the BB was left at this exit */
+    jCC*  jcc_list; /* JCCs used for this exit */
+};
+
+
+/*
+ * Basic Block Cost Center
+ *
+ * On demand, multiple BBCCs will be created for the same BB
+ * dependend on command line options and:
+ * - current function (it's possible that a BB is executed in the
+ *   context of different functions, e.g. in manual assembler/PLT)
+ * - current thread ID
+ * - position where current function is called from
+ * - recursion level of current function
+ *
+ * The cost centres for the instructions of a basic block are
+ * stored in a contiguous array.
+ * They are distinguishable by their tag field.
+ */
+struct _BBCC {
+    BB*      bb;           /* BB for this cost center */
+
+    Context* cxt;          /* execution context of this BBCC */
+    ThreadId tid;          /* only for assertion check purpose */
+    UInt     rec_index;    /* Recursion index in rec->bbcc for this bbcc */
+    BBCC**   rec_array;    /* Variable sized array of pointers to 
+			    * recursion BBCCs. Shared. */
+    ULong    ret_counter;  /* how often returned from jccs of this bbcc;
+			    * used to check if a dump for this BBCC is needed */
+    
+    BBCC*    next_bbcc;    /* Chain of BBCCs for same BB */
+    BBCC*    lru_next_bbcc; /* BBCC executed next the last time */
+    
+    jCC*     lru_from_jcc; /* Temporary: Cached for faster access (LRU) */
+    jCC*     lru_to_jcc;   /* Temporary: Cached for faster access (LRU) */
+    FullCost skipped;      /* cost for skipped functions called from 
+			    * jmp_addr. Allocated lazy */
+    
+    BBCC*    next;         /* entry chain in hash */
+    ULong*   cost;         /* start of 64bit costs for this BBCC */
+    ULong    ecounter_sum; /* execution counter for first instruction of BB */
+    JmpData  jmp[0];
+};
+
+
+/* the <number> of fn_node, file_node and obj_node are for compressed dumping
+ * and a index into the dump boolean table and fn_info_table
+ */
+
+struct _fn_node {
+  HChar*     name;
+  UInt       number;
+  Context*   last_cxt; /* LRU info */
+  Context*   pure_cxt; /* the context with only the function itself */
+  file_node* file;     /* reverse mapping for 2nd hash */
+  fn_node* next;
+
+  Bool dump_before :1;
+  Bool dump_after :1;
+  Bool zero_before :1;
+  Bool toggle_collect :1;
+  Bool skip :1;
+  Bool pop_on_jump : 1;
+
+  Bool is_malloc :1;
+  Bool is_realloc :1;
+  Bool is_free :1;
+
+  Int  group;
+  Int  separate_callers;
+  Int  separate_recursions;
+#if CLG_ENABLE_DEBUG
+  Int  verbosity; /* Stores old verbosity level while in function */
+#endif
+};
+
+/* Quite arbitrary fixed hash sizes */
+
+#define   N_OBJ_ENTRIES         47
+#define  N_FILE_ENTRIES         53
+#define    N_FN_ENTRIES         87
+
+struct _file_node {
+   HChar*     name;
+   fn_node*   fns[N_FN_ENTRIES];
+   UInt       number;
+   obj_node*  obj;
+   file_node* next;
+};
+
+/* If an object is dlopened multiple times, we hope that <name> is unique;
+ * <start> and <offset> can change with each dlopen, and <start> is
+ * zero when object is unmapped (possible at dump time).
+ */
+struct _obj_node {
+   const HChar* name;
+   UInt       last_slash_pos;
+
+   Addr       start;  /* Start address of text segment mapping */
+   SizeT      size;   /* Length of mapping */
+   PtrdiffT   offset; /* Offset between symbol address and file offset */
+
+   file_node* files[N_FILE_ENTRIES];
+   UInt       number;
+   obj_node*  next;
+};
+
+/* an entry in the callstack
+ *
+ * <nonskipped> is 0 if the function called is not skipped (usual case).
+ * Otherwise, it is the last non-skipped BBCC. This one gets all
+ * the calls to non-skipped functions and all costs in skipped 
+ * instructions.
+ */
+struct _call_entry {
+    jCC* jcc;           /* jCC for this call */
+    FullCost enter_cost; /* cost event counters at entering frame */
+    Addr sp;            /* stack pointer directly after call */
+    Addr ret_addr;      /* address to which to return to
+			 * is 0 on a simulated call */
+    BBCC* nonskipped;   /* see above */
+    Context* cxt;       /* context before call */
+    Int fn_sp;          /* function stack index before call */
+};
+
+
+/*
+ * Execution state of main thread or a running signal handler in
+ * a thread while interrupted by another signal handler.
+ * As there's no scheduling among running signal handlers of one thread,
+ * we only need a subset of a full thread state:
+ * - event counter
+ * - collect state
+ * - last BB, last jump kind, last nonskipped BB
+ * - callstack pointer for sanity checking and correct unwinding
+ *   after exit
+ */
+typedef struct _exec_state exec_state;
+struct _exec_state {
+
+  /* the signum of the handler, 0 for main thread context
+   */
+  Int sig;
+  
+  /* the old call stack pointer at entering the signal handler */
+  Int orig_sp;
+  
+  FullCost cost;
+  Bool     collect;
+  Context* cxt;
+  
+  /* number of conditional jumps passed in last BB */
+  Int   jmps_passed;
+  BBCC* bbcc;      /* last BB executed */
+  BBCC* nonskipped;
+
+  Int call_stack_bottom; /* Index into fn_stack */
+};
+
+/* Global state structures */
+typedef struct _bb_hash bb_hash;
+struct _bb_hash {
+  UInt size, entries;
+  BB** table;
+};
+
+typedef struct _cxt_hash cxt_hash;
+struct _cxt_hash {
+  UInt size, entries;
+  Context** table;
+};  
+
+/* Thread specific state structures, i.e. parts of a thread state.
+ * There are variables for the current state of each part,
+ * on which a thread state is copied at thread switch.
+ */
+typedef struct _bbcc_hash bbcc_hash;
+struct _bbcc_hash {
+  UInt size, entries;
+  BBCC** table;
+};
+
+typedef struct _jcc_hash jcc_hash;
+struct _jcc_hash {
+  UInt size, entries;
+  jCC** table;
+  jCC* spontaneous;
+};
+
+typedef struct _fn_array fn_array;
+struct _fn_array {
+  UInt size;
+  UInt* array;
+};
+
+typedef struct _call_stack call_stack;
+struct _call_stack {
+  UInt size;
+  Int sp;
+  call_entry* entry;
+};
+
+typedef struct _fn_stack fn_stack;
+struct _fn_stack {
+  UInt size;
+  fn_node **bottom, **top;
+};
+
+/* The maximum number of simultaneous running signal handlers per thread.
+ * This is the number of execution states storable in a thread.
+ */
+#define MAX_SIGHANDLERS 10
+
+typedef struct _exec_stack exec_stack;
+struct _exec_stack {
+  Int sp; /* > 0 if a handler is running */
+  exec_state* entry[MAX_SIGHANDLERS];
+};
+
+/* Thread State 
+ *
+ * This structure stores thread specific info while a thread is *not*
+ * running. See function switch_thread() for save/restore on thread switch.
+ *
+ * If --separate-threads=no, BBCCs and JCCs can be shared by all threads, i.e.
+ * only structures of thread 1 are used.
+ * This involves variables fn_info_table, bbcc_table and jcc_table.
+ */
+struct _thread_info {
+
+  /* state */
+  fn_stack fns;       /* function stack */
+  call_stack calls;   /* context call arc stack */
+  exec_stack states;  /* execution states interrupted by signals */
+
+  /* dump statistics */
+  FullCost lastdump_cost;    /* Cost at last dump */
+  FullCost sighandler_cost;
+
+  /* thread specific data structure containers */
+  fn_array fn_active;
+  jcc_hash jccs;
+  bbcc_hash bbccs;
+};
+
+/* Structs used for dumping */
+
+/* Address position inside of a BBCC:
+ * This includes
+ * - the address offset from the BB start address
+ * - file/line from debug info for that address (can change inside a BB)
+ */
+typedef struct _AddrPos AddrPos;
+struct _AddrPos {
+    Addr addr;
+    Addr bb_addr;
+    file_node* file;
+    UInt line;
+};
+
+/* a simulator cost entity that can be written out in one line */
+typedef struct _AddrCost AddrCost;
+struct _AddrCost {
+    AddrPos p;
+    SimCost cost;
+};
+
+/* A function in an execution context */
+typedef struct _FnPos FnPos;
+struct _FnPos {
+    file_node* file;
+    fn_node* fn;
+    obj_node* obj;
+    Context* cxt;
+    int rec_index;
+    UInt line;
+};
+
+/*------------------------------------------------------------*/
+/*--- Cache simulator interface                            ---*/
+/*------------------------------------------------------------*/
+
+struct cachesim_if
+{
+    void (*print_opts)(void);
+    Bool (*parse_opt)(const HChar* arg);
+    void (*post_clo_init)(void);
+    void (*clear)(void);
+    void (*dump_desc)(VgFile *fp);
+    void (*printstat)(Int,Int,Int);
+    void (*add_icost)(SimCost, BBCC*, InstrInfo*, ULong);
+    void (*finish)(void);
+    
+    void (*log_1I0D)(InstrInfo*) VG_REGPARM(1);
+    void (*log_2I0D)(InstrInfo*, InstrInfo*) VG_REGPARM(2);
+    void (*log_3I0D)(InstrInfo*, InstrInfo*, InstrInfo*) VG_REGPARM(3);
+
+    void (*log_1I1Dr)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+    void (*log_1I1Dw)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+
+    void (*log_0I1Dr)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+    void (*log_0I1Dw)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+
+    // function names of helpers (for debugging generated code)
+    const HChar *log_1I0D_name, *log_2I0D_name, *log_3I0D_name;
+    const HChar *log_1I1Dr_name, *log_1I1Dw_name;
+    const HChar *log_0I1Dr_name, *log_0I1Dw_name;
+};
+
+// Event groups
+#define EG_USE   0
+#define EG_IR    1
+#define EG_DR    2
+#define EG_DW    3
+#define EG_BC    4
+#define EG_BI    5
+#define EG_BUS   6
+#define EG_ALLOC 7
+#define EG_SYS   8
+
+struct event_sets {
+    EventSet *base, *full;
+};
+
+#define fullOffset(group) (CLG_(sets).full->offset[group])
+
+
+/*------------------------------------------------------------*/
+/*--- Functions                                            ---*/
+/*------------------------------------------------------------*/
+
+/* from clo.c */
+
+void SGL_(set_clo_defaults)(void);
+void CLG_(set_clo_defaults)(void);
+void CLG_(update_fn_config)(fn_node*);
+Bool CLG_(process_cmd_line_option)(const HChar*);
+void CLG_(print_usage)(void);
+void CLG_(print_debug_usage)(void);
+
+/* from sim.c */
+void CLG_(init_eventsets)(void);
+
+/* from main.c */
+Bool CLG_(get_debug_info)(Addr, const HChar **dirname,
+                          const HChar **filename,
+                          const HChar **fn_name, UInt*, DebugInfo**);
+void CLG_(collectBlockInfo)(IRSB* bbIn, UInt*, UInt*, Bool*);
+void CLG_(set_instrument_state)(const HChar*,Bool);
+void CLG_(dump_profile)(const HChar* trigger,Bool only_current_thread);
+void CLG_(zero_all_cost)(Bool only_current_thread);
+Int CLG_(get_dump_counter)(void);
+void CLG_(fini)(Int exitcode);
+
+/* from bb.c */
+void CLG_(init_bb_hash)(void);
+bb_hash* CLG_(get_bb_hash)(void);
+BB*  CLG_(get_bb)(Addr addr, IRSB* bb_in, Bool *seen_before);
+void CLG_(delete_bb)(Addr addr);
+
+static __inline__ Addr bb_addr(BB* bb)
+ { return bb->offset + bb->obj->offset; }
+static __inline__ Addr bb_jmpaddr(BB* bb)
+ { UInt off = (bb->instr_count > 0) ? bb->instr[bb->instr_count-1].instr_offset : 0;
+   return off + bb->offset + bb->obj->offset; }
+
+/* from fn.c */
+void CLG_(init_fn_array)(fn_array*);
+void CLG_(copy_current_fn_array)(fn_array* dst);
+fn_array* CLG_(get_current_fn_array)(void);
+void CLG_(set_current_fn_array)(fn_array*);
+UInt* CLG_(get_fn_entry)(Int n);
+
+void      CLG_(init_obj_table)(void);
+obj_node* CLG_(get_obj_node)(DebugInfo* si);
+file_node* CLG_(get_file_node)(obj_node*, const HChar *dirname,
+                               const HChar* filename);
+fn_node*  CLG_(get_fn_node)(BB* bb);
+
+/* from bbcc.c */
+void CLG_(init_bbcc_hash)(bbcc_hash* bbccs);
+void CLG_(copy_current_bbcc_hash)(bbcc_hash* dst);
+bbcc_hash* CLG_(get_current_bbcc_hash)(void);
+void CLG_(set_current_bbcc_hash)(bbcc_hash*);
+void CLG_(forall_bbccs)(void (*func)(BBCC*));
+void CLG_(zero_bbcc)(BBCC* bbcc);
+BBCC* CLG_(get_bbcc)(BB* bb);
+BBCC* CLG_(clone_bbcc)(BBCC* orig, Context* cxt, Int rec_index);
+void CLG_(setup_bbcc)(BB* bb) VG_REGPARM(1);
+
+
+/* from jumps.c */
+void CLG_(init_jcc_hash)(jcc_hash*);
+void CLG_(copy_current_jcc_hash)(jcc_hash* dst);
+void CLG_(set_current_jcc_hash)(jcc_hash*);
+jCC* CLG_(get_jcc)(BBCC* from, UInt, BBCC* to);
+
+/* from callstack.c */
+void CLG_(init_call_stack)(call_stack*);
+void CLG_(copy_current_call_stack)(call_stack* dst);
+void CLG_(set_current_call_stack)(call_stack*);
+call_entry* CLG_(get_call_entry)(Int n);
+
+void CLG_(push_call_stack)(BBCC* from, UInt jmp, BBCC* to, Addr sp, Bool skip);
+void CLG_(pop_call_stack)(void);
+Int CLG_(unwind_call_stack)(Addr sp, Int);
+
+/* from context.c */
+void CLG_(init_fn_stack)(fn_stack*);
+void CLG_(copy_current_fn_stack)(fn_stack*);
+void CLG_(set_current_fn_stack)(fn_stack*);
+
+void CLG_(init_cxt_table)(void);
+Context* CLG_(get_cxt)(fn_node** fn);
+void CLG_(push_cxt)(fn_node* fn);
+
+/* from threads.c */
+void CLG_(init_threads)(void);
+thread_info** CLG_(get_threads)(void);
+thread_info* CLG_(get_current_thread)(void);
+void CLG_(switch_thread)(ThreadId tid);
+void CLG_(forall_threads)(void (*func)(thread_info*));
+void CLG_(run_thread)(ThreadId tid);
+void SGL_(switch_thread)(ThreadId tid);
+
+void CLG_(init_exec_state)(exec_state* es);
+void CLG_(init_exec_stack)(exec_stack*);
+void CLG_(copy_current_exec_stack)(exec_stack*);
+void CLG_(set_current_exec_stack)(exec_stack*);
+void CLG_(pre_signal)(ThreadId tid, Int sigNum, Bool alt_stack);
+void CLG_(post_signal)(ThreadId tid, Int sigNum);
+void CLG_(run_post_signal_on_call_stack_bottom)(void);
+
+/* from dump.c */
+void CLG_(init_dumps)(void);
+
+/*------------------------------------------------------------*/
+/*--- Exported global variables                            ---*/
+/*------------------------------------------------------------*/
+
+extern Bool SGL_(is_in_event_collect_func); //ML: long name? feel free to change
+extern SglCommandLineOptions SGL_(clo);
+extern Bool* SGL_(thread_in_synccall);
+extern ThreadId SGL_(active_tid);
+
+#define EVENT_GENERATION_ENABLED  \
+   (!SGL_(thread_in_synccall)[SGL_(active_tid)] && (SGL_(is_in_event_collect_func)))
+
+extern CommandLineOptions CLG_(clo);
+extern Statistics CLG_(stat);
+extern EventMapping* CLG_(dumpmap);
+
+/* Function active counter array, indexed by function number */
+extern UInt* CLG_(fn_active_array);
+extern Bool CLG_(instrument_state);
+ /* min of L1 and LL cache line sizes */
+extern Int CLG_(min_line_size);
+extern call_stack CLG_(current_call_stack);
+extern fn_stack   CLG_(current_fn_stack);
+extern exec_state CLG_(current_state);
+extern ThreadId   CLG_(current_tid);
+extern FullCost   CLG_(total_cost);
+extern struct cachesim_if CLG_(cachesim);
+extern struct event_sets  CLG_(sets);
+
+// set by setup_bbcc at start of every BB, and needed by log_* helpers
+extern Addr   CLG_(bb_base);
+extern ULong* CLG_(cost_base);
+
+
+/*------------------------------------------------------------*/
+/*--- Debug output                                         ---*/
+/*------------------------------------------------------------*/
+
+#if CLG_ENABLE_DEBUG
+
+#define CLG_DEBUGIF(x) \
+  if (UNLIKELY( (CLG_(clo).verbose >x) && \
+                (CLG_(stat).bb_executions >= CLG_(clo).verbose_start)))
+
+#define CLG_DEBUG(x,format,args...)   \
+    CLG_DEBUGIF(x) {                  \
+      CLG_(print_bbno)();	      \
+      VG_(printf)(format,##args);     \
+    }
+
+#define CLG_ASSERT(cond)              \
+    if (UNLIKELY(!(cond))) {          \
+      CLG_(print_context)();          \
+      CLG_(print_bbno)();	      \
+      tl_assert(cond);                \
+     }
+
+#else
+#define CLG_DEBUGIF(x) if (0)
+#define CLG_DEBUG(x...) {}
+#define CLG_ASSERT(cond) tl_assert(cond);
+#endif
+
+/* from debug.c */
+void CLG_(print_bbno)(void);
+void CLG_(print_context)(void);
+void CLG_(print_jcc)(int s, jCC* jcc);
+void CLG_(print_bbcc)(int s, BBCC* bbcc);
+void CLG_(print_bbcc_fn)(BBCC* bbcc);
+void CLG_(print_execstate)(int s, exec_state* es);
+void CLG_(print_eventset)(int s, EventSet* es);
+void CLG_(print_cost)(int s, EventSet*, ULong* cost);
+void CLG_(print_bb)(int s, BB* bb);
+void CLG_(print_bbcc_cost)(int s, BBCC*);
+void CLG_(print_cxt)(int s, Context* cxt, int rec_index);
+void CLG_(print_short_jcc)(jCC* jcc);
+void CLG_(print_stackentry)(int s, int sp);
+void CLG_(print_addr)(Addr addr);
+void CLG_(print_addr_ln)(Addr addr);
+
+void* CLG_(malloc)(const HChar* cc, UWord s, const HChar* f);
+void* CLG_(free)(void* p, const HChar* f);
+#if 0
+#define CLG_MALLOC(_cc,x) CLG_(malloc)((_cc),x,__FUNCTION__)
+#define CLG_FREE(p)       CLG_(free)(p,__FUNCTION__)
+#else
+#define CLG_MALLOC(_cc,x) VG_(malloc)((_cc),x)
+#define CLG_FREE(p)       VG_(free)(p)
+#endif
+
+#endif /* CLG_GLOBAL */
diff --git a/sigrind/jumps.c b/sigrind/jumps.c
new file mode 100644
index 000000000..dbd453353
--- /dev/null
+++ b/sigrind/jumps.c
@@ -0,0 +1,233 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                   ct_jumps.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/*------------------------------------------------------------*/
+/*--- Jump Cost Center (JCC) operations, including Calls   ---*/
+/*------------------------------------------------------------*/
+
+#define N_JCC_INITIAL_ENTRIES  4437
+
+static jcc_hash current_jccs;
+
+void CLG_(init_jcc_hash)(jcc_hash* jccs)
+{
+   Int i;
+
+   CLG_ASSERT(jccs != 0);
+
+   jccs->size    = N_JCC_INITIAL_ENTRIES;
+   jccs->entries = 0;
+   jccs->table = (jCC**) CLG_MALLOC("cl.jumps.ijh.1",
+                                    jccs->size * sizeof(jCC*));
+   jccs->spontaneous = 0;
+
+   for (i = 0; i < jccs->size; i++)
+     jccs->table[i] = 0;
+}
+
+
+void CLG_(copy_current_jcc_hash)(jcc_hash* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size        = current_jccs.size;
+  dst->entries     = current_jccs.entries;
+  dst->table       = current_jccs.table;
+  dst->spontaneous = current_jccs.spontaneous;
+}
+
+void CLG_(set_current_jcc_hash)(jcc_hash* h)
+{
+  CLG_ASSERT(h != 0);
+
+  current_jccs.size        = h->size;
+  current_jccs.entries     = h->entries;
+  current_jccs.table       = h->table;
+  current_jccs.spontaneous = h->spontaneous;
+}
+
+__inline__
+static UInt jcc_hash_idx(BBCC* from, UInt jmp, BBCC* to, UInt size)
+{
+  return (UInt) ( (UWord)from + 7* (UWord)to + 13*jmp) % size;
+} 
+
+/* double size of jcc table  */
+static void resize_jcc_table(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    jCC** new_table;
+    UInt new_idx;
+    jCC *curr_jcc, *next_jcc;
+
+    new_size  = 2* current_jccs.size +3;
+    new_table = (jCC**) CLG_MALLOC("cl.jumps.rjt.1",
+                                   new_size * sizeof(jCC*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < current_jccs.size; i++) {
+	if (current_jccs.table[i] == NULL) continue;
+ 
+	curr_jcc = current_jccs.table[i];
+	while (NULL != curr_jcc) {
+	    next_jcc = curr_jcc->next_hash;
+
+	    new_idx = jcc_hash_idx(curr_jcc->from, curr_jcc->jmp,
+				    curr_jcc->to, new_size);
+
+	    curr_jcc->next_hash = new_table[new_idx];
+	    new_table[new_idx] = curr_jcc;
+	    if (curr_jcc->next_hash) {
+		conflicts1++;
+		if (curr_jcc->next_hash->next_hash)
+		    conflicts2++;
+	    }
+
+	    curr_jcc = next_jcc;
+	}
+    }
+
+    VG_(free)(current_jccs.table);
+
+
+    CLG_DEBUG(0, "Resize JCC Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     current_jccs.size, new_size,
+	     current_jccs.entries, conflicts1, conflicts2);
+
+    current_jccs.size  = new_size;
+    current_jccs.table = new_table;
+    CLG_(stat).jcc_hash_resizes++;
+}
+
+
+
+/* new jCC structure: a call was done to a BB of a BBCC 
+ * for a spontaneous call, from is 0 (i.e. caller unknown)
+ */
+static jCC* new_jcc(BBCC* from, UInt jmp, BBCC* to)
+{
+   jCC* jcc;
+   UInt new_idx;
+
+   /* check fill degree of jcc hash table and resize if needed (>80%) */
+   current_jccs.entries++;
+   if (10 * current_jccs.entries / current_jccs.size > 8)
+       resize_jcc_table();
+
+   jcc = (jCC*) CLG_MALLOC("cl.jumps.nj.1", sizeof(jCC));
+
+   jcc->from      = from;
+   jcc->jmp       = jmp;
+   jcc->to        = to;
+   jcc->jmpkind   = jk_Call;
+   jcc->call_counter = 0;
+   jcc->cost = 0;
+
+   /* insert into JCC chain of calling BBCC.
+    * This list is only used at dumping time */
+
+   if (from) {
+       /* Prohibit corruption by array overrun */
+       CLG_ASSERT((0 <= jmp) && (jmp <= from->bb->cjmp_count));
+       jcc->next_from = from->jmp[jmp].jcc_list;
+       from->jmp[jmp].jcc_list = jcc;
+   }
+   else {
+       jcc->next_from = current_jccs.spontaneous;
+       current_jccs.spontaneous = jcc;
+   }
+
+   /* insert into JCC hash table */
+   new_idx = jcc_hash_idx(from, jmp, to, current_jccs.size);
+   jcc->next_hash = current_jccs.table[new_idx];
+   current_jccs.table[new_idx] = jcc;
+
+   CLG_(stat).distinct_jccs++;
+
+   CLG_DEBUGIF(3) {
+     VG_(printf)("  new_jcc (now %d): %p\n",
+		 CLG_(stat).distinct_jccs, jcc);
+   }
+
+   return jcc;
+}
+
+
+/* get the jCC for a call arc (BBCC->BBCC) */
+jCC* CLG_(get_jcc)(BBCC* from, UInt jmp, BBCC* to)
+{
+    jCC* jcc;
+    UInt idx;
+
+    CLG_DEBUG(5, "+ get_jcc(bbcc %p/%u => bbcc %p)\n",
+		from, jmp, to);
+
+    /* first check last recently used JCC */
+    jcc = to->lru_to_jcc;
+    if (jcc && (jcc->from == from) && (jcc->jmp == jmp)) {
+	CLG_ASSERT(to == jcc->to);
+	CLG_DEBUG(5,"- get_jcc: [LRU to] jcc %p\n", jcc);
+	return jcc;
+    }
+
+    jcc = from->lru_from_jcc;
+    if (jcc && (jcc->to == to) && (jcc->jmp == jmp)) {
+	CLG_ASSERT(from == jcc->from);
+	CLG_DEBUG(5, "- get_jcc: [LRU from] jcc %p\n", jcc);
+	return jcc;
+    }
+
+    CLG_(stat).jcc_lru_misses++;
+
+    idx = jcc_hash_idx(from, jmp, to, current_jccs.size);
+    jcc = current_jccs.table[idx];
+
+    while(jcc) {
+	if ((jcc->from == from) &&
+	    (jcc->jmp == jmp) &&
+	    (jcc->to == to)) break;
+	jcc = jcc->next_hash;
+    }
+
+    if (!jcc)
+	jcc = new_jcc(from, jmp, to);
+
+    /* set LRU */
+    from->lru_from_jcc = jcc;
+    to->lru_to_jcc = jcc;
+
+    CLG_DEBUG(5, "- get_jcc(bbcc %p => bbcc %p)\n",
+		from, to);
+
+    return jcc;
+}
+
diff --git a/sigrind/log_events.c b/sigrind/log_events.c
new file mode 100644
index 000000000..74519a578
--- /dev/null
+++ b/sigrind/log_events.c
@@ -0,0 +1,239 @@
+/* This file is part of Callgrind, a Valgrind tool for call graph profiling programs.
+Copyright (C) 2003-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "log_events.h"
+#include "sigil2_ipc.h"
+#include "coregrind/pub_core_libcprint.h"
+
+//TODO(someday) these aren't needed for Sigil, but deleting them causes
+/* Following global vars are setup before by setup_bbcc():
+ *
+ * - Addr   CLG_(bb_base)     (instruction start address of original BB)
+ * - ULong* CLG_(cost_base)   (start of cost array for BB)
+ */
+Addr   CLG_(bb_base);
+ULong* CLG_(cost_base);
+
+//#define COUNT_EVENT_CHECK
+#ifdef COUNT_EVENT_CHECK
+static unsigned long long mem_events = 0;
+static unsigned long long comp_events = 0;
+static unsigned long long sync_events = 0;
+static unsigned long long cxt_events = 0;
+#endif
+
+void SGL_(end_logging)()
+{
+#ifdef COUNT_EVENT_CHECK
+    VG_(printf)("Total Mem Events: %llu\n",  mem_events);
+    VG_(printf)("Total Comp Events: %llu\n", comp_events);
+    VG_(printf)("Total Sync Events: %llu\n", sync_events);
+    VG_(printf)("Total Cxt Events: %llu\n",  cxt_events);
+#endif
+}
+
+
+void SGL_(log_1I0D)(InstrInfo* ii)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        cxt_events++;
+#endif
+
+        SglEvVariant* slot = SGL_(acq_event_slot)();
+        slot->tag          = SGL_CXT_TAG;
+        slot->cxt.type     = SGLPRIM_CXT_INSTR;
+        slot->cxt.id       = ii->instr_addr;
+    }
+}
+
+
+/* Note that addEvent_D_guarded assumes that log_0I1Dr and log_0I1Dw
+   have exactly the same prototype.  If you change them, you must
+   change addEvent_D_guarded too. */
+static inline void log_mem(Int type, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++mem_events;
+#endif
+
+        SglEvVariant* slot   = SGL_(acq_event_slot)();
+        slot->tag            = SGL_MEM_TAG;
+        slot->mem.type       = type;
+        slot->mem.begin_addr = data_addr;
+        slot->mem.size       = data_size;
+    }
+}
+void SGL_(log_0I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    log_mem(SGLPRIM_MEM_LOAD, data_addr, data_size);
+}
+void SGL_(log_0I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    log_mem(SGLPRIM_MEM_STORE, data_addr, data_size);
+}
+
+
+void SGL_(log_comp_event)(InstrInfo* ii, IRType op_type, IRExprTag arity)
+{
+    /* SIMD and decimal floating point are unsupported
+     * See VEX/pub/libvex_ir.h : IROp
+     * for future updates on specific ops */
+    tl_assert(op_type < Ity_D32 || op_type == Ity_F128);
+
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++comp_events;
+#endif
+
+        SglEvVariant* slot = SGL_(acq_event_slot)();
+        slot->tag = SGL_COMP_TAG;
+
+        if (op_type < Ity_F16)
+            slot->comp.type = SGLPRIM_COMP_IOP;
+        else
+            slot->comp.type = SGLPRIM_COMP_FLOP;
+
+        switch (arity)
+        {
+        case Iex_Unop:
+            slot->comp.arity = SGLPRIM_COMP_UNARY;
+            break;
+        case Iex_Binop:
+            slot->comp.arity = SGLPRIM_COMP_BINARY;
+            break;
+        case Iex_Triop:
+            slot->comp.arity = SGLPRIM_COMP_TERNARY;
+            break;
+        case Iex_Qop:
+            slot->comp.arity = SGLPRIM_COMP_QUARTERNARY;
+            break;
+        default:
+            tl_assert(False);
+            break;
+        }
+    }
+}
+
+
+void SGL_(log_sync)(UChar type, UWord data1, UWord data2)
+{
+    if (SGL_(clo).gen_sync == True)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++sync_events;
+#endif
+
+        SglEvVariant* slot  = SGL_(acq_event_slot)();
+        slot->tag           = SGL_SYNC_TAG;
+        slot->sync.type     = type;
+        slot->sync.data[0]  = data1;
+        slot->sync.data[1]  = data2;
+    }
+}
+
+
+static inline void log_fn(Int type, fn_node* fn)
+{
+    if (EVENT_GENERATION_ENABLED && SGL_(clo).gen_fn == True)
+    {
+#ifdef COUNT_EVENT_CHECK
+        cxt_events++;
+#endif
+
+        /* request both slots simultaneously to allow proper flushing */
+        /* TODO set max size for name length? */
+        Int len = VG_(strlen)(fn->name) + 1;
+        EventNameSlotTuple tuple = SGL_(acq_event_name_slot)(len);
+
+        VG_(strncpy)(tuple.name_slot, fn->name, len);
+        tuple.event_slot->tag      = SGL_CXT_TAG;
+        tuple.event_slot->cxt.type = type;
+        tuple.event_slot->cxt.len  = len;
+        tuple.event_slot->cxt.idx  = tuple.name_idx;
+    }
+}
+void SGL_(log_fn_entry)(fn_node* fn)
+{
+    log_fn(SGLPRIM_CXT_FUNC_ENTER, fn);
+}
+void SGL_(log_fn_leave)(fn_node* fn)
+{
+    log_fn(SGLPRIM_CXT_FUNC_EXIT, fn);
+}
+
+
+/***************************
+ * Aggregate event logging
+ ***************************/
+void SGL_(log_2I0D)(InstrInfo* ii1, InstrInfo* ii2)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii1);
+        SGL_(log_1I0D)(ii2);
+    }
+}
+void SGL_(log_3I0D)(InstrInfo* ii1, InstrInfo* ii2, InstrInfo* ii3)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii1);
+        SGL_(log_1I0D)(ii2);
+        SGL_(log_1I0D)(ii3);
+    }
+}
+void SGL_(log_1I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii);
+        SGL_(log_0I1Dr)(ii, data_addr, data_size);
+    }
+}
+void SGL_(log_1I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii);
+        SGL_(log_0I1Dw)(ii, data_addr, data_size);
+    }
+}
+
+/***************************
+ * Unimplemented Logging
+ ***************************/
+void SGL_(log_global_event)(InstrInfo* ii)
+{
+}
+void SGL_(log_cond_branch)(InstrInfo* ii, Word taken)
+{
+}
+void SGL_(log_ind_branch)(InstrInfo* ii, UWord actual_dst)
+{
+}
diff --git a/sigrind/log_events.h b/sigrind/log_events.h
new file mode 100644
index 000000000..413158a0d
--- /dev/null
+++ b/sigrind/log_events.h
@@ -0,0 +1,63 @@
+#ifndef SGL_LOG_EVENTS_H
+#define SGL_LOG_EVENTS_H
+
+#include "global.h"
+
+/********************************************************************
+ * Event Logging in Sigrind
+ *
+ * Sigrind piggy-backs off of Callgrind's event instrumentation
+ * infrastructure. All Callgrind event processing (cost tracking)
+ * has been gutted, and replaced with interprocess communication
+ * handling. This is how dynamic application info is sent to Sigil2.
+ ********************************************************************/
+
+void SGL_(end_logging)(void);
+
+/* 1 Instruction */
+void SGL_(log_1I0D)(InstrInfo* ii);
+
+/* 2 Instructions */
+void SGL_(log_2I0D)(InstrInfo* ii1, InstrInfo* ii2);
+
+/* 3 Instructions */
+void SGL_(log_3I0D)(InstrInfo* ii1, InstrInfo* ii2, InstrInfo* ii3);
+
+/* 1 Instruction, 1 Data Read */
+void SGL_(log_1I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Data Read */
+void SGL_(log_0I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Instruction, 1 Data Write */
+void SGL_(log_1I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Data Write */
+void SGL_(log_0I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Compute event */
+void SGL_(log_comp_event)(InstrInfo* ii, IRType op_type, IRExprTag arity);
+
+/* Function fn entered */
+void SGL_(log_fn_entry)(fn_node* fn);
+
+/* Function fn exited */
+void SGL_(log_fn_leave)(fn_node* fn);
+
+/* Synchronization event or thread context swap
+ * Some sync events have two pieces of data,
+ * e.g. mutex and condition variable in a conditional wait.
+ * Otherwise only the first data argument is used */
+#define UNUSED_SYNC_DATA 0
+void SGL_(log_sync)(UChar type, UWord data1, UWord data2);
+
+/* unimplemented */
+void SGL_(log_global_event)(InstrInfo* ii);
+
+/* unimplemented */
+void SGL_(log_cond_branch)(InstrInfo* ii, Word taken);
+
+/* unimplemented */
+void SGL_(log_ind_branch)(InstrInfo* ii, UWord actual_dst);
+
+#endif
diff --git a/sigrind/sg_main.c b/sigrind/sg_main.c
new file mode 100644
index 000000000..bdf6208e4
--- /dev/null
+++ b/sigrind/sg_main.c
@@ -0,0 +1,1872 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                       main.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+// TODO(cleanup) leftover Callgrind functionality
+
+
+#include "config.h"
+
+#include "global.h"
+#include "callgrind.h"
+
+#include "sigil2_ipc.h"
+#include "log_events.h"
+#include "Core/PrimitiveEnums.h"
+
+#include "coregrind/pub_core_libcfile.h"
+#include "coregrind/pub_core_clientstate.h"
+#include "pub_tool_threadstate.h"
+#include "pub_tool_gdbserver.h"
+#include "pub_tool_transtab.h"       // VG_(discard_translations_safely)
+
+
+/*------------------------------------------------------------*/
+/*--- Instrumentation structures and event queue handling  ---*/
+/*------------------------------------------------------------*/
+
+/* Maintain an ordered list of memory events which are outstanding, in
+   the sense that no IR has yet been generated to do the relevant
+   helper calls.  The BB is scanned top to bottom and memory events
+   are added to the end of the list, merging with the most recent
+   notified event where possible (Dw immediately following Dr and
+   having the same size and EA can be merged).
+
+   This merging is done so that for architectures which have
+   load-op-store instructions (x86, amd64), the insn is treated as if
+   it makes just one memory reference (a modify), rather than two (a
+   read followed by a write at the same address).
+
+   At various points the list will need to be flushed, that is, IR
+   generated from it.  That must happen before any possible exit from
+   the block (the end, or an IRStmt_Exit).  Flushing also takes place
+   when there is no space to add a new event.
+
+   If we require the simulation statistics to be up to date with
+   respect to possible memory exceptions, then the list would have to
+   be flushed before each memory reference.  That would however lose
+   performance by inhibiting event-merging during flushing.
+
+   Flushing the list consists of walking it start to end and emitting
+   instrumentation IR for each event, in the order in which they
+   appear.  It may be possible to emit a single call for two adjacent
+   events in order to reduce the number of helper function calls made.
+   For example, it could well be profitable to handle two adjacent Ir
+   events with a single helper call.  */
+
+typedef IRExpr IRAtom;
+
+typedef enum _EventTag EventTag;
+enum _EventTag {
+      Ev_Ir,  // Instruction read
+      Ev_Dr,  // Data read
+      Ev_Dw,  // Data write
+      Ev_Dm,  // Data modify (read then write)
+      Ev_Bc,  // branch conditional
+      Ev_Bi,  // branch indirect (to unknown destination)
+      Ev_G,   // Global bus event
+      Ev_Comp // Compute event
+};
+
+typedef struct _Event Event;
+struct _Event {
+   EventTag   tag;
+   InstrInfo* inode;
+   union 
+   {
+      struct {
+      } Ir;
+      struct {
+         IRAtom* ea;
+         Int     szB;
+      } Dr;
+      struct {
+         IRAtom* ea;
+         Int     szB;
+      } Dw;
+      struct 
+      {
+         IRAtom* ea;
+         Int     szB;
+      } Dm;
+      struct {
+         IRAtom* taken; /* :: Ity_I1 */
+      } Bc;
+      struct {
+         IRAtom* dst;
+      } Bi;
+      struct {
+      } G;
+      struct {
+         IRExprTag arity;
+         IRType op_type;
+      } Comp;
+   } Ev;
+};
+
+/* Up to this many unnotified events are allowed.  Number is
+   arbitrary.  Larger numbers allow more event merging to occur, but
+   potentially induce more spilling due to extending live ranges of
+   address temporaries. */
+#define N_EVENTS 16
+
+/* A struct which holds all the running state during instrumentation.
+   Mostly to avoid passing loads of parameters everywhere. */
+typedef struct {
+    /* The current outstanding-memory-event list. */
+    Event events[N_EVENTS];
+    Int   events_used;
+
+    /* The array of InstrInfo's is part of BB struct. */
+    BB* bb;
+
+    /* BB seen before (ie. re-instrumentation) */
+    Bool seen_before;
+
+    /* Number InstrInfo bins 'used' so far. */
+    UInt ii_index;
+
+    // current offset of guest instructions from BB start
+    UInt instr_offset;
+
+    /* The output SB being constructed. */
+    IRSB* sbOut;
+} ClgState;
+
+/*------------------------------------------------------------*/
+/*--- Global variables                                     ---*/
+/*------------------------------------------------------------*/
+
+Bool SGL_(is_in_event_collect_func);
+SglCommandLineOptions SGL_(clo);
+
+/* for all threads */
+CommandLineOptions CLG_(clo);
+
+Statistics CLG_(stat);
+Bool CLG_(instrument_state) = True; /* Instrumentation on ? */
+
+/* thread and signal handler specific */
+exec_state CLG_(current_state);
+
+/*------------------------------------------------------------*/
+/*--- Local declarations                                   ---*/
+/*------------------------------------------------------------*/
+
+static void flushEvents ( ClgState* clgs );
+static void addEvent_Comp( ClgState* clgs, InstrInfo* inode, IRExprTag arity, IRType op_type );
+static void addEvent_G ( ClgState* clgs, InstrInfo* inode );
+static void addEvent_Bi ( ClgState* clgs, InstrInfo* inode, IRAtom* whereTo );
+static void addEvent_Bc ( ClgState* clgs, InstrInfo* inode, IRAtom* guard );
+static void addEvent_D_guarded ( ClgState* clgs, InstrInfo* inode,
+                          Int datasize, IRAtom* ea, IRAtom* guard,
+                          Bool isWrite );
+static void addEvent_Dw ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea );
+static void addEvent_Dr ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea );
+static void addEvent_Ir ( ClgState* clgs, InstrInfo* inode );
+static InstrInfo* next_InstrInfo ( ClgState* clgs, Addr instr_addr, UInt instr_size );
+
+/* Inititalization functions */
+static void init_Event ( Event* ev );
+/* Helper functions */
+static IRAtom* get_Event_dea ( Event* ev );
+static Int get_Event_dszB ( Event* ev );
+static void showEvent ( Event* ev );
+static Addr IRConst2Addr(IRConst* con);
+static void addConstMemStoreStmt( IRSB* bbOut, UWord addr, UInt val, IRType hWordTy);
+
+
+
+/*------------------------------------------------------------*/
+/*--- Instrumentation                                      ---*/
+/*------------------------------------------------------------*/
+
+/* add helper call to setup_bbcc, with pointer to BB struct as argument
+ *
+ * precondition for setup_bbcc:
+ * - jmps_passed has number of cond.jumps passed in last executed BB
+ * - current_bbcc has a pointer to the BBCC of the last executed BB
+ *   Thus, if bbcc_jmpkind is != -1 (JmpNone),
+ *     current_bbcc->bb->jmp_addr
+ *   gives the address of the jump source.
+ *
+ * the setup does 1 thing:
+ * - trace call:
+ *   * Unwind own call stack, i.e sync our ESP with real ESP
+ *     This is for ESP manipulation (longjmps, C++ exec handling) and RET
+ *   * For CALLs or JMPs crossing objects, record call arg +
+ *     push are on own call stack
+ */
+static void addBBSetupCall(ClgState* clgs)
+{
+   IRDirty* di;
+   IRExpr  *arg1, **argv;
+
+   arg1 = mkIRExpr_HWord( (HWord)clgs->bb );
+   argv = mkIRExprVec_1(arg1);
+   di = unsafeIRDirty_0_N( 1, "setup_bbcc",
+			      VG_(fnptr_to_fnentry)( & CLG_(setup_bbcc) ),
+			      argv);
+   addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+}
+
+static IRSB* CLG_(instrument)( VgCallbackClosure* closure,
+                        IRSB* sbIn,
+			const VexGuestLayout* layout,
+			const VexGuestExtents* vge,
+                        const VexArchInfo* archinfo_host,
+			IRType gWordTy, IRType hWordTy )
+{
+   Int        i;
+   IRStmt*    st;
+   Addr       origAddr;
+   InstrInfo* curr_inode = NULL;
+   ClgState   clgs;
+   UInt       cJumps = 0;
+   IRTypeEnv* tyenv = sbIn->tyenv;
+
+   if (gWordTy != hWordTy) {
+      /* We don't currently support this case. */
+      VG_(tool_panic)("host/guest word size mismatch");
+   }
+
+   // No instrumentation if it is switched off
+   if (! CLG_(instrument_state)) {
+       CLG_DEBUG(5, "instrument(BB %#lx) [Instrumentation OFF]\n",
+		 (Addr)closure->readdr);
+       return sbIn;
+   }
+
+   CLG_DEBUG(3, "+ instrument(BB %#lx)\n", (Addr)closure->readdr);
+
+   /* Set up SB for instrumented IR */
+   clgs.sbOut = deepCopyIRSBExceptStmts(sbIn);
+
+   // Copy verbatim any IR preamble preceding the first IMark
+   i = 0;
+   while (i < sbIn->stmts_used && sbIn->stmts[i]->tag != Ist_IMark) {
+      addStmtToIRSB( clgs.sbOut, sbIn->stmts[i] );
+      i++;
+   }
+
+   // Get the first statement, and origAddr from it
+   CLG_ASSERT(sbIn->stmts_used >0);
+   CLG_ASSERT(i < sbIn->stmts_used);
+   st = sbIn->stmts[i];
+   CLG_ASSERT(Ist_IMark == st->tag);
+
+   origAddr = st->Ist.IMark.addr + st->Ist.IMark.delta;
+   CLG_ASSERT(origAddr == st->Ist.IMark.addr 
+                          + st->Ist.IMark.delta);  // XXX: check no overflow
+
+   /* Get BB struct (creating if necessary).
+    * JS: The hash table is keyed with orig_addr_noredir -- important!
+    * JW: Why? If it is because of different chasing of the redirection,
+    *     this is not needed, as chasing is switched off in callgrind
+    */
+   clgs.bb = CLG_(get_bb)(origAddr, sbIn, &(clgs.seen_before));
+
+   addBBSetupCall(&clgs);
+
+   // Set up running state
+   clgs.events_used = 0;
+   clgs.ii_index = 0;
+   clgs.instr_offset = 0;
+
+   for (/*use current i*/; i < sbIn->stmts_used; i++) 
+   {
+      st = sbIn->stmts[i];
+      CLG_ASSERT(isFlatIRStmt(st));
+
+      if (st->tag == Ist_IMark)
+      {
+         Addr   cia   = st->Ist.IMark.addr + st->Ist.IMark.delta;
+         UInt   isize = st->Ist.IMark.len;
+         CLG_ASSERT(clgs.instr_offset == cia - origAddr);
+         // If Vex fails to decode an instruction, the size will be zero.
+         // Pretend otherwise.
+         if (isize == 0) isize = VG_MIN_INSTR_SZB;
+         // Sanity-check size.
+         tl_assert( (VG_MIN_INSTR_SZB <= isize && isize <= VG_MAX_INSTR_SZB)
+            || VG_CLREQ_SZB == isize );
+
+         // Init the inode, record it as the current one.
+         // Subsequent Dr/Dw/Dm events from the same instruction will
+         // also use it.
+         curr_inode = next_InstrInfo (&clgs, cia, isize);
+      }
+
+      switch (st->tag) 
+      {
+         case Ist_NoOp:
+         case Ist_AbiHint:
+         case Ist_Put:
+         case Ist_PutI:
+         case Ist_MBE:
+             break;
+         case Ist_IMark: 
+         {
+            if (SGL_(clo).gen_instr == True)
+               addEvent_Ir( &clgs, curr_inode );
+            break;
+         }
+         case Ist_WrTmp: 
+         {
+            IRExpr* data = st->Ist.WrTmp.data;
+            switch (data->tag)
+            {
+               case Iex_Load:
+               {
+                  IRExpr* aexpr = data->Iex.Load.addr;
+                  // Note also, endianness info is ignored.  I guess
+                  // that's not interesting.
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dr( &clgs, curr_inode, sizeofIRType(data->Iex.Load.ty), aexpr );
+                  break;
+               }
+               case Iex_Unop:
+               case Iex_Binop:
+               case Iex_Triop:
+               case Iex_Qop:
+                  if (SGL_(clo).gen_comp == True)
+                  {
+                     IRType op_type = typeOfIRExpr(sbIn->tyenv, data);
+                     /* SIMD and decimal floating point ops are unsupported.
+                      * See VEX/pub/libvex_ir.h */
+                     if (op_type < Ity_D32 || op_type == Ity_F128)
+                        addEvent_Comp( &clgs, curr_inode, data->tag, op_type );
+                  }
+                  break;
+               default:
+                  /*don't care*/
+                  break;
+            }
+            break;
+         }
+         case Ist_Store: 
+         {
+            IRExpr* data  = st->Ist.Store.data;
+            IRExpr* aexpr = st->Ist.Store.addr;
+            if (SGL_(clo).gen_mem == True)
+               addEvent_Dw( &clgs, curr_inode, sizeofIRType(typeOfIRExpr(sbIn->tyenv, data)), aexpr );
+            break;
+         }
+         case Ist_StoreG: 
+         {
+            IRStoreG* sg   = st->Ist.StoreG.details;
+            IRExpr*   data = sg->data;
+            IRExpr*   addr = sg->addr;
+            IRType    type = typeOfIRExpr(tyenv, data);
+            tl_assert(type != Ity_INVALID);
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_D_guarded( &clgs, curr_inode,
+                                   sizeofIRType(type),
+                                   addr, sg->guard, True/*isWrite*/ );
+            }
+            break;
+         }
+         case Ist_LoadG: 
+         {
+            IRLoadG* lg       = st->Ist.LoadG.details;
+            IRType   type     = Ity_INVALID; /* loaded type */
+            IRType   typeWide = Ity_INVALID; /* after implicit widening */
+            IRExpr*  addr     = lg->addr;
+            typeOfIRLoadGOp(lg->cvt, &typeWide, &type);
+            tl_assert(type != Ity_INVALID);
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_D_guarded( &clgs, curr_inode,
+                                   sizeofIRType(type), addr, lg->guard,
+                                   False/*!isWrite*/ );
+            }
+            break;
+         }
+         case Ist_Dirty: 
+         {
+            Int      dataSize;
+            IRDirty* d = st->Ist.Dirty.details;
+            if (d->mFx != Ifx_None) 
+            {
+               /* This dirty helper accesses memory.  Collect the details. */
+               tl_assert(d->mAddr != NULL);
+               tl_assert(d->mSize != 0);
+               dataSize = d->mSize;
+
+               if (d->mFx == Ifx_Read || d->mFx == Ifx_Modify)
+               {
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dr( &clgs, curr_inode, dataSize, d->mAddr );
+               }
+               if (d->mFx == Ifx_Write || d->mFx == Ifx_Modify)
+               {
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dw( &clgs, curr_inode, dataSize, d->mAddr );
+               }
+            }
+            else 
+            {
+               tl_assert(d->mAddr == NULL);
+               tl_assert(d->mSize == 0);
+            }
+            break;
+         }
+         case Ist_CAS: 
+         {
+            /* We treat it as a read and a write of the location.  I
+               think that is the same behaviour as it was before IRCAS
+               was introduced, since prior to that point, the Vex
+               front ends would translate a lock-prefixed instruction
+               into a (normal) read followed by a (normal) write. */
+            Int    dataSize;
+            IRCAS* cas = st->Ist.CAS.details;
+            CLG_ASSERT(cas->addr && isIRAtom(cas->addr));
+            CLG_ASSERT(cas->dataLo);
+            dataSize = sizeofIRType(typeOfIRExpr(sbIn->tyenv, cas->dataLo));
+            if (cas->dataHi != NULL)
+               dataSize *= 2; /* since this is a doubleword-cas */
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_Dr( &clgs, curr_inode, dataSize, cas->addr );
+               addEvent_Dw( &clgs, curr_inode, dataSize, cas->addr );
+            }
+            addEvent_G(  &clgs, curr_inode );
+            break;
+         }
+         case Ist_LLSC:
+            if (st->Ist.LLSC.storedata == NULL) {
+               /* LL */
+               IRType dataTy = typeOfIRTemp(sbIn->tyenv, st->Ist.LLSC.result);
+               if (SGL_(clo).gen_mem == True)
+                  addEvent_Dr( &clgs, curr_inode, sizeofIRType(dataTy), st->Ist.LLSC.addr );
+               /* flush events before LL, should help SC to succeed */
+               flushEvents( &clgs );
+            } else {
+               /* SC */
+               IRType dataTy = typeOfIRExpr(sbIn->tyenv, st->Ist.LLSC.storedata);
+               if (SGL_(clo).gen_mem == True)
+                  addEvent_Dw( &clgs, curr_inode, sizeofIRType(dataTy), st->Ist.LLSC.addr );
+               /* I don't know whether the global-bus-lock cost should
+                  be attributed to the LL or the SC, but it doesn't
+                  really matter since they always have to be used in
+                  pairs anyway.  Hence put it (quite arbitrarily) on
+                  the SC. */
+               addEvent_G(  &clgs, curr_inode );
+            }
+            break;
+         case Ist_Exit: {
+            Bool guest_exit, inverted;
+
+            /* VEX code generation sometimes inverts conditional branches.
+             * As Callgrind counts (conditional) jumps, it has to correct
+             * inversions. The heuristic is the following:
+             * (1) Callgrind switches off SB chasing and unrolling, and
+             *     therefore it assumes that a candidate for inversion only is
+             *     the last conditional branch in an SB.
+             * (2) inversion is assumed if the branch jumps to the address of
+             *     the next guest instruction in memory.
+             * This heuristic is precalculated in CLG_(collectBlockInfo)().
+             *
+             * Branching behavior is also used for branch prediction. Note that
+             * above heuristic is different from what Cachegrind does.
+             * Cachegrind uses (2) for all branches.
+             */
+            if (cJumps+1 == clgs.bb->cjmp_count)
+                inverted = clgs.bb->cjmp_inverted;
+            else
+                inverted = False;
+
+            // call branch predictor only if this is a branch in guest code
+            guest_exit = (st->Ist.Exit.jk == Ijk_Boring) ||
+                         (st->Ist.Exit.jk == Ijk_Call) ||
+                         (st->Ist.Exit.jk == Ijk_Ret);
+
+            if (guest_exit) {
+                /* Stuff to widen the guard expression to a host word, so
+                   we can pass it to the branch predictor simulation
+                   functions easily. */
+                IRType   tyW    = hWordTy;
+                IROp     widen  = tyW==Ity_I32  ? Iop_1Uto32  : Iop_1Uto64;
+                IROp     opXOR  = tyW==Ity_I32  ? Iop_Xor32   : Iop_Xor64;
+                IRTemp   guard1 = newIRTemp(clgs.sbOut->tyenv, Ity_I1);
+                IRTemp   guardW = newIRTemp(clgs.sbOut->tyenv, tyW);
+                IRTemp   guard  = newIRTemp(clgs.sbOut->tyenv, tyW);
+                IRExpr*  one    = tyW==Ity_I32 ? IRExpr_Const(IRConst_U32(1))
+                                               : IRExpr_Const(IRConst_U64(1));
+
+                /* Widen the guard expression. */
+                addStmtToIRSB( clgs.sbOut,
+                               IRStmt_WrTmp( guard1, st->Ist.Exit.guard ));
+                addStmtToIRSB( clgs.sbOut,
+                               IRStmt_WrTmp( guardW,
+                                             IRExpr_Unop(widen,
+                                                         IRExpr_RdTmp(guard1))) );
+                /* If the exit is inverted, invert the sense of the guard. */
+                addStmtToIRSB(
+                        clgs.sbOut,
+                        IRStmt_WrTmp(
+                                guard,
+                                inverted ? IRExpr_Binop(opXOR, IRExpr_RdTmp(guardW), one)
+                                    : IRExpr_RdTmp(guardW)
+                                    ));
+                /* And post the event. */
+                addEvent_Bc( &clgs, curr_inode, IRExpr_RdTmp(guard) );
+            }
+
+            /* We may never reach the next statement, so need to flush
+               all outstanding transactions now. */
+            flushEvents( &clgs );
+
+            CLG_ASSERT(clgs.ii_index>0);
+            if (!clgs.seen_before) {
+               ClgJumpKind jk;
+
+               if (st->Ist.Exit.jk == Ijk_Call) jk = jk_Call;
+               else if (st->Ist.Exit.jk == Ijk_Ret) jk = jk_Return;
+               else {
+                 if (IRConst2Addr(st->Ist.Exit.dst) == origAddr + curr_inode->instr_offset + curr_inode->instr_size)
+                    jk = jk_None;
+                 else
+                    jk = jk_Jump;
+               }
+
+               clgs.bb->jmp[cJumps].instr = clgs.ii_index-1;
+               clgs.bb->jmp[cJumps].jmpkind = jk;
+            }
+
+            /* Update global variable jmps_passed before the jump
+             * A correction is needed if VEX inverted the last jump condition
+             */
+            UInt val = inverted ? cJumps+1 : cJumps;
+            addConstMemStoreStmt( clgs.sbOut,
+                (UWord) &CLG_(current_state).jmps_passed,
+                val, hWordTy);
+
+            cJumps++;
+
+            break;
+         }
+         default:
+            tl_assert(0);
+            break;
+      } //end switch
+
+
+      /* Copy the original statement */
+      addStmtToIRSB( clgs.sbOut, st );
+
+      CLG_DEBUGIF(5) 
+      {
+         VG_(printf)("   pass  ");
+	      ppIRStmt(st);
+	      VG_(printf)("\n");
+      }
+   } //end foreach statement
+
+   /* Deal with branches to unknown destinations.  Except ignore ones
+      which are function returns as we assume the return stack
+      predictor never mispredicts. */
+   if ((sbIn->jumpkind == Ijk_Boring) || (sbIn->jumpkind == Ijk_Call)) {
+      if (0) { ppIRExpr( sbIn->next ); VG_(printf)("\n"); }
+      switch (sbIn->next->tag) {
+         case Iex_Const:
+            break; /* boring - branch to known address */
+         case Iex_RdTmp:
+            /* looks like an indirect branch (branch to unknown) */
+            addEvent_Bi( &clgs, curr_inode, sbIn->next );
+            break;
+         default:
+            /* shouldn't happen - if the incoming IR is properly
+               flattened, should only have tmp and const cases to
+               consider. */
+            tl_assert(0);
+      }
+   }
+
+   /* At the end of the bb.  Flush outstandings. */
+   flushEvents( &clgs );
+
+   /* Update global variable jmps_passed at end of SB.
+    * As CLG_(current_state).jmps_passed is reset to 0 in setup_bbcc,
+    * this can be omitted if there is no conditional jump in this SB.
+    * A correction is needed if VEX inverted the last jump condition
+    */
+   if (cJumps>0) {
+      UInt jmps_passed = cJumps;
+      if (clgs.bb->cjmp_inverted) jmps_passed--;
+      addConstMemStoreStmt( clgs.sbOut,
+			    (UWord) &CLG_(current_state).jmps_passed,
+			    jmps_passed, hWordTy);
+   }
+   CLG_ASSERT(clgs.bb->cjmp_count == cJumps);
+   CLG_ASSERT(clgs.bb->instr_count == clgs.ii_index);
+
+   /* Info for final exit from BB */
+   {
+     ClgJumpKind jk;
+
+     if      (sbIn->jumpkind == Ijk_Call) jk = jk_Call;
+     else if (sbIn->jumpkind == Ijk_Ret)  jk = jk_Return;
+     else {
+       jk = jk_Jump;
+       if ((sbIn->next->tag == Iex_Const) &&
+	   (IRConst2Addr(sbIn->next->Iex.Const.con) ==
+	    origAddr + clgs.instr_offset))
+	 jk = jk_None;
+     }
+     clgs.bb->jmp[cJumps].jmpkind = jk;
+     /* Instruction index of the call/ret at BB end
+      * (it is wrong for fall-through, but does not matter) */
+     clgs.bb->jmp[cJumps].instr = clgs.ii_index-1;
+   }
+
+   /* swap information of last exit with final exit if inverted */
+   if (clgs.bb->cjmp_inverted) {
+     ClgJumpKind jk;
+     UInt instr;
+
+     jk = clgs.bb->jmp[cJumps].jmpkind;
+     clgs.bb->jmp[cJumps].jmpkind = clgs.bb->jmp[cJumps-1].jmpkind;
+     clgs.bb->jmp[cJumps-1].jmpkind = jk;
+     instr = clgs.bb->jmp[cJumps].instr;
+     clgs.bb->jmp[cJumps].instr = clgs.bb->jmp[cJumps-1].instr;
+     clgs.bb->jmp[cJumps-1].instr = instr;
+   }
+
+   if (clgs.seen_before) {
+       CLG_ASSERT(clgs.bb->instr_len == clgs.instr_offset);
+   }
+   else {
+       clgs.bb->instr_len = clgs.instr_offset;
+   }
+
+   if (cJumps>0) {
+       CLG_DEBUG(3, "                     [ ");
+       for (i=0;(UInt)i<cJumps;i++)
+	   CLG_DEBUG(3, "%u ", clgs.bb->jmp[i].instr);
+       CLG_DEBUG(3, "], last inverted: %s \n",
+		 clgs.bb->cjmp_inverted ? "yes":"no");
+   }
+
+  return clgs.sbOut;
+}
+
+
+
+
+
+/* Generate code for all outstanding memory events, and mark the queue
+   empty.  Code is generated into cgs->sbOut, and this activity
+   'consumes' slots in cgs->bb. */
+static void flushEvents ( ClgState* clgs )
+{
+	Int        i, regparms, inew;
+	const HChar* helperName;
+	void*      helperAddr;
+	IRExpr**   argv;
+	IRExpr*    i_node_expr;
+	IRDirty*   di;
+	Event*     ev;
+	Event*     ev2;
+	Event*     ev3;
+
+	for(i = 0; i < clgs->events_used; i = inew) 
+	{
+		helperName = NULL;
+		helperAddr = NULL;
+		argv       = NULL;
+		regparms   = 0;
+
+		/* generate IR to notify event i and possibly the ones
+		immediately following it. */
+		tl_assert(i >= 0 && i < clgs->events_used);
+
+		ev  = &clgs->events[i];
+		ev2 = ( i < clgs->events_used-1 ? &clgs->events[i+1] : NULL );
+		ev3 = ( i < clgs->events_used-2 ? &clgs->events[i+2] : NULL );
+
+		CLG_DEBUGIF(5) 
+		{
+			VG_(printf)("  flush ");
+			showEvent( ev );
+		}
+
+		i_node_expr = mkIRExpr_HWord( (HWord)ev->inode );
+
+		/* Decide on helper fn to call and args to pass it, and advance
+		 * i appropriately.
+		 * Dm events have same effect as Dw events */
+		switch (ev->tag) 
+		{
+		case Ev_Ir:
+			/* Merge an Ir with a following Dr. */
+			if (ev2 && ev2->tag == Ev_Dr) 
+			{
+				tl_assert(ev2->inode == ev->inode);	/* Why is this true?  It's because we're merging an Ir
+													with a following Dr.  The Ir derives from the
+													instruction's IMark and the Dr from data
+													references which follow it.  In short it holds
+													because each insn starts with an IMark, hence an
+													Ev_Ir, and so these Dr must pertain to the
+													immediately preceding Ir.  Same applies to analogous
+													assertions in the subsequent cases. */
+				helperName = "log_1I1Dr";
+				helperAddr = SGL_(log_1I1Dr);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						get_Event_dea(ev2),
+						mkIRExpr_HWord(get_Event_dszB(ev2)) 
+						);
+				regparms = 3;
+				inew = i+2;
+			}
+			/* Merge an Ir with a following Dw/Dm. */
+			else if (ev2 && (ev2->tag == Ev_Dw || ev2->tag == Ev_Dm)) 
+			{
+				tl_assert(ev2->inode == ev->inode);
+				helperName = "log_1I1Dw";
+				helperAddr = SGL_(log_1I1Dw);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						get_Event_dea(ev2),
+						mkIRExpr_HWord(get_Event_dszB(ev2)) 
+						);
+				regparms = 3;
+				inew = i+2;
+			}
+			/* Merge an Ir with two following Irs. */
+			else if (ev2 && ev3 && ev2->tag == Ev_Ir && ev3->tag == Ev_Ir) 
+			{
+				helperName = "log_3I0D";
+				helperAddr = SGL_(log_3I0D);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						mkIRExpr_HWord((HWord)ev2->inode),
+						mkIRExpr_HWord((HWord)ev3->inode) 
+						);
+				regparms = 3;
+				inew = i+3;
+			}
+			/* Merge an Ir with one following Ir. */
+			else
+			if (ev2 && ev2->tag == Ev_Ir) {
+				helperName = "log_2I0D";
+				helperAddr = SGL_(log_2I0D);
+				argv = mkIRExprVec_2(
+						i_node_expr,
+						mkIRExpr_HWord((HWord)ev2->inode) 
+						);
+				regparms = 2;
+				inew = i+2;
+			}
+			/* No merging possible; emit as-is. */
+			else {
+				helperName = "log_1I0D";
+				helperAddr = SGL_(log_1I0D);
+				argv = mkIRExprVec_1( i_node_expr );
+				regparms = 1;
+				inew = i+1;
+			}
+			break;
+		case Ev_Dr:
+			/* Data read or modify */
+			helperName = "log_0I1Dr";
+			helperAddr = SGL_(log_0I1Dr);
+			argv = mkIRExprVec_3( 
+					i_node_expr,
+					get_Event_dea(ev),
+					mkIRExpr_HWord(get_Event_dszB(ev)) 
+					);
+			regparms = 3;
+			inew = i+1;
+			break;
+		case Ev_Dw:
+		case Ev_Dm:
+		   /* Data write */
+		   helperName = "log_0I1Dw";
+		   helperAddr = SGL_(log_0I1Dw);
+		   argv = mkIRExprVec_3( 
+				   i_node_expr,
+		   		   get_Event_dea(ev),
+		   		   mkIRExpr_HWord( get_Event_dszB(ev)) 
+				   );
+		   regparms = 3;
+		   inew = i+1;
+		   break;
+		case Ev_Bc:
+			/* Conditional branch */
+			helperName = "log_cond_branch";
+			helperAddr = SGL_(log_cond_branch);
+			argv = mkIRExprVec_2( i_node_expr, ev->Ev.Bc.taken );
+			regparms = 2;
+			inew = i+1;
+			break;
+		case Ev_Bi:
+			/* Branch to an unknown destination */
+			helperName = "log_ind_branch";
+			helperAddr = SGL_(log_ind_branch);
+			argv = mkIRExprVec_2( i_node_expr, ev->Ev.Bi.dst );
+			regparms = 2;
+			inew = i+1;
+			break;
+		case Ev_G:
+			/* Global bus event (CAS, LOCK-prefix, LL-SC, etc) */
+			helperName = "log_global_event";
+			helperAddr = SGL_(log_global_event);
+			argv = mkIRExprVec_1( i_node_expr );
+			regparms = 1;
+			inew = i+1;
+			break;
+		case Ev_Comp:
+		   /* Compute */
+		   helperName = "log_comp_event";
+		   helperAddr = SGL_(log_comp_event);
+		   argv = mkIRExprVec_3( 
+				   i_node_expr,
+		   		   mkIRExpr_HWord(ev->Ev.Comp.op_type),
+		   		   mkIRExpr_HWord(ev->Ev.Comp.arity)
+				   );
+		   regparms = 3;
+		   inew = i+1;
+		   break;
+		default:
+			tl_assert(0);
+		}
+
+		CLG_DEBUGIF(5) {
+		if (inew > i+1) {
+			VG_(printf)("   merge ");
+			showEvent( ev2 );
+		}
+		if (inew > i+2) {
+			VG_(printf)("   merge ");
+			showEvent( ev3 );
+		}
+		if (helperAddr)
+			VG_(printf)("   call  %s (%p)\n",
+			helperName, helperAddr);
+		}
+		
+		/* helper could be unset depending on the simulator used */
+		if (helperAddr == 0) continue;
+		
+		/* Add the helper. */
+		tl_assert(helperName);
+		tl_assert(helperAddr);
+		tl_assert(argv);
+		di = unsafeIRDirty_0_N( regparms,
+		  	      helperName, VG_(fnptr_to_fnentry)( helperAddr ),
+		  	      argv );
+		addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+	}
+
+	clgs->events_used = 0;
+}
+
+static void addEvent_Ir ( ClgState* clgs, InstrInfo* inode )
+{
+   Event* evt;
+   tl_assert(clgs->seen_before || (inode->eventset == 0));
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag      = Ev_Ir;
+   evt->inode    = inode;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Dr ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea )
+{
+   Event* evt;
+   tl_assert(isIRAtom(ea));
+   tl_assert(datasize >= 1);
+   
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Dr;
+   evt->inode     = inode;
+   evt->Ev.Dr.szB = datasize;
+   evt->Ev.Dr.ea  = ea;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Dw ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea )
+{
+   Event* lastEvt;
+   Event* evt;
+   tl_assert(isIRAtom(ea));
+   tl_assert(datasize >= 1);
+  
+   /* Is it possible to merge this write with the preceding read? */
+   lastEvt = &clgs->events[clgs->events_used-1];
+   if (clgs->events_used > 0
+       && lastEvt->tag       == Ev_Dr
+       && lastEvt->Ev.Dr.szB == datasize
+       && lastEvt->inode     == inode
+       && eqIRAtom(lastEvt->Ev.Dr.ea, ea))
+   {
+      lastEvt->tag   = Ev_Dm;
+      return;
+   }
+
+   /* No.  Add as normal. */
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Dw;
+   evt->inode     = inode;
+   evt->Ev.Dw.szB = datasize;
+   evt->Ev.Dw.ea  = ea;
+   clgs->events_used++;
+}
+
+static
+void addEvent_D_guarded ( ClgState* clgs, InstrInfo* inode,
+                          Int datasize, IRAtom* ea, IRAtom* guard,
+                          Bool isWrite )
+{
+   tl_assert(isIRAtom(ea));
+   tl_assert(guard);
+   tl_assert(isIRAtom(guard));
+   tl_assert(datasize >= 1);
+ 
+   /* Adding guarded memory actions and merging them with the existing
+      queue is too complex.  Simply flush the queue and add this
+      action immediately.  Since guarded loads and stores are pretty
+      rare, this is not thought likely to cause any noticeable
+      performance loss as a result of the loss of event-merging
+      opportunities. */
+   tl_assert(clgs->events_used >= 0);
+   flushEvents(clgs);
+   tl_assert(clgs->events_used == 0);
+   /* Same as case Ev_Dw / case Ev_Dr in flushEvents, except with guard */
+   IRExpr*      i_node_expr;
+   const HChar* helperName;
+   void*        helperAddr;
+   IRExpr**     argv;
+   Int          regparms;
+   IRDirty*     di;
+   i_node_expr = mkIRExpr_HWord( (HWord)inode );
+   helperName  = isWrite ? "log_0I1Dw"
+                         : "log_0I1Dr";
+   helperAddr  = isWrite ? SGL_(log_0I1Dw)
+                         : SGL_(log_0I1Dr);
+   argv        = mkIRExprVec_3( i_node_expr,
+                                ea, mkIRExpr_HWord( datasize ) );
+   regparms    = 3;
+   di          = unsafeIRDirty_0_N(
+                    regparms, 
+                    helperName, VG_(fnptr_to_fnentry)( helperAddr ), 
+                    argv );
+   di->guard = guard;
+   addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+}
+
+static
+void addEvent_Bc ( ClgState* clgs, InstrInfo* inode, IRAtom* guard )
+{
+   Event* evt;
+   tl_assert(isIRAtom(guard));
+   tl_assert(typeOfIRExpr(clgs->sbOut->tyenv, guard)
+             == (sizeof(HWord)==4 ? Ity_I32 : Ity_I64));
+   if (!CLG_(clo).simulate_branch) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag         = Ev_Bc;
+   evt->inode       = inode;
+   evt->Ev.Bc.taken = guard;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Bi ( ClgState* clgs, InstrInfo* inode, IRAtom* whereTo )
+{
+   Event* evt;
+   tl_assert(isIRAtom(whereTo));
+   tl_assert(typeOfIRExpr(clgs->sbOut->tyenv, whereTo)
+             == (sizeof(HWord)==4 ? Ity_I32 : Ity_I64));
+   if (!CLG_(clo).simulate_branch) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Bi;
+   evt->inode     = inode;
+   evt->Ev.Bi.dst = whereTo;
+   clgs->events_used++;
+}
+
+static
+void addEvent_G ( ClgState* clgs, InstrInfo* inode )
+{
+   Event* evt;
+   if (!CLG_(clo).collect_bus) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_G;
+   evt->inode     = inode;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Comp( ClgState* clgs, InstrInfo* inode, IRExprTag arity, IRType op_type )
+{
+   Event* evt;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+
+   evt->tag       = Ev_Comp;
+   evt->inode     = inode;
+   evt->Ev.Comp.arity = arity;
+   evt->Ev.Comp.op_type = op_type;
+   clgs->events_used++;
+}
+
+/* Initialise or check (if already seen before) an InstrInfo for next insn.
+   We only can set instr_offset/instr_size here. The required event set and
+   resulting cost offset depend on events (Ir/Dr/Dw/Dm) in guest
+   instructions. The event set is extended as required on flush of the event
+   queue (when Dm events were determined), cost offsets are determined at
+   end of BB instrumentation. */
+static
+InstrInfo* next_InstrInfo ( ClgState* clgs, Addr instr_addr, UInt instr_size )
+{
+   InstrInfo* ii;
+   tl_assert(clgs->ii_index >= 0);
+   tl_assert(clgs->ii_index < clgs->bb->instr_count);
+   ii = &clgs->bb->instr[ clgs->ii_index ];
+
+   if (clgs->seen_before) {
+       CLG_ASSERT(ii->instr_offset == clgs->instr_offset);
+       CLG_ASSERT(ii->instr_size == instr_size);
+   }
+   else {
+       ii->instr_offset = clgs->instr_offset;
+       ii->instr_size = instr_size;
+	   ii->instr_addr = instr_addr;
+       ii->cost_offset = 0;
+       ii->eventset = 0;
+   }
+
+   clgs->ii_index++;
+   clgs->instr_offset += instr_size;
+   CLG_(stat).distinct_instrs++;
+
+   return ii;
+}
+
+/*--------------------------------------------------------------------*/
+/*--- Discarding BB info                                           ---*/
+/*--------------------------------------------------------------------*/
+
+// Called when a translation is removed from the translation cache for
+// any reason at all: to free up space, because the guest code was
+// unmapped or modified, or for any arbitrary reason.
+static
+void clg_discard_superblock_info ( Addr orig_addr, VexGuestExtents vge )
+{
+    tl_assert(vge.n_used > 0);
+
+   if (0)
+      VG_(printf)( "discard_superblock_info: %p, %p, %llu\n",
+                   (void*)orig_addr,
+                   (void*)vge.base[0], (ULong)vge.len[0]);
+
+   // Get BB info, remove from table, free BB info.  Simple!
+   // When created, the BB is keyed by the first instruction address,
+   // (not orig_addr, but eventually redirected address). Thus, we
+   // use the first instruction address in vge.
+   CLG_(delete_bb)(vge.base[0]);
+}
+
+
+/*------------------------------------------------------------*/
+/*--- CLG_(fini)() and related function                     ---*/
+/*------------------------------------------------------------*/
+
+
+
+static void zero_thread_cost(thread_info* t)
+{
+  Int i;
+
+  for(i = 0; i < CLG_(current_call_stack).sp; i++) {
+    if (!CLG_(current_call_stack).entry[i].jcc) continue;
+
+    /* reset call counters to current for active calls */
+    CLG_(current_call_stack).entry[i].jcc->call_counter = 0;
+  }
+
+  CLG_(forall_bbccs)(CLG_(zero_bbcc));
+
+}
+
+void CLG_(zero_all_cost)(Bool only_current_thread)
+{
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "  Zeroing costs...\n");
+
+  if (only_current_thread)
+    zero_thread_cost(CLG_(get_current_thread)());
+  else
+    CLG_(forall_threads)(zero_thread_cost);
+
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "  ...done\n");
+}
+
+static
+void unwind_thread(thread_info* t)
+{
+  /* unwind signal handlers */
+  while(CLG_(current_state).sig !=0)
+    CLG_(post_signal)(CLG_(current_tid),CLG_(current_state).sig);
+
+  /* unwind regular call stack */
+  while(CLG_(current_call_stack).sp>0)
+    CLG_(pop_call_stack)();
+
+  /* reset context and function stack for context generation */
+  CLG_(init_exec_state)( &CLG_(current_state) );
+  CLG_(current_fn_stack).top = CLG_(current_fn_stack).bottom;
+}
+
+void CLG_(set_instrument_state)(const HChar* reason, Bool state)
+{
+  if (CLG_(instrument_state) == state) {
+    CLG_DEBUG(2, "%s: instrumentation already %s\n",
+	     reason, state ? "ON" : "OFF");
+    return;
+  }
+  CLG_(instrument_state) = state;
+  CLG_DEBUG(2, "%s: Switching instrumentation %s ...\n",
+	   reason, state ? "ON" : "OFF");
+
+  VG_(discard_translations_safely)( (Addr)0x1000, ~(SizeT)0xfff, "callgrind");
+
+  /* reset internal state: call stacks, simulator */
+  CLG_(forall_threads)(unwind_thread);
+
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "%s: instrumentation switched %s\n",
+		 reason, state ? "ON" : "OFF");
+}
+
+
+/* Dump current state */
+static void dump_state_togdb(void)
+{
+    thread_info** th;
+    int t;
+    Int orig_tid = CLG_(current_tid);
+
+    VG_(gdb_printf)("instrumentation: %s\n",
+		    CLG_(instrument_state) ? "on":"off");
+    if (!CLG_(instrument_state)) return;
+
+    VG_(gdb_printf)("executed-bbs: %llu\n", CLG_(stat).bb_executions);
+    VG_(gdb_printf)("executed-calls: %llu\n", CLG_(stat).call_counter);
+    VG_(gdb_printf)("distinct-bbs: %d\n", CLG_(stat).distinct_bbs);
+    VG_(gdb_printf)("distinct-calls: %d\n", CLG_(stat).distinct_jccs);
+    VG_(gdb_printf)("distinct-functions: %d\n", CLG_(stat).distinct_fns);
+    VG_(gdb_printf)("distinct-contexts: %d\n", CLG_(stat).distinct_contexts);
+
+    /* threads */
+    th = CLG_(get_threads)();
+    VG_(gdb_printf)("threads:");
+    for(t=1;t<VG_N_THREADS;t++) {
+	if (!th[t]) continue;
+	VG_(gdb_printf)(" %d", t);
+    }
+    VG_(gdb_printf)("\n");
+    VG_(gdb_printf)("current-tid: %d\n", orig_tid);
+}
+
+  
+static void print_monitor_help ( void )
+{
+   VG_(gdb_printf) ("\n");
+   VG_(gdb_printf) ("callgrind monitor commands:\n");
+   VG_(gdb_printf) ("  dump [<dump_hint>]\n");
+   VG_(gdb_printf) ("        dump counters\n");
+   VG_(gdb_printf) ("  zero\n");
+   VG_(gdb_printf) ("        zero counters\n");
+   VG_(gdb_printf) ("  status\n");
+   VG_(gdb_printf) ("        print status\n");
+   VG_(gdb_printf) ("  instrumentation [on|off]\n");
+   VG_(gdb_printf) ("        get/set (if on/off given) instrumentation state\n");
+   VG_(gdb_printf) ("\n");
+}
+
+/* return True if request recognised, False otherwise */
+static Bool handle_gdb_monitor_command (ThreadId tid, const HChar *req)
+{
+   HChar* wcmd;
+   HChar s[VG_(strlen(req)) + 1]; /* copy for strtok_r */
+   HChar *ssaveptr;
+
+   VG_(strcpy) (s, req);
+
+   wcmd = VG_(strtok_r) (s, " ", &ssaveptr);
+   switch (VG_(keyword_id) ("help dump zero status instrumentation", 
+                            wcmd, kwd_report_duplicated_matches)) {
+   case -2: /* multiple matches */
+      return True;
+   case -1: /* not found */
+      return False;
+   case  0: /* help */
+      print_monitor_help();
+      return True;
+   case  1: { /* dump */
+      return True;
+   }
+   case  2: { /* zero */
+      CLG_(zero_all_cost)(False);
+      return True;
+   }
+
+   case 3: { /* status */
+     HChar* arg = VG_(strtok_r) (0, " ", &ssaveptr);
+     if (arg && (VG_(strcmp)(arg, "internal") == 0)) {
+       /* internal interface to callgrind_control */
+       dump_state_togdb();
+       return True;
+     }
+
+     if (!CLG_(instrument_state)) {
+       VG_(gdb_printf)("No status available as instrumentation is switched off\n");
+     } else {
+       // Status information to be improved ...
+       thread_info** th = CLG_(get_threads)();
+       Int t, tcount = 0;
+       for(t=1;t<VG_N_THREADS;t++)
+	 if (th[t]) tcount++;
+       VG_(gdb_printf)("%d thread(s) running.\n", tcount);
+     }
+     return True;
+   }
+
+   case 4: { /* instrumentation */
+     HChar* arg = VG_(strtok_r) (0, " ", &ssaveptr);
+     if (!arg) {
+       VG_(gdb_printf)("instrumentation: %s\n",
+		       CLG_(instrument_state) ? "on":"off");
+     }
+     else
+       CLG_(set_instrument_state)("Command", VG_(strcmp)(arg,"off")!=0);
+     return True;
+   }
+
+   default: 
+      tl_assert(0);
+      return False;
+   }
+}
+
+static
+Bool CLG_(handle_client_request)(ThreadId tid, UWord *args, UWord *ret)
+{
+   if (!VG_IS_TOOL_USERREQ('C','T',args[0])
+       && VG_USERREQ__GDB_MONITOR_COMMAND   != args[0])
+      return False;
+
+   switch(args[0]) 
+   {
+   case VG_USERREQ__DUMP_STATS:     
+      *ret = 0;                 /* meaningless */
+      break;
+
+   case VG_USERREQ__DUMP_STATS_AT:
+     {
+       const HChar *arg = (HChar*)args[1];
+       HChar buf[30 + VG_(strlen)(arg)];    // large enough
+       VG_(sprintf)(buf,"Client Request: %s", arg);
+       *ret = 0;                 /* meaningless */
+     }
+     break;
+
+   case VG_USERREQ__ZERO_STATS:
+     CLG_(zero_all_cost)(True);
+      *ret = 0;                 /* meaningless */
+      break;
+
+   case VG_USERREQ__TOGGLE_COLLECT:
+     CLG_(current_state).collect = !CLG_(current_state).collect;
+     CLG_DEBUG(2, "Client Request: toggled collection state to %s\n",
+	      CLG_(current_state).collect ? "ON" : "OFF");
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__START_INSTRUMENTATION:
+     CLG_(set_instrument_state)("Client Request", True);
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__STOP_INSTRUMENTATION:
+     CLG_(set_instrument_state)("Client Request", False);
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__GDB_MONITOR_COMMAND: {
+      Bool handled = handle_gdb_monitor_command (tid, (HChar*)args[1]);
+      if (handled)
+         *ret = 1;
+      else
+         *ret = 0;
+      return handled;
+   }
+
+   /*******************************************
+    * Synchronixation API intercepts 
+	*
+	* FIXME default OpenMP behavior modeled after
+	* KS need for SynchroTrace, reassess if this
+	* is appropriate */
+										 
+   case VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE:
+      /* enable and log once the thread has been CREATED and waiting */
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CREATE, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER:
+      /* log when the thread join is ENTERED and disable */
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_JOIN, args[1], UNUSED_SYNC_DATA);
+      }
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_LOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE:
+      /* enable and log once the lock has been acquired */
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_LOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_UNLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER:
+      /* log once the barrier is ENTERED and waiting and disable */
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_BARRIER, args[1], UNUSED_SYNC_DATA);
+      }
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDWAIT, args[1], args[2]);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDSIG, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDBROAD, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_SPINLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_SPINUNLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   default:
+      return False;
+   }
+
+
+   return True;
+}
+
+
+/* Syscall Timing */
+
+/* struct timeval syscalltime[VG_N_THREADS]; */
+#if CLG_MICROSYSTIME
+ULong *syscalltime;
+#else
+UInt *syscalltime;
+#endif
+
+
+static
+void clg_print_stats(void)
+{
+}
+
+static void finish(void)
+{
+  CLG_DEBUG(0, "finish()\n");
+
+  /* pop all remaining items from CallStack for correct sum
+   */
+  CLG_(forall_threads)(unwind_thread);
+
+  /* finish IPC with Sigil2 */
+  SGL_(term_IPC)();
+  SGL_(end_logging)();
+}
+
+
+void CLG_(fini)(Int exitcode)
+{
+  finish();
+}
+
+
+/*--------------------------------------------------------------------*/
+/*--- Setup                                                        ---*/
+/*--------------------------------------------------------------------*/
+
+static void clg_start_client_code_callback ( ThreadId tid, ULong blocks_done )
+{
+   static ULong last_blocks_done = 0;
+
+   if (0)
+      VG_(printf)("%d R %llu\n", (Int)tid, blocks_done);
+
+   /* throttle calls to CLG_(run_thread) by number of BBs executed */
+   if (blocks_done - last_blocks_done < 5000) return;
+   last_blocks_done = blocks_done;
+
+   CLG_(run_thread)( tid );
+}
+
+static
+void CLG_(post_clo_init)(void)
+{
+   SGL_(init_IPC)(); // initialize interface to Sigil
+   SGL_(is_in_event_collect_func) = False;
+
+   if (SGL_(clo).collect_func == NULL &&
+       SGL_(clo).start_collect_func == NULL)
+   {
+      VG_(umsg)("*********************************************\n");
+      VG_(umsg)("Beginning event generation from program start\n");
+      VG_(umsg)("*********************************************\n");
+      SGL_(is_in_event_collect_func) = True;
+   }
+
+   if (SGL_(clo).gen_cf == True)
+      VG_(umsg)("WARNING: Control Flow events unsupported\n");
+   if (SGL_(clo).gen_bb == True)
+      VG_(umsg)("WARNING: Basic Block context events unsupported\n");
+
+   if (VG_(clo_vex_control).iropt_register_updates_default
+       != VexRegUpdSpAtMemAccess) {
+      CLG_DEBUG(1, " Using user specified value for "
+                "--vex-iropt-register-updates\n");
+   } else {
+      CLG_DEBUG(1, 
+                " Using default --vex-iropt-register-updates="
+                "sp-at-mem-access\n");
+   }
+
+   if (VG_(clo_px_file_backed) != VexRegUpdSpAtMemAccess) {
+      CLG_DEBUG(1, " Using user specified value for "
+                "--px-file-backed\n");
+   } else {
+      CLG_DEBUG(1, 
+                " Using default --px-file-backed="
+                "sp-at-mem-access\n");
+   }
+
+   if (VG_(clo_vex_control).iropt_unroll_thresh != 0) {
+      VG_(message)(Vg_UserMsg, 
+                   "callgrind only works with --vex-iropt-unroll-thresh=0\n"
+                   "=> resetting it back to 0\n");
+      VG_(clo_vex_control).iropt_unroll_thresh = 0;   // cannot be overriden.
+   }
+   if (VG_(clo_vex_control).guest_chase_thresh != 0) {
+      VG_(message)(Vg_UserMsg,
+                   "callgrind only works with --vex-guest-chase-thresh=0\n"
+                   "=> resetting it back to 0\n");
+      VG_(clo_vex_control).guest_chase_thresh = 0; // cannot be overriden.
+   }
+   
+   CLG_DEBUG(1, "  dump threads: %s\n", CLG_(clo).separate_threads ? "Yes":"No");
+   CLG_DEBUG(1, "  call sep. : %d\n", CLG_(clo).separate_callers);
+   CLG_DEBUG(1, "  rec. sep. : %d\n", CLG_(clo).separate_recursions);
+
+   if (!CLG_(clo).dump_line && !CLG_(clo).dump_instr && !CLG_(clo).dump_bb) {
+       VG_(message)(Vg_UserMsg, "Using source line as position.\n");
+       CLG_(clo).dump_line = True;
+   }
+
+   /* initialize hash tables */
+   CLG_(init_obj_table)();
+   CLG_(init_cxt_table)();
+   CLG_(init_bb_hash)();
+
+   CLG_(init_threads)();
+   CLG_(run_thread)(1);
+
+   CLG_(instrument_state) = CLG_(clo).instrument_atstart;
+}
+
+static
+void CLG_(pre_clo_init)(void)
+{
+    VG_(details_name)            ("Sigrind");
+    VG_(details_version)         (NULL);
+    VG_(details_description)     ("");
+    VG_(details_copyright_author)("Copyright (C) 2015-2017, "
+				  "by Michael Lui et al.");
+    VG_(details_bug_reports_to)  (VG_BUGS_TO);
+    VG_(details_avg_translation_sizeB) ( 500 );
+
+    VG_(clo_vex_control).iropt_register_updates_default
+       = VG_(clo_px_file_backed)
+       = VexRegUpdSpAtMemAccess; // overridable by the user.
+
+    VG_(clo_vex_control).iropt_unroll_thresh = 0;   // cannot be overriden.
+    VG_(clo_vex_control).guest_chase_thresh = 0;    // cannot be overriden.
+
+    VG_(basic_tool_funcs)        (CLG_(post_clo_init),
+                                  CLG_(instrument),
+                                  CLG_(fini));
+
+    VG_(needs_superblock_discards)(clg_discard_superblock_info);
+
+
+    VG_(needs_command_line_options)(CLG_(process_cmd_line_option),
+				    CLG_(print_usage),
+				    CLG_(print_debug_usage));
+
+    VG_(needs_client_requests)(CLG_(handle_client_request));
+    VG_(needs_print_stats)    (clg_print_stats);
+
+    VG_(track_start_client_code)  ( & clg_start_client_code_callback );
+    VG_(track_pre_deliver_signal) ( & CLG_(pre_signal) );
+    VG_(track_post_deliver_signal)( & CLG_(post_signal) );
+
+    /* Track syscalls */
+    /* XXX MDL20170226
+     * Right now syscalls are not being monitored.
+     * There hasn't been a convincing case made that memory reads/writes
+     * from syscalls are significant enough to warrant the extra monitoring.
+     * If required, the following callbacks can be used to get addt'l info
+     * every time a syscall writes to an address, and also in general when
+     * a syscall is invoked.
+     *
+     * VG_(track_post_mem_write) (vgcore_memwrite_callback)
+     * VG_(needs_syscall_wrapp)  (pre_syscall_callback,
+     *                            post_syscall_callback)
+     *
+     * For more info look in ../include/pub_tool_tooliface.h
+     * The other core event callbacks offered by Valgrind probably aren't
+     * relelvant for us. */
+
+    /* Defaults */
+    SGL_(set_clo_defaults)();
+    CLG_(set_clo_defaults)();
+}
+
+VG_DETERMINE_INTERFACE_VERSION(CLG_(pre_clo_init))
+
+/*------------------------------------------------------------*/
+/*--- Initialization and helpers                           ---*/
+/*------------------------------------------------------------*/
+
+
+static void init_Event ( Event* ev ) {
+   VG_(memset)(ev, 0, sizeof(Event));
+}
+
+static IRAtom* get_Event_dea ( Event* ev ) {
+   switch (ev->tag) {
+      case Ev_Dr: return ev->Ev.Dr.ea;
+      case Ev_Dw: return ev->Ev.Dw.ea;
+      case Ev_Dm: return ev->Ev.Dm.ea;
+      default:    tl_assert(0);
+   }
+}
+
+static Int get_Event_dszB ( Event* ev ) {
+   switch (ev->tag) {
+      case Ev_Dr: return ev->Ev.Dr.szB;
+      case Ev_Dw: return ev->Ev.Dw.szB;
+      case Ev_Dm: return ev->Ev.Dm.szB;
+      default:    tl_assert(0);
+   }
+}
+
+#if defined(VG_BIGENDIAN)
+# define CLGEndness Iend_BE
+#elif defined(VG_LITTLEENDIAN)
+# define CLGEndness Iend_LE
+#else
+# error "Unknown endianness"
+#endif
+
+static
+Addr IRConst2Addr(IRConst* con)
+{
+    Addr addr;
+
+    if (sizeof(Addr) == 4) {
+	CLG_ASSERT( con->tag == Ico_U32 );
+	addr = con->Ico.U32;
+    }
+    else if (sizeof(Addr) == 8) {
+	CLG_ASSERT( con->tag == Ico_U64 );
+	addr = con->Ico.U64;
+    }
+    else
+	VG_(tool_panic)("Callgrind: invalid Addr type");
+
+    return addr;
+}
+
+/* First pass over a BB to instrument, counting instructions and jumps
+ * This is needed for the size of the BB struct to allocate
+ *
+ * Called from CLG_(get_bb)
+ */
+void CLG_(collectBlockInfo)(IRSB* sbIn,
+			    /*INOUT*/ UInt* instrs,
+			    /*INOUT*/ UInt* cjmps,
+			    /*INOUT*/ Bool* cjmp_inverted)
+{
+    Int i;
+    IRStmt* st;
+    Addr instrAddr =0, jumpDst;
+    UInt instrLen = 0;
+    Bool toNextInstr = False;
+
+    // Ist_Exit has to be ignored in preamble code, before first IMark:
+    // preamble code is added by VEX for self modifying code, and has
+    // nothing to do with client code
+    Bool inPreamble = True;
+
+    if (!sbIn) return;
+
+    for (i = 0; i < sbIn->stmts_used; i++) {
+	  st = sbIn->stmts[i];
+	  if (Ist_IMark == st->tag) {
+	      inPreamble = False;
+
+	      instrAddr = st->Ist.IMark.addr;
+	      instrLen  = st->Ist.IMark.len;
+
+	      (*instrs)++;
+	      toNextInstr = False;
+	  }
+	  if (inPreamble) continue;
+	  if (Ist_Exit == st->tag) {
+	      jumpDst = IRConst2Addr(st->Ist.Exit.dst);
+	      toNextInstr =  (jumpDst == instrAddr + instrLen);
+
+	      (*cjmps)++;
+	  }
+    }
+
+    /* if the last instructions of BB conditionally jumps to next instruction
+     * (= first instruction of next BB in memory), this is a inverted by VEX.
+     */
+    *cjmp_inverted = toNextInstr;
+}
+
+static
+void addConstMemStoreStmt( IRSB* bbOut, UWord addr, UInt val, IRType hWordTy)
+{
+    addStmtToIRSB( bbOut,
+		   IRStmt_Store(CLGEndness,
+				IRExpr_Const(hWordTy == Ity_I32 ?
+					     IRConst_U32( addr ) :
+					     IRConst_U64( addr )),
+				IRExpr_Const(IRConst_U32(val)) ));
+}   
+
+
+
+static void showEvent ( Event* ev )
+{
+   switch (ev->tag) {
+      case Ev_Ir:
+	 VG_(printf)("Ir (InstrInfo %p) at +%u\n",
+		     ev->inode, ev->inode->instr_offset);
+	 break;
+      case Ev_Dr:
+	 VG_(printf)("Dr (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dr.szB);
+	 ppIRExpr(ev->Ev.Dr.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Dw:
+	 VG_(printf)("Dw (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dw.szB);
+	 ppIRExpr(ev->Ev.Dw.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Dm:
+	 VG_(printf)("Dm (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dm.szB);
+	 ppIRExpr(ev->Ev.Dm.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Bc:
+         VG_(printf)("Bc %p   GA=", ev->inode);
+         ppIRExpr(ev->Ev.Bc.taken);
+         VG_(printf)("\n");
+         break;
+      case Ev_Bi:
+         VG_(printf)("Bi %p  DST=", ev->inode);
+         ppIRExpr(ev->Ev.Bi.dst);
+         VG_(printf)("\n");
+         break;
+      case Ev_G:
+         VG_(printf)("G  %p\n", ev->inode);
+         break;
+      default:
+	 tl_assert(0);
+	 break;
+   }
+}
+
+/*--------------------------------------------------------------------*/
+/*--- end                                                   main.c ---*/
+/*--------------------------------------------------------------------*/
diff --git a/sigrind/sigil2_ipc.c b/sigrind/sigil2_ipc.c
new file mode 100644
index 000000000..9b97a5947
--- /dev/null
+++ b/sigrind/sigil2_ipc.c
@@ -0,0 +1,266 @@
+#include "sigil2_ipc.h"
+#include "coregrind/pub_core_libcfile.h"
+#include "coregrind/pub_core_aspacemgr.h"
+#include "coregrind/pub_core_syscall.h"
+#include "pub_tool_basics.h"
+#include "pub_tool_vki.h"       // errnum, vki_timespec
+#include "pub_tool_vkiscnums.h" // __NR_nanosleep
+
+static Bool initialized = False;
+static Int emptyfd;
+static Int fullfd;
+static Sigil2DBISharedData* shmem;
+/* IPC channel */
+
+
+static UInt           curr_idx;
+static EventBuffer*   curr_ev_buf;
+static SglEvVariant*  curr_ev_slot;
+static NameBuffer*    curr_name_buf;
+static char*          curr_name_slot;
+/* cached IPC state */
+
+
+static Bool is_full[SIGIL2_IPC_BUFFERS];
+/* track available buffers */
+
+
+static inline void set_and_init_buffer(UInt buf_idx)
+{
+    curr_ev_buf = shmem->eventBuffers + buf_idx;
+    curr_ev_buf->used = 0;
+    curr_ev_slot = curr_ev_buf->events + curr_ev_buf->used;
+
+    curr_name_buf = shmem->nameBuffers + buf_idx;
+    curr_name_buf->used = 0;
+    curr_name_slot = curr_name_buf->names + curr_name_buf->used;
+}
+
+
+static inline void flush_to_sigil2(void)
+{
+    /* Mark that the buffer is being flushed,
+     * and tell Sigil2 the buffer is ready to consume */
+    is_full[curr_idx] = True;
+    Int res = VG_(write)(fullfd, &curr_idx, sizeof(curr_idx));
+    if (res != sizeof(curr_idx))
+    {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+}
+
+
+static inline void set_next_buffer(void)
+{
+    /* try the next buffer, circular */
+    ++curr_idx;
+    if (curr_idx == SIGIL2_IPC_BUFFERS)
+        curr_idx = 0;
+
+    /* if the next buffer is full,
+     * wait until Sigil2 communicates that it's free */
+    if (is_full[curr_idx])
+    {
+        UInt buf_idx;
+        Int res = VG_(read)(emptyfd, &buf_idx, sizeof(buf_idx));
+        if (res != sizeof(buf_idx))
+        {
+            VG_(umsg)("error VG_(read)\n");
+            VG_(umsg)("error reading from Sigrind fifo\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit)(1);
+        }
+
+        tl_assert(buf_idx < SIGIL2_IPC_BUFFERS);
+        tl_assert(buf_idx == curr_idx);
+        curr_idx = buf_idx;
+        is_full[curr_idx] = False;
+    }
+
+    set_and_init_buffer(curr_idx);
+}
+
+
+static inline Bool is_events_full(void)
+{
+    return curr_ev_buf->used == SIGIL2_EVENTS_BUFFER_SIZE;
+}
+
+
+static inline Bool is_names_full(UInt size)
+{
+    return (curr_name_buf->used + size) > SIGIL2_EVENTS_BUFFER_SIZE;
+}
+
+
+SglEvVariant* SGL_(acq_event_slot)()
+{
+    tl_assert(initialized == True);
+
+    if (is_events_full())
+    {
+        flush_to_sigil2();
+        set_next_buffer();
+    }
+
+    curr_ev_buf->used++;
+    return curr_ev_slot++;
+}
+
+
+EventNameSlotTuple SGL_(acq_event_name_slot)(UInt size)
+{
+    tl_assert(initialized == True);
+
+    if (is_events_full() || is_names_full(size))
+    {
+        flush_to_sigil2();
+        set_next_buffer();
+    }
+
+    EventNameSlotTuple tuple = {curr_ev_slot, curr_name_slot, curr_name_buf->used};
+    curr_ev_buf->used   += 1;
+    curr_ev_slot        += 1;
+    curr_name_buf->used += size;
+    curr_name_slot      += size;
+
+    return tuple;
+}
+
+
+/******************************
+ * Initialization/Termination
+ ******************************/
+static int open_fifo(const HChar *fifo_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int tries = 0;
+    const int max_tries = 4;
+    int fd = VG_(fd_open)(fifo_path, flags, 0600);
+    while (fd < 0)
+    {
+        if (++tries < max_tries)
+        {
+#if defined(VGO_linux) && defined(VGA_amd64)
+            /* TODO any serious implications in Valgrind of calling syscalls directly?
+             * MDL20170220 The "VG_(syscall)" wrappers don't look like they do much
+             * else besides doing platform specific setup.
+             * In our case, we only accommodate x86_64 or aarch64. */
+            struct vki_timespec req;
+            req.tv_sec = 0;
+            req.tv_nsec = 500000000;
+            /* wait some time before trying to connect,
+             * giving Sigil2 time to bring up IPC */
+            VG_(do_syscall2)(__NR_nanosleep, (UWord)&req, 0);
+#else
+#error "Only linux is supported"
+#endif
+            fd = VG_(fd_open)(fifo_path, flags, 0600);
+        }
+        else
+        {
+            VG_(umsg)("FIFO for Sigrind failed\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit) (1);
+        }
+    }
+
+    return fd;
+}
+
+
+static Sigil2DBISharedData* open_shmem(const HChar *shmem_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int shared_mem_fd = VG_(fd_open)(shmem_path, flags, 0600);
+    if (shared_mem_fd < 0)
+    {
+        VG_(umsg)("Cannot open shared_mem file %s\n", shmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    SysRes res = VG_(am_shared_mmap_file_float_valgrind)(sizeof(Sigil2DBISharedData),
+                                                         VKI_PROT_READ|VKI_PROT_WRITE,
+                                                         shared_mem_fd, (Off64T)0);
+    if (sr_isError(res))
+    {
+        VG_(umsg)("error %lu %s\n", sr_Err(res), VG_(strerror)(sr_Err(res)));
+        VG_(umsg)("error VG_(am_shared_mmap_file_float_valgrind) %s\n", shmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    Addr addr_shared = sr_Res (res);
+    VG_(close)(shared_mem_fd);
+
+    return (Sigil2DBISharedData*) addr_shared;
+}
+
+
+void SGL_(init_IPC)()
+{
+    tl_assert(initialized == False);
+
+    if (SGL_(clo).ipc_dir == NULL)
+    {
+       VG_(fmsg)("No --ipc-dir argument found, shutting down...\n");
+       VG_(exit)(1);
+    }
+
+    Int ipc_dir_len = VG_(strlen)(SGL_(clo).ipc_dir);
+    Int filename_len;
+
+    //len is strlen + null + other chars (/ and -0)
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_SHMEM_BASENAME) + 4;
+    HChar shmem_path[filename_len];
+    VG_(snprintf)(shmem_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_SHMEM_BASENAME);
+
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_EMPTYFIFO_BASENAME) + 4;
+    HChar emptyfifo_path[filename_len];
+    VG_(snprintf)(emptyfifo_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_EMPTYFIFO_BASENAME);
+
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_FULLFIFO_BASENAME) + 4;
+    HChar fullfifo_path[filename_len];
+    VG_(snprintf)(fullfifo_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_FULLFIFO_BASENAME);
+
+    emptyfd = open_fifo(emptyfifo_path, VKI_O_RDONLY);
+    fullfd  = open_fifo(fullfifo_path, VKI_O_WRONLY);
+    shmem   = open_shmem(shmem_path, VKI_O_RDWR);
+
+    /* initialize cached IPC state */
+    curr_idx = 0;
+    set_and_init_buffer(curr_idx);
+    for (UInt i=0; i<SIGIL2_IPC_BUFFERS; ++i)
+        is_full[i] = False;
+
+    initialized = True;
+}
+
+
+void SGL_(term_IPC)(void)
+{
+    tl_assert(initialized == True);
+
+    /* send finish sequence */
+    UInt finished = SIGIL2_IPC_FINISHED;
+    if (VG_(write)(fullfd, &curr_idx, sizeof(curr_idx)) != sizeof(curr_idx) ||
+        VG_(write)(fullfd, &finished, sizeof(finished)) != sizeof(finished))
+    {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    /* wait until Sigrind disconnects */
+    while (VG_(read)(emptyfd, &finished, sizeof(finished)) > 0);
+
+    VG_(close)(emptyfd);
+    VG_(close)(fullfd);
+}
diff --git a/sigrind/sigil2_ipc.h b/sigrind/sigil2_ipc.h
new file mode 100644
index 000000000..d779653f3
--- /dev/null
+++ b/sigrind/sigil2_ipc.h
@@ -0,0 +1,28 @@
+#ifndef SGL_IPC_H
+#define SGL_IPC_H
+
+#include "Frontends/CommonShmemIPC.h"
+#include "global.h"
+
+/* An implementation of interprocess communication with the Sigil2 frontend.
+ * IPC includes initialization, termination, shared memory buffer writes, and
+ * synchronization via named pipes */
+
+typedef struct EventNameSlotTuple
+{
+    SglEvVariant*  event_slot;
+    char*          name_slot;
+    UInt           name_idx;
+} EventNameSlotTuple;
+
+void SGL_(init_IPC)(void);
+void SGL_(term_IPC)(void);
+
+SglEvVariant* SGL_(acq_event_slot)(void);
+/* Get a buffer slot to add an event */
+
+EventNameSlotTuple SGL_(acq_event_name_slot)(UInt size);
+/* Get a buffer slot to add an event (probably a context event)
+ * and a name slot to add a name with it (like a function name) */
+
+#endif
diff --git a/sigrind/tests/Makefile.am b/sigrind/tests/Makefile.am
new file mode 100644
index 000000000..972acc3ce
--- /dev/null
+++ b/sigrind/tests/Makefile.am
@@ -0,0 +1,5 @@
+
+include $(top_srcdir)/Makefile.tool-tests.am
+
+SUBDIRS = .
+DIST_SUBDIRS = .
diff --git a/sigrind/threads.c b/sigrind/threads.c
new file mode 100644
index 000000000..800250d14
--- /dev/null
+++ b/sigrind/threads.c
@@ -0,0 +1,451 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                 ct_threads.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "log_events.h"
+#include "global.h"
+#include "Core/PrimitiveEnums.h"
+
+#include "pub_tool_threadstate.h"
+
+/* forward decls */
+static exec_state* exec_state_save(void);
+static exec_state* exec_state_restore(void);
+static exec_state* push_exec_state(int);
+static exec_state* top_exec_state(void);
+
+static exec_stack current_states;
+
+
+/*------------------------------------------------------------*/
+/*--- Support for multi-threading                          ---*/
+/*------------------------------------------------------------*/
+
+
+/*
+ * For Valgrind, MT is cooperative (no preemting in our code),
+ * so we don't need locks...
+ *
+ * Per-thread data:
+ *  - BBCCs
+ *  - call stack
+ *  - call hash
+ *  - event counters: last, current
+ *
+ * Even when ignoring MT, we need this functions to set up some
+ * datastructures for the process (= Thread 1).
+ */
+
+/* current running thread */
+ThreadId CLG_(current_tid);
+ThreadId SGL_(active_tid);
+
+static thread_info** thread;
+Bool* SGL_(thread_in_synccall);
+
+thread_info** CLG_(get_threads)()
+{
+  return thread;
+}
+
+thread_info* CLG_(get_current_thread)()
+{
+  return thread[CLG_(current_tid)];
+}
+
+void CLG_(init_threads)()
+{
+    UInt i;
+
+    thread = CLG_MALLOC("cl.threads.it.1", VG_N_THREADS * sizeof thread[0]);
+    SGL_(thread_in_synccall) = CLG_MALLOC("cl.threads.it.1", VG_N_THREADS * sizeof *SGL_(thread_in_synccall));
+
+    for(i=0;i<VG_N_THREADS;i++)
+    {
+        thread[i] = 0;
+        SGL_(thread_in_synccall)[i] = False;
+    }
+    CLG_(current_tid) = VG_INVALID_THREADID;
+    SGL_(active_tid) = VG_INVALID_THREADID;
+}
+
+/* switches through all threads and calls func */
+void CLG_(forall_threads)(void (*func)(thread_info*))
+{
+  Int t, orig_tid = CLG_(current_tid);
+
+  for(t=1;t<VG_N_THREADS;t++) {
+    if (!thread[t]) continue;
+    CLG_(switch_thread)(t);
+    (*func)(thread[t]);
+  }
+  CLG_(switch_thread)(orig_tid);
+}
+
+
+static
+thread_info* new_thread(void)
+{
+    thread_info* t;
+
+    t = (thread_info*) CLG_MALLOC("cl.threads.nt.1",
+                                  sizeof(thread_info));
+
+    /* init state */
+    CLG_(init_exec_stack)( &(t->states) );
+    CLG_(init_call_stack)( &(t->calls) );
+    CLG_(init_fn_stack)  ( &(t->fns) );
+    /* t->states.entry[0]->cxt = CLG_(get_cxt)(t->fns.bottom); */
+
+    /* init data containers */
+    CLG_(init_fn_array)( &(t->fn_active) );
+    CLG_(init_bbcc_hash)( &(t->bbccs) );
+    CLG_(init_jcc_hash)( &(t->jccs) );
+
+    return t;
+}
+
+void SGL_(switch_thread)(ThreadId tid)
+{
+  if (tid == SGL_(active_tid))
+    return;
+
+  SGL_(active_tid) = tid;
+
+  /* ML: always send thread switch events; 
+   * valgrind can change at any time, even if sigrind is 'inside' 
+   * a synchronization call or outside the function being collected */ 
+  SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+}
+
+void CLG_(switch_thread)(ThreadId tid)
+{
+  SGL_(switch_thread)(tid);
+  if (tid == CLG_(current_tid)) return;
+
+  CLG_DEBUG(0, ">> thread %u (was %u)\n", tid, CLG_(current_tid));
+
+  if (CLG_(current_tid) != VG_INVALID_THREADID) {    
+    /* save thread state */
+    thread_info* t = thread[CLG_(current_tid)];
+
+    CLG_ASSERT(t != 0);
+
+    /* current context (including signal handler contexts) */
+    exec_state_save();
+    CLG_(copy_current_exec_stack)( &(t->states) );
+    CLG_(copy_current_call_stack)( &(t->calls) );
+    CLG_(copy_current_fn_stack)  ( &(t->fns) );
+
+    CLG_(copy_current_fn_array) ( &(t->fn_active) );
+    /* If we cumulate costs of threads, use TID 1 for all jccs/bccs */
+    if (!CLG_(clo).separate_threads) t = thread[1];
+    CLG_(copy_current_bbcc_hash)( &(t->bbccs) );
+    CLG_(copy_current_jcc_hash) ( &(t->jccs) );
+  }
+
+  CLG_(current_tid) = tid;
+  CLG_ASSERT(tid < VG_N_THREADS);
+
+  if (tid != VG_INVALID_THREADID) {
+    thread_info* t;
+
+    /* load thread state */
+
+    if (thread[tid] == 0) thread[tid] = new_thread();
+    t = thread[tid];
+
+    /* current context (including signal handler contexts) */
+    CLG_(set_current_exec_stack)( &(t->states) );
+    exec_state_restore();
+    CLG_(set_current_call_stack)( &(t->calls) );
+    CLG_(set_current_fn_stack)  ( &(t->fns) );
+
+    CLG_(set_current_fn_array)  ( &(t->fn_active) );
+    /* If we cumulate costs of threads, use TID 1 for all jccs/bccs */
+    if (!CLG_(clo).separate_threads) t = thread[1];
+    CLG_(set_current_bbcc_hash) ( &(t->bbccs) );
+    CLG_(set_current_jcc_hash)  ( &(t->jccs) );
+  }
+}
+
+
+void CLG_(run_thread)(ThreadId tid)
+{
+    /* now check for thread switch */
+    CLG_(switch_thread)(tid);
+}
+
+void CLG_(pre_signal)(ThreadId tid, Int sigNum, Bool alt_stack)
+{
+    exec_state *es;
+
+    CLG_DEBUG(0, ">> pre_signal(TID %u, sig %d, alt_st %s)\n",
+	     tid, sigNum, alt_stack ? "yes":"no");
+
+    /* switch to the thread the handler runs in */
+    CLG_(switch_thread)(tid);
+
+    /* save current execution state */
+    exec_state_save();
+
+    /* setup new cxtinfo struct for this signal handler */
+    es = push_exec_state(sigNum);
+    es->call_stack_bottom = CLG_(current_call_stack).sp;
+
+    /* setup current state for a spontaneous call */
+    CLG_(init_exec_state)( &CLG_(current_state) );
+    CLG_(current_state).sig = sigNum;
+    CLG_(push_cxt)(0);
+}
+
+/* Run post-signal if the stackpointer for call stack is at
+ * the bottom in current exec state (e.g. a signal handler)
+ *
+ * Called from CLG_(pop_call_stack)
+ */
+void CLG_(run_post_signal_on_call_stack_bottom)()
+{
+    exec_state* es = top_exec_state();
+    CLG_ASSERT(es != 0);
+    CLG_ASSERT(CLG_(current_state).sig >0);
+
+    if (CLG_(current_call_stack).sp == es->call_stack_bottom)
+	CLG_(post_signal)( CLG_(current_tid), CLG_(current_state).sig );
+}
+
+void CLG_(post_signal)(ThreadId tid, Int sigNum)
+{
+    exec_state* es;
+    UInt fn_number, *pactive;
+
+    CLG_DEBUG(0, ">> post_signal(TID %u, sig %d)\n",
+	     tid, sigNum);
+
+    /* thread switching potentially needed, eg. with instrumentation off */
+    CLG_(switch_thread)(tid);
+    CLG_ASSERT(sigNum == CLG_(current_state).sig);
+
+    /* Unwind call stack of this signal handler.
+     * This should only be needed at finalisation time
+     */
+    es = top_exec_state();
+    CLG_ASSERT(es != 0);
+    while(CLG_(current_call_stack).sp > es->call_stack_bottom)
+      CLG_(pop_call_stack)();
+
+    if (CLG_(current_state).cxt) {
+      /* correct active counts */
+      fn_number = CLG_(current_state).cxt->fn[0]->number;
+      pactive = CLG_(get_fn_entry)(fn_number);
+      (*pactive)--;
+      CLG_DEBUG(0, "  set active count of %s back to %u\n",
+	       CLG_(current_state).cxt->fn[0]->name, *pactive);
+    }
+
+    if (CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom) {
+	/* set fn_stack_top back.
+	 * top can point to 0 if nothing was executed in the signal handler;
+	 * this is possible at end on unwinding handlers.
+	 */
+	if (*(CLG_(current_fn_stack).top) != 0) {
+	    CLG_(current_fn_stack).top--;
+	    CLG_ASSERT(*(CLG_(current_fn_stack).top) == 0);
+	}
+      if (CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom)
+	CLG_(current_fn_stack).top--;
+    }
+
+    /* restore previous context */
+    es->sig = -1;
+    current_states.sp--;
+    es = top_exec_state();
+    CLG_(current_state).sig = es->sig;
+    exec_state_restore();
+
+    /* There is no way to reliable get the thread ID we are switching to
+     * after this handler returns. So we sync with actual TID at start of
+     * CLG_(setup_bb)(), which should be the next for callgrind.
+     */
+}
+
+
+
+/*------------------------------------------------------------*/
+/*--- Execution states in a thread & signal handlers       ---*/
+/*------------------------------------------------------------*/
+
+/* Each thread can be interrupted by a signal handler, and they
+ * themselves again. But as there's no scheduling among handlers
+ * of the same thread, we don't need additional stacks.
+ * So storing execution contexts and
+ * adding separators in the callstack(needed to not intermix normal/handler
+ * functions in contexts) should be enough.
+ */
+
+/* not initialized: call_stack_bottom, sig */
+void CLG_(init_exec_state)(exec_state* es)
+{
+  es->collect = CLG_(clo).collect_atstart;
+  es->cxt  = 0;
+  es->jmps_passed = 0;
+  es->bbcc = 0;
+  es->nonskipped = 0;
+}
+
+
+static exec_state* new_exec_state(Int sigNum)
+{
+    exec_state* es;
+    es = (exec_state*) CLG_MALLOC("cl.threads.nes.1",
+                                  sizeof(exec_state));
+
+    /* allocate real cost space: needed as incremented by
+     * simulation functions */
+
+    CLG_(init_exec_state)(es);
+    es->sig        = sigNum;
+    es->call_stack_bottom  = 0;
+
+    return es;
+}
+
+void CLG_(init_exec_stack)(exec_stack* es)
+{
+  Int i;
+
+  /* The first element is for the main thread */
+  es->entry[0] = new_exec_state(0);
+  for(i=1;i<MAX_SIGHANDLERS;i++)
+    es->entry[i] = 0;
+  es->sp = 0;
+}
+
+void CLG_(copy_current_exec_stack)(exec_stack* dst)
+{
+  Int i;
+
+  dst->sp = current_states.sp;
+  for(i=0;i<MAX_SIGHANDLERS;i++)
+    dst->entry[i] = current_states.entry[i];
+}
+
+void CLG_(set_current_exec_stack)(exec_stack* dst)
+{
+  Int i;
+
+  current_states.sp = dst->sp;
+  for(i=0;i<MAX_SIGHANDLERS;i++)
+    current_states.entry[i] = dst->entry[i];
+}
+
+
+/* Get top context info struct of current thread */
+static
+exec_state* top_exec_state(void)
+{
+  Int sp = current_states.sp;
+  exec_state* es;
+
+  CLG_ASSERT((sp >= 0) && (sp < MAX_SIGHANDLERS));
+  es = current_states.entry[sp];
+  CLG_ASSERT(es != 0);
+  return es;
+}
+
+/* Allocates a free context info structure for a new entered
+ * signal handler, putting it on the context stack.
+ * Returns a pointer to the structure.
+ */
+static exec_state* push_exec_state(int sigNum)
+{
+  Int sp;
+  exec_state* es;
+
+  current_states.sp++;
+  sp = current_states.sp;
+
+  CLG_ASSERT((sigNum > 0) && (sigNum <= _VKI_NSIG));
+  CLG_ASSERT((sp > 0) && (sp < MAX_SIGHANDLERS));
+  es = current_states.entry[sp];
+  if (!es) {
+    es = new_exec_state(sigNum);
+    current_states.entry[sp] = es;
+  }
+  else
+    es->sig = sigNum;
+
+  return es;
+}
+
+/* Save current context to top cxtinfo struct */
+static
+exec_state* exec_state_save(void)
+{
+  exec_state* es = top_exec_state();
+
+  es->cxt         = CLG_(current_state).cxt;
+  es->collect     = CLG_(current_state).collect;
+  es->jmps_passed = CLG_(current_state).jmps_passed;
+  es->bbcc        = CLG_(current_state).bbcc;
+  es->nonskipped  = CLG_(current_state).nonskipped;
+  CLG_ASSERT(es->cost == CLG_(current_state).cost);
+
+  CLG_DEBUGIF(1) {
+    CLG_DEBUG(1, "  cxtinfo_save(sig %d): collect %s, jmps_passed %d\n",
+	     es->sig, es->collect ? "Yes": "No", es->jmps_passed);	
+    CLG_(print_bbcc)(-9, es->bbcc);
+  }
+
+  /* signal number does not need to be saved */
+  CLG_ASSERT(CLG_(current_state).sig == es->sig);
+
+  return es;
+}
+
+static
+exec_state* exec_state_restore(void)
+{
+  exec_state* es = top_exec_state();
+
+  CLG_(current_state).cxt     = es->cxt;
+  CLG_(current_state).collect = es->collect;
+  CLG_(current_state).jmps_passed = es->jmps_passed;
+  CLG_(current_state).bbcc    = es->bbcc;
+  CLG_(current_state).nonskipped = es->nonskipped;
+  CLG_(current_state).cost    = es->cost;
+  CLG_(current_state).sig     = es->sig;
+
+  CLG_DEBUGIF(1) {
+	CLG_DEBUG(1, "  exec_state_restore(sig %d): collect %s, jmps_passed %d\n",
+		  es->sig, es->collect ? "Yes": "No", es->jmps_passed);
+	CLG_(print_bbcc)(-9, es->bbcc);
+	CLG_(print_cxt)(-9, es->cxt, 0);
+  }
+
+  return es;
+}
-- 
2.15.0

